<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[webrtc video流程分析]]></title>
    <url>%2F2023%2F03%2F01%2Fwebrtc-video%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[video收集candidate之后，就可以建立好数据传输的通道了，现在通过源码走读下视频数据是如何 采集 &amp; 编码 &amp; 传输的 videosource &amp;&amp; videotrack123456789101112131415_localVideoTrack = [self createLocalVideoTrack];if (_localVideoTrack) [_peerConnection addTrack:_localVideoTrack streamIds:@[ kARDMediaStreamId ]];``` ```c++- (RTC_OBJC_TYPE(RTCVideoTrack) *)createLocalVideoTrack &#123; if ([_settings currentAudioOnlySettingFromStore]) &#123; return nil; &#125; RTC_OBJC_TYPE(RTCVideoSource) *source = [_factory videoSource]; return [_factory videoTrackWithSource:source trackId:kARDVideoTrackId];&#125; 这里的 videosource 跟 videotrack 就是 后面 capture的过程的 delegate capture123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687RTC_OBJC_TYPE(RTCCameraVideoCapturer) *capturer = [[RTC_OBJC_TYPE(RTCCameraVideoCapturer) alloc] initWithDelegate:source];ARDSettingsModel *settingsModel = [[ARDSettingsModel alloc] init]; _captureController = [[ARDCaptureController alloc] initWithCapturer:localCapturer settings:settingsModel]; [_captureController startCapture];- (void)startCapture &#123; [self startCapture:nil];&#125;- (void)startCapture:(void (^)(NSError *))completion &#123; AVCaptureDevicePosition position = _usingFrontCamera ? AVCaptureDevicePositionFront : AVCaptureDevicePositionBack; AVCaptureDevice *device = [self findDeviceForPosition:position]; AVCaptureDeviceFormat *format = [self selectFormatForDevice:device]; if (format == nil) &#123; RTCLogError(@"No valid formats for device %@", device); NSAssert(NO, @""); return; &#125; NSInteger fps = [self selectFpsForFormat:format]; [_capturer startCaptureWithDevice:device format:format fps:fps completionHandler:completion];&#125;- (void)stopCapture &#123; [_capturer stopCapture];&#125;- (void)switchCamera &#123; _usingFrontCamera = !_usingFrontCamera; [self startCapture:nil];&#125;- (void)switchCamera:(void (^)(NSError *))completion &#123; _usingFrontCamera = !_usingFrontCamera; [self startCapture:completion];&#125;#pragma mark - Private- (AVCaptureDevice *)findDeviceForPosition:(AVCaptureDevicePosition)position &#123; NSArray&lt;AVCaptureDevice *&gt; *captureDevices = [RTC_OBJC_TYPE(RTCCameraVideoCapturer) captureDevices]; for (AVCaptureDevice *device in captureDevices) &#123; if (device.position == position) &#123; return device; &#125; &#125; return captureDevices[0];&#125;- (AVCaptureDeviceFormat *)selectFormatForDevice:(AVCaptureDevice *)device &#123; NSArray&lt;AVCaptureDeviceFormat *&gt; *formats = [RTC_OBJC_TYPE(RTCCameraVideoCapturer) supportedFormatsForDevice:device]; int targetWidth = [_settings currentVideoResolutionWidthFromStore]; int targetHeight = [_settings currentVideoResolutionHeightFromStore]; AVCaptureDeviceFormat *selectedFormat = nil; int currentDiff = INT_MAX; for (AVCaptureDeviceFormat *format in formats) &#123; CMVideoDimensions dimension = CMVideoFormatDescriptionGetDimensions(format.formatDescription); FourCharCode pixelFormat = CMFormatDescriptionGetMediaSubType(format.formatDescription); int diff = abs(targetWidth - dimension.width) + abs(targetHeight - dimension.height); if (diff &lt; currentDiff) &#123; selectedFormat = format; currentDiff = diff; &#125; else if (diff == currentDiff &amp;&amp; pixelFormat == [_capturer preferredOutputPixelFormat]) &#123; selectedFormat = format; &#125; &#125; return selectedFormat;&#125;- (NSInteger)selectFpsForFormat:(AVCaptureDeviceFormat *)format &#123; Float64 maxSupportedFramerate = 0; for (AVFrameRateRange *fpsRange in format.videoSupportedFrameRateRanges) &#123; maxSupportedFramerate = fmax(maxSupportedFramerate, fpsRange.maxFrameRate); &#125; return fmin(maxSupportedFramerate, kFramerateLimit);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121- (void)startCaptureWithDevice:(AVCaptureDevice *)device format:(AVCaptureDeviceFormat *)format fps:(NSInteger)fps completionHandler:(nullable void (^)(NSError *_Nullable error))completionHandler &#123; _willBeRunning = YES; [RTC_OBJC_TYPE(RTCDispatcher) dispatchAsyncOnType:RTCDispatcherTypeCaptureSession block:^&#123; RTCLogInfo("startCaptureWithDevice %@ @ %ld fps", format, (long)fps);#if TARGET_OS_IPHONE dispatch_async(dispatch_get_main_queue(), ^&#123; if (!self-&gt;_generatingOrientationNotifications) &#123; [[UIDevice currentDevice] beginGeneratingDeviceOrientationNotifications]; self-&gt;_generatingOrientationNotifications = YES; &#125; &#125;);#endif self.currentDevice = device; NSError *error = nil; if (![self.currentDevice lockForConfiguration:&amp;error]) &#123; RTCLogError(@"Failed to lock device %@. Error: %@", self.currentDevice, error.userInfo); if (completionHandler) &#123; completionHandler(error); &#125; self.willBeRunning = NO; return; &#125; [self reconfigureCaptureSessionInput]; [self updateOrientation]; [self updateDeviceCaptureFormat:format fps:fps]; [self updateVideoDataOutputPixelFormat:format]; [self.captureSession startRunning]; [self.currentDevice unlockForConfiguration]; self.isRunning = YES; if (completionHandler) &#123; completionHandler(nil); &#125; &#125;];&#125;#pragma mark AVCaptureVideoDataOutputSampleBufferDelegate- (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection &#123; NSParameterAssert(captureOutput == _videoDataOutput); if (CMSampleBufferGetNumSamples(sampleBuffer) != 1 || !CMSampleBufferIsValid(sampleBuffer) || !CMSampleBufferDataIsReady(sampleBuffer)) &#123; return; &#125; CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer); if (pixelBuffer == nil) &#123; return; &#125;#if TARGET_OS_IPHONE // Default to portrait orientation on iPhone. BOOL usingFrontCamera = NO; // Check the image's EXIF for the camera the image came from as the image could have been // delayed as we set alwaysDiscardsLateVideoFrames to NO. AVCaptureDevicePosition cameraPosition = [AVCaptureSession devicePositionForSampleBuffer:sampleBuffer]; if (cameraPosition != AVCaptureDevicePositionUnspecified) &#123; usingFrontCamera = AVCaptureDevicePositionFront == cameraPosition; &#125; else &#123; AVCaptureDeviceInput *deviceInput = (AVCaptureDeviceInput *)((AVCaptureInputPort *)connection.inputPorts.firstObject).input; usingFrontCamera = AVCaptureDevicePositionFront == deviceInput.device.position; &#125; switch (_orientation) &#123; case UIDeviceOrientationPortrait: _rotation = RTCVideoRotation_90; break; case UIDeviceOrientationPortraitUpsideDown: _rotation = RTCVideoRotation_270; break; case UIDeviceOrientationLandscapeLeft: _rotation = usingFrontCamera ? RTCVideoRotation_180 : RTCVideoRotation_0; break; case UIDeviceOrientationLandscapeRight: _rotation = usingFrontCamera ? RTCVideoRotation_0 : RTCVideoRotation_180; break; case UIDeviceOrientationFaceUp: case UIDeviceOrientationFaceDown: case UIDeviceOrientationUnknown: // Ignore. break; &#125;#else // No rotation on Mac. _rotation = RTCVideoRotation_0;#endif RTC_OBJC_TYPE(RTCCVPixelBuffer) *rtcPixelBuffer = [[RTC_OBJC_TYPE(RTCCVPixelBuffer) alloc] initWithPixelBuffer:pixelBuffer]; int64_t timeStampNs = CMTimeGetSeconds(CMSampleBufferGetPresentationTimeStamp(sampleBuffer)) * kNanosecondsPerSecond; RTC_OBJC_TYPE(RTCVideoFrame) *videoFrame = [[RTC_OBJC_TYPE(RTCVideoFrame) alloc] initWithBuffer:rtcPixelBuffer rotation:_rotation timeStampNs:timeStampNs]; [self.delegate capturer:self didCaptureVideoFrame:videoFrame];&#125;@implementation RTCObjCVideoSourceAdapter@synthesize objCVideoTrackSource = _objCVideoTrackSource;- (void)capturer:(RTC_OBJC_TYPE(RTCVideoCapturer) *)capturer didCaptureVideoFrame:(RTC_OBJC_TYPE(RTCVideoFrame) *)frame &#123; _objCVideoTrackSource-&gt;OnCapturedFrame(frame);&#125;@end 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687void ObjCVideoTrackSource::OnCapturedFrame(RTC_OBJC_TYPE(RTCVideoFrame) * frame) &#123; const int64_t timestamp_us = frame.timeStampNs / rtc::kNumNanosecsPerMicrosec; const int64_t translated_timestamp_us = timestamp_aligner_.TranslateTimestamp(timestamp_us, rtc::TimeMicros()); int adapted_width; int adapted_height; int crop_width; int crop_height; int crop_x; int crop_y; if (!AdaptFrame(frame.width, frame.height, timestamp_us, &amp;adapted_width, &amp;adapted_height, &amp;crop_width, &amp;crop_height, &amp;crop_x, &amp;crop_y)) &#123; return; &#125; rtc::scoped_refptr&lt;VideoFrameBuffer&gt; buffer; if (adapted_width == frame.width &amp;&amp; adapted_height == frame.height) &#123; // No adaption - optimized path. buffer = rtc::make_ref_counted&lt;ObjCFrameBuffer&gt;(frame.buffer); &#125; else if ([frame.buffer isKindOfClass:[RTC_OBJC_TYPE(RTCCVPixelBuffer) class]]) &#123; // Adapted CVPixelBuffer frame. RTC_OBJC_TYPE(RTCCVPixelBuffer) *rtcPixelBuffer = (RTC_OBJC_TYPE(RTCCVPixelBuffer) *)frame.buffer; buffer = rtc::make_ref_counted&lt;ObjCFrameBuffer&gt;([[RTC_OBJC_TYPE(RTCCVPixelBuffer) alloc] initWithPixelBuffer:rtcPixelBuffer.pixelBuffer adaptedWidth:adapted_width adaptedHeight:adapted_height cropWidth:crop_width cropHeight:crop_height cropX:crop_x + rtcPixelBuffer.cropX cropY:crop_y + rtcPixelBuffer.cropY]); &#125; else &#123; // Adapted I420 frame. // TODO(magjed): Optimize this I420 path. rtc::scoped_refptr&lt;I420Buffer&gt; i420_buffer = I420Buffer::Create(adapted_width, adapted_height); buffer = rtc::make_ref_counted&lt;ObjCFrameBuffer&gt;(frame.buffer); i420_buffer-&gt;CropAndScaleFrom(*buffer-&gt;ToI420(), crop_x, crop_y, crop_width, crop_height); buffer = i420_buffer; &#125; // Applying rotation is only supported for legacy reasons and performance is // not critical here. VideoRotation rotation = static_cast&lt;VideoRotation&gt;(frame.rotation); if (apply_rotation() &amp;&amp; rotation != kVideoRotation_0) &#123; buffer = I420Buffer::Rotate(*buffer-&gt;ToI420(), rotation); rotation = kVideoRotation_0; &#125; OnFrame(VideoFrame::Builder() .set_video_frame_buffer(buffer) .set_rotation(rotation) .set_timestamp_us(translated_timestamp_us) .build());&#125;void AdaptedVideoTrackSource::OnFrame(const webrtc::VideoFrame&amp; frame) &#123; rtc::scoped_refptr&lt;webrtc::VideoFrameBuffer&gt; buffer( frame.video_frame_buffer()); /* Note that this is a "best effort" approach to wants.rotation_applied; apply_rotation_ can change from false to true between the check of apply_rotation() and the call to broadcaster_.OnFrame(), in which case we generate a frame with pending rotation despite some sink with wants.rotation_applied == true was just added. The VideoBroadcaster enforces synchronization for us in this case, by not passing the frame on to sinks which don't want it. */ if (apply_rotation() &amp;&amp; frame.rotation() != webrtc::kVideoRotation_0 &amp;&amp; buffer-&gt;type() == webrtc::VideoFrameBuffer::Type::kI420) &#123; /* Apply pending rotation. */ webrtc::VideoFrame rotated_frame(frame); rotated_frame.set_video_frame_buffer( webrtc::I420Buffer::Rotate(*buffer-&gt;GetI420(), frame.rotation())); rotated_frame.set_rotation(webrtc::kVideoRotation_0); broadcaster_.OnFrame(rotated_frame); &#125; else &#123; broadcaster_.OnFrame(frame); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839.../webrtc/src/media/base/video_broadcaster.ccvoid VideoBroadcaster::OnFrame(const webrtc::VideoFrame&amp; frame) &#123; webrtc::MutexLock lock(&amp;sinks_and_wants_lock_); bool current_frame_was_discarded = false; for (auto&amp; sink_pair : sink_pairs()) &#123; if (sink_pair.wants.rotation_applied &amp;&amp; frame.rotation() != webrtc::kVideoRotation_0) &#123; // Calls to OnFrame are not synchronized with changes to the sink wants. // When rotation_applied is set to true, one or a few frames may get here // with rotation still pending. Protect sinks that don't expect any // pending rotation. RTC_LOG(LS_VERBOSE) &lt;&lt; "Discarding frame with unexpected rotation."; sink_pair.sink-&gt;OnDiscardedFrame(); current_frame_was_discarded = true; continue; &#125; if (sink_pair.wants.black_frames) &#123; webrtc::VideoFrame black_frame = webrtc::VideoFrame::Builder() .set_video_frame_buffer( GetBlackFrameBuffer(frame.width(), frame.height())) .set_rotation(frame.rotation()) .set_timestamp_us(frame.timestamp_us()) .set_id(frame.id()) .build(); sink_pair.sink-&gt;OnFrame(black_frame); &#125; else if (!previous_frame_sent_to_all_sinks_ &amp;&amp; frame.has_update_rect()) &#123; // Since last frame was not sent to some sinks, no reliable update // information is available, so we need to clear the update rect. webrtc::VideoFrame copy = frame; copy.clear_update_rect(); sink_pair.sink-&gt;OnFrame(copy); &#125; else &#123; sink_pair.sink-&gt;OnFrame(frame); &#125; &#125; previous_frame_sent_to_all_sinks_ = !current_frame_was_discarded;&#125; capture采集到frame最后都会回调到 video_broadcaster，然后通过 broadcaster 遍历sink 分发出去 通过 track 添加的sink 都会 经过 source 进到 broadcaster 来管理 12345678910111213void AdaptedVideoTrackSource::AddOrUpdateSink( rtc::VideoSinkInterface&lt;webrtc::VideoFrame&gt;* sink, const rtc::VideoSinkWants&amp; wants) &#123; broadcaster_.AddOrUpdateSink(sink, wants); OnSinkWantsChanged(broadcaster_.wants());&#125;void AdaptedVideoTrackSource::RemoveSink( rtc::VideoSinkInterface&lt;webrtc::VideoFrame&gt;* sink) &#123; broadcaster_.RemoveSink(sink); OnSinkWantsChanged(broadcaster_.wants());&#125; sink 拿到 采集的 frame 就会 进到 encode 的过程 encode 过程中 frame_cadence_adapter 实现了 VideoSinkInterface 接口 会作为sink 添加到 broadcaster中，最终会进到 VideoStreamEncoder的 核心逻辑中。 encodeencode 的过程是最复杂的逻辑。。。。 .../webrtc/src/pc/sdp_offer_answer.cc SdpOfferAnswerHandler::ApplyLocalDescription -&gt; SdpOfferAnswerHandler::UpdateTransceiversAndDataChannels -&gt; SdpOfferAnswerHandler::UpdateTransceiverChannel // 创建 channel SdpOfferAnswerHandler::UpdateSessionState -&gt; SdpOfferAnswerHandler::PushdownMediaDescription -&gt; 12345678910111213141516171819202122232425262728293031RTCError SdpOfferAnswerHandler::UpdateTransceiverChannel( rtc::scoped_refptr&lt;RtpTransceiverProxyWithInternal&lt;RtpTransceiver&gt;&gt; transceiver, const cricket::ContentInfo&amp; content, const cricket::ContentGroup* bundle_group) &#123; TRACE_EVENT0("webrtc", "SdpOfferAnswerHandler::UpdateTransceiverChannel"); RTC_DCHECK(IsUnifiedPlan()); RTC_DCHECK(transceiver); cricket::ChannelInterface* channel = transceiver-&gt;internal()-&gt;channel(); if (content.rejected) &#123; if (channel) &#123; transceiver-&gt;internal()-&gt;ClearChannel(); &#125; &#125; else &#123; if (!channel) &#123; auto error = transceiver-&gt;internal()-&gt;CreateChannel( content.name, pc_-&gt;call_ptr(), pc_-&gt;configuration()-&gt;media_config, pc_-&gt;SrtpRequired(), pc_-&gt;GetCryptoOptions(), audio_options(), video_options(), video_bitrate_allocator_factory_.get(), [&amp;](absl::string_view mid) &#123; RTC_DCHECK_RUN_ON(network_thread()); return transport_controller_n()-&gt;GetRtpTransport(mid); &#125;); if (!error.ok()) &#123; return error; &#125; &#125; &#125; return RTCError::OK();&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374.../webrtc/src/pc/rtp_transceiver.ccRTCError RtpTransceiver::CreateChannel( absl::string_view mid, Call* call_ptr, const cricket::MediaConfig&amp; media_config, bool srtp_required, CryptoOptions crypto_options, const cricket::AudioOptions&amp; audio_options, const cricket::VideoOptions&amp; video_options, VideoBitrateAllocatorFactory* video_bitrate_allocator_factory, std::function&lt;RtpTransportInternal*(absl::string_view)&gt; transport_lookup) &#123; RTC_DCHECK_RUN_ON(thread_); if (!media_engine()) &#123; // TODO(hta): Must be a better way return RTCError(RTCErrorType::INTERNAL_ERROR, "No media engine for mid=" + std::string(mid)); &#125; std::unique_ptr&lt;cricket::ChannelInterface&gt; new_channel; if (media_type() == cricket::MEDIA_TYPE_AUDIO) &#123; // TODO(bugs.webrtc.org/11992): CreateVideoChannel internally switches to // the worker thread. We shouldn't be using the `call_ptr_` hack here but // simply be on the worker thread and use `call_` (update upstream code). RTC_DCHECK(call_ptr); RTC_DCHECK(media_engine()); // TODO(bugs.webrtc.org/11992): Remove this workaround after updates in // PeerConnection and add the expectation that we're already on the right // thread. context()-&gt;worker_thread()-&gt;BlockingCall([&amp;] &#123; RTC_DCHECK_RUN_ON(context()-&gt;worker_thread()); cricket::VoiceMediaChannel* media_channel = media_engine()-&gt;voice().CreateMediaChannel( call_ptr, media_config, audio_options, crypto_options); if (!media_channel) &#123; return; &#125; new_channel = std::make_unique&lt;cricket::VoiceChannel&gt;( context()-&gt;worker_thread(), context()-&gt;network_thread(), context()-&gt;signaling_thread(), absl::WrapUnique(media_channel), mid, srtp_required, crypto_options, context()-&gt;ssrc_generator()); &#125;); &#125; else &#123; RTC_DCHECK_EQ(cricket::MEDIA_TYPE_VIDEO, media_type()); // TODO(bugs.webrtc.org/11992): CreateVideoChannel internally switches to // the worker thread. We shouldn't be using the `call_ptr_` hack here but // simply be on the worker thread and use `call_` (update upstream code). context()-&gt;worker_thread()-&gt;BlockingCall([&amp;] &#123; RTC_DCHECK_RUN_ON(context()-&gt;worker_thread()); cricket::VideoMediaChannel* media_channel = media_engine()-&gt;video().CreateMediaChannel( call_ptr, media_config, video_options, crypto_options, video_bitrate_allocator_factory); if (!media_channel) &#123; return; &#125; new_channel = std::make_unique&lt;cricket::VideoChannel&gt;( context()-&gt;worker_thread(), context()-&gt;network_thread(), context()-&gt;signaling_thread(), absl::WrapUnique(media_channel), mid, srtp_required, crypto_options, context()-&gt;ssrc_generator()); &#125;); &#125; if (!new_channel) &#123; // TODO(hta): Must be a better way return RTCError(RTCErrorType::INTERNAL_ERROR, "Failed to create channel for mid=" + std::string(mid)); &#125; SetChannel(std::move(new_channel), transport_lookup); return RTCError::OK();&#125; media_engine 是创建 factory 的过程中 生成的 dependencies.media_engine = cricket::CreateMediaEngine(std::move(media_dependencies)); 1234567891011121314151617181920212223242526272829.../src/media/engine/webrtc_media_engine.ccstd::unique_ptr&lt;MediaEngineInterface&gt; CreateMediaEngine( MediaEngineDependencies dependencies) &#123; // TODO(sprang): Make populating `dependencies.trials` mandatory and remove // these fallbacks. std::unique_ptr&lt;webrtc::FieldTrialsView&gt; fallback_trials( dependencies.trials ? nullptr : new webrtc::FieldTrialBasedConfig()); const webrtc::FieldTrialsView&amp; trials = dependencies.trials ? *dependencies.trials : *fallback_trials; auto audio_engine = std::make_unique&lt;WebRtcVoiceEngine&gt;( dependencies.task_queue_factory, dependencies.adm.get(), std::move(dependencies.audio_encoder_factory), std::move(dependencies.audio_decoder_factory), std::move(dependencies.audio_mixer), std::move(dependencies.audio_processing), dependencies.audio_frame_processor, trials);#ifdef HAVE_WEBRTC_VIDEO auto video_engine = std::make_unique&lt;WebRtcVideoEngine&gt;( std::move(dependencies.video_encoder_factory), std::move(dependencies.video_decoder_factory), trials);#else auto video_engine = std::make_unique&lt;NullWebRtcVideoEngine&gt;();#endif return std::make_unique&lt;CompositeMediaEngine&gt;(std::move(fallback_trials), std::move(audio_engine), std::move(video_engine));&#125; 12345678910111213.../src/media/engine/webrtc_video_engine.ccVideoMediaChannel* WebRtcVideoEngine::CreateMediaChannel( webrtc::Call* call, const MediaConfig&amp; config, const VideoOptions&amp; options, const webrtc::CryptoOptions&amp; crypto_options, webrtc::VideoBitrateAllocatorFactory* video_bitrate_allocator_factory) &#123; RTC_LOG(LS_INFO) &lt;&lt; "CreateMediaChannel. Options: " &lt;&lt; options.ToString(); return new WebRtcVideoChannel(call, config, options, crypto_options, encoder_factory_.get(), decoder_factory_.get(), video_bitrate_allocator_factory);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198.../webrtc/src/pc/channel.ccVideoChannel::VideoChannel(rtc::Thread* worker_thread, rtc::Thread* network_thread, rtc::Thread* signaling_thread, std::unique_ptr&lt;VideoMediaChannel&gt; media_channel, absl::string_view mid, bool srtp_required, webrtc::CryptoOptions crypto_options, UniqueRandomIdGenerator* ssrc_generator) : BaseChannel(worker_thread, network_thread, signaling_thread, std::move(media_channel), mid, srtp_required, crypto_options, ssrc_generator), send_channel_(this-&gt;media_channel()-&gt;AsVideoChannel()), receive_channel_(this-&gt;media_channel()-&gt;AsVideoChannel()) &#123;&#125;bool BaseChannel::SetLocalContent(const MediaContentDescription* content, SdpType type, std::string&amp; error_desc) &#123; RTC_DCHECK_RUN_ON(worker_thread()); TRACE_EVENT0("webrtc", "BaseChannel::SetLocalContent"); return SetLocalContent_w(content, type, error_desc);&#125;bool VideoChannel::SetLocalContent_w(const MediaContentDescription* content, SdpType type, std::string&amp; error_desc) &#123; TRACE_EVENT0("webrtc", "VideoChannel::SetLocalContent_w"); RTC_DLOG(LS_INFO) &lt;&lt; "Setting local video description for " &lt;&lt; ToString(); RTC_LOG_THREAD_BLOCK_COUNT(); RtpHeaderExtensions header_extensions = GetDeduplicatedRtpHeaderExtensions(content-&gt;rtp_header_extensions()); bool update_header_extensions = true; media_send_channel()-&gt;SetExtmapAllowMixed(content-&gt;extmap_allow_mixed()); VideoRecvParameters recv_params = last_recv_params_; RtpParametersFromMediaDescription( content-&gt;as_video(), header_extensions, webrtc::RtpTransceiverDirectionHasRecv(content-&gt;direction()), &amp;recv_params); VideoSendParameters send_params = last_send_params_; bool needs_send_params_update = false; if (type == SdpType::kAnswer || type == SdpType::kPrAnswer) &#123; for (auto&amp; send_codec : send_params.codecs) &#123; auto* recv_codec = FindMatchingCodec(recv_params.codecs, send_codec); if (recv_codec) &#123; if (!recv_codec-&gt;packetization &amp;&amp; send_codec.packetization) &#123; send_codec.packetization.reset(); needs_send_params_update = true; &#125; else if (recv_codec-&gt;packetization != send_codec.packetization) &#123; error_desc = StringFormat( "Failed to set local answer due to invalid codec packetization " "specified in m-section with mid='%s'.", mid().c_str()); return false; &#125; &#125; &#125; &#125; if (!media_receive_channel()-&gt;SetRecvParameters(recv_params)) &#123; error_desc = StringFormat( "Failed to set local video description recv parameters for m-section " "with mid='%s'.", mid().c_str()); return false; &#125; bool criteria_modified = false; if (webrtc::RtpTransceiverDirectionHasRecv(content-&gt;direction())) &#123; for (const VideoCodec&amp; codec : content-&gt;as_video()-&gt;codecs()) &#123; if (MaybeAddHandledPayloadType(codec.id)) criteria_modified = true; &#125; &#125; last_recv_params_ = recv_params; if (needs_send_params_update) &#123; if (!media_send_channel()-&gt;SetSendParameters(send_params)) &#123; error_desc = StringFormat( "Failed to set send parameters for m-section with mid='%s'.", mid().c_str()); return false; &#125; last_send_params_ = send_params; &#125; if (!UpdateLocalStreams_w(content-&gt;as_video()-&gt;streams(), type, error_desc)) &#123; RTC_DCHECK(!error_desc.empty()); return false; &#125; set_local_content_direction(content-&gt;direction()); UpdateMediaSendRecvState_w(); RTC_DCHECK_BLOCK_COUNT_NO_MORE_THAN(0); bool success = MaybeUpdateDemuxerAndRtpExtensions_w( criteria_modified, update_header_extensions ? absl::optional&lt;RtpHeaderExtensions&gt;(std::move(header_extensions)) : absl::nullopt, error_desc); RTC_DCHECK_BLOCK_COUNT_NO_MORE_THAN(1); return success;&#125;bool BaseChannel::UpdateLocalStreams_w(const std::vector&lt;StreamParams&gt;&amp; streams, SdpType type, std::string&amp; error_desc) &#123; // In the case of RIDs (where SSRCs are not negotiated), this method will // generate an SSRC for each layer in StreamParams. That representation will // be stored internally in `local_streams_`. // In subsequent offers, the same stream can appear in `streams` again // (without the SSRCs), so it should be looked up using RIDs (if available) // and then by primary SSRC. // In both scenarios, it is safe to assume that the media channel will be // created with a StreamParams object with SSRCs. However, it is not safe to // assume that `local_streams_` will always have SSRCs as there are scenarios // in which niether SSRCs or RIDs are negotiated. // Check for streams that have been removed. bool ret = true; for (const StreamParams&amp; old_stream : local_streams_) &#123; if (!old_stream.has_ssrcs() || GetStream(streams, StreamFinder(&amp;old_stream))) &#123; continue; &#125; if (!media_send_channel()-&gt;RemoveSendStream(old_stream.first_ssrc())) &#123; error_desc = StringFormat( "Failed to remove send stream with ssrc %u from m-section with " "mid='%s'.", old_stream.first_ssrc(), mid().c_str()); ret = false; &#125; &#125; // Check for new streams. std::vector&lt;StreamParams&gt; all_streams; for (const StreamParams&amp; stream : streams) &#123; StreamParams* existing = GetStream(local_streams_, StreamFinder(&amp;stream)); if (existing) &#123; // Parameters cannot change for an existing stream. all_streams.push_back(*existing); continue; &#125; all_streams.push_back(stream); StreamParams&amp; new_stream = all_streams.back(); if (!new_stream.has_ssrcs() &amp;&amp; !new_stream.has_rids()) &#123; continue; &#125; RTC_DCHECK(new_stream.has_ssrcs() || new_stream.has_rids()); if (new_stream.has_ssrcs() &amp;&amp; new_stream.has_rids()) &#123; error_desc = StringFormat( "Failed to add send stream: %u into m-section with mid='%s'. Stream " "has both SSRCs and RIDs.", new_stream.first_ssrc(), mid().c_str()); ret = false; continue; &#125; // At this point we use the legacy simulcast group in StreamParams to // indicate that we want multiple layers to the media channel. if (!new_stream.has_ssrcs()) &#123; // TODO(bugs.webrtc.org/10250): Indicate if flex is desired here. new_stream.GenerateSsrcs(new_stream.rids().size(), /* rtx = */ true, /* flex_fec = */ false, ssrc_generator_); &#125; if (media_send_channel()-&gt;AddSendStream(new_stream)) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Add send stream ssrc: " &lt;&lt; new_stream.ssrcs[0] &lt;&lt; " into " &lt;&lt; ToString(); &#125; else &#123; error_desc = StringFormat( "Failed to add send stream ssrc: %u into m-section with mid='%s'", new_stream.first_ssrc(), mid().c_str()); ret = false; &#125; &#125; local_streams_ = all_streams; return ret;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151.../webrtc/src/media/engine/webrtc_video_engine.ccbool WebRtcVideoChannel::AddSendStream(const StreamParams&amp; sp) &#123; RTC_DCHECK_RUN_ON(&amp;thread_checker_); RTC_LOG(LS_INFO) &lt;&lt; "AddSendStream: " &lt;&lt; sp.ToString(); if (!ValidateStreamParams(sp)) return false; if (!ValidateSendSsrcAvailability(sp)) return false; for (uint32_t used_ssrc : sp.ssrcs) send_ssrcs_.insert(used_ssrc); webrtc::VideoSendStream::Config config(this); for (const RidDescription&amp; rid : sp.rids()) &#123; config.rtp.rids.push_back(rid.rid); &#125; config.suspend_below_min_bitrate = video_config_.suspend_below_min_bitrate; config.periodic_alr_bandwidth_probing = video_config_.periodic_alr_bandwidth_probing; config.encoder_settings.experiment_cpu_load_estimator = video_config_.experiment_cpu_load_estimator; config.encoder_settings.encoder_factory = encoder_factory_; config.encoder_settings.bitrate_allocator_factory = bitrate_allocator_factory_; config.encoder_settings.encoder_switch_request_callback = this; config.crypto_options = crypto_options_; config.rtp.extmap_allow_mixed = ExtmapAllowMixed(); config.rtcp_report_interval_ms = video_config_.rtcp_report_interval_ms; WebRtcVideoSendStream* stream = new WebRtcVideoSendStream( call_, sp, std::move(config), default_send_options_, video_config_.enable_cpu_adaptation, bitrate_config_.max_bitrate_bps, send_codec_, send_rtp_extensions_, send_params_); uint32_t ssrc = sp.first_ssrc(); RTC_DCHECK(ssrc != 0); send_streams_[ssrc] = stream; if (rtcp_receiver_report_ssrc_ == kDefaultRtcpReceiverReportSsrc) &#123; SetReceiverReportSsrc(ssrc); &#125; if (sending_) &#123; stream-&gt;SetSend(true); &#125; return true;&#125;void WebRtcVideoChannel::WebRtcVideoSendStream::SetSend(bool send) &#123; RTC_DCHECK_RUN_ON(&amp;thread_checker_); sending_ = send; UpdateSendState();&#125;void WebRtcVideoChannel::WebRtcVideoSendStream::SetSendParameters( const ChangedSendParameters&amp; params) &#123; RTC_DCHECK_RUN_ON(&amp;thread_checker_); // `recreate_stream` means construction-time parameters have changed and the // sending stream needs to be reset with the new config. bool recreate_stream = false; if (params.rtcp_mode) &#123; parameters_.config.rtp.rtcp_mode = *params.rtcp_mode; rtp_parameters_.rtcp.reduced_size = parameters_.config.rtp.rtcp_mode == webrtc::RtcpMode::kReducedSize; recreate_stream = true; &#125; if (params.extmap_allow_mixed) &#123; parameters_.config.rtp.extmap_allow_mixed = *params.extmap_allow_mixed; recreate_stream = true; &#125; if (params.rtp_header_extensions) &#123; parameters_.config.rtp.extensions = *params.rtp_header_extensions; rtp_parameters_.header_extensions = *params.rtp_header_extensions; recreate_stream = true; &#125; if (params.mid) &#123; parameters_.config.rtp.mid = *params.mid; recreate_stream = true; &#125; if (params.max_bandwidth_bps) &#123; parameters_.max_bitrate_bps = *params.max_bandwidth_bps; ReconfigureEncoder(nullptr); &#125; if (params.conference_mode) &#123; parameters_.conference_mode = *params.conference_mode; &#125; // Set codecs and options. if (params.send_codec) &#123; SetCodec(*params.send_codec); recreate_stream = false; // SetCodec has already recreated the stream. &#125; else if (params.conference_mode &amp;&amp; parameters_.codec_settings) &#123; SetCodec(*parameters_.codec_settings); recreate_stream = false; // SetCodec has already recreated the stream. &#125; if (recreate_stream) &#123; RTC_LOG(LS_INFO) &lt;&lt; "RecreateWebRtcStream (send) because of SetSendParameters"; RecreateWebRtcStream(); &#125;&#125;void WebRtcVideoChannel::WebRtcVideoSendStream::RecreateWebRtcStream() &#123; RTC_DCHECK_RUN_ON(&amp;thread_checker_); if (stream_ != NULL) &#123; call_-&gt;DestroyVideoSendStream(stream_); &#125; RTC_CHECK(parameters_.codec_settings); RTC_DCHECK_EQ((parameters_.encoder_config.content_type == webrtc::VideoEncoderConfig::ContentType::kScreen), parameters_.options.is_screencast.value_or(false)) &lt;&lt; "encoder content type inconsistent with screencast option"; parameters_.encoder_config.encoder_specific_settings = ConfigureVideoEncoderSettings(parameters_.codec_settings-&gt;codec); webrtc::VideoSendStream::Config config = parameters_.config.Copy(); if (!config.rtp.rtx.ssrcs.empty() &amp;&amp; config.rtp.rtx.payload_type == -1) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "RTX SSRCs configured but there's no configured RTX " "payload type the set codec. Ignoring RTX."; config.rtp.rtx.ssrcs.clear(); &#125; if (parameters_.encoder_config.number_of_streams == 1) &#123; // SVC is used instead of simulcast. Remove unnecessary SSRCs. if (config.rtp.ssrcs.size() &gt; 1) &#123; config.rtp.ssrcs.resize(1); if (config.rtp.rtx.ssrcs.size() &gt; 1) &#123; config.rtp.rtx.ssrcs.resize(1); &#125; &#125; &#125; stream_ = call_-&gt;CreateVideoSendStream(std::move(config), parameters_.encoder_config.Copy()); parameters_.encoder_config.encoder_specific_settings = NULL; // Calls stream_-&gt;StartPerRtpStream() to start the VideoSendStream // if necessary conditions are met. UpdateSendState(); // Attach the source after starting the send stream to prevent frames from // being injected into a not-yet initializated video stream encoder. if (source_) &#123; stream_-&gt;SetSource(source_, GetDegradationPreference()); &#125;&#125; 123456789101112131415161718.../webrtc/src/call/degraded_call.ccVideoSendStream* DegradedCall::CreateVideoSendStream( VideoSendStream::Config config, VideoEncoderConfig encoder_config) &#123; std::unique_ptr&lt;FakeNetworkPipeTransportAdapter&gt; transport_adapter; if (!send_configs_.empty()) &#123; transport_adapter = std::make_unique&lt;FakeNetworkPipeTransportAdapter&gt;( send_pipe_.get(), call_.get(), clock_, config.send_transport); config.send_transport = transport_adapter.get(); &#125; VideoSendStream* send_stream = call_-&gt;CreateVideoSendStream( std::move(config), std::move(encoder_config)); if (send_stream &amp;&amp; transport_adapter) &#123; video_send_transport_adapters_[send_stream] = std::move(transport_adapter); &#125; return send_stream;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162.../webrtc/src/call/call.ccwebrtc::VideoSendStream* Call::CreateVideoSendStream( webrtc::VideoSendStream::Config config, VideoEncoderConfig encoder_config, std::unique_ptr&lt;FecController&gt; fec_controller) &#123; TRACE_EVENT0("webrtc", "Call::CreateVideoSendStream"); RTC_DCHECK_RUN_ON(worker_thread_); EnsureStarted(); video_send_delay_stats_-&gt;AddSsrcs(config); for (size_t ssrc_index = 0; ssrc_index &lt; config.rtp.ssrcs.size(); ++ssrc_index) &#123; event_log_-&gt;Log(std::make_unique&lt;RtcEventVideoSendStreamConfig&gt;( CreateRtcLogStreamConfig(config, ssrc_index))); &#125; // TODO(mflodman): Base the start bitrate on a current bandwidth estimate, if // the call has already started. // Copy ssrcs from `config` since `config` is moved. std::vector&lt;uint32_t&gt; ssrcs = config.rtp.ssrcs; VideoSendStream* send_stream = new VideoSendStream( clock_, num_cpu_cores_, task_queue_factory_, network_thread_, call_stats_-&gt;AsRtcpRttStats(), transport_send_.get(), bitrate_allocator_.get(), video_send_delay_stats_.get(), event_log_, std::move(config), std::move(encoder_config), suspended_video_send_ssrcs_, suspended_video_payload_states_, std::move(fec_controller), *config_.trials); for (uint32_t ssrc : ssrcs) &#123; RTC_DCHECK(video_send_ssrcs_.find(ssrc) == video_send_ssrcs_.end()); video_send_ssrcs_[ssrc] = send_stream; &#125; video_send_streams_.insert(send_stream); video_send_streams_empty_.store(false, std::memory_order_relaxed); // Forward resources that were previously added to the call to the new stream. for (const auto&amp; resource_forwarder : adaptation_resource_forwarders_) &#123; resource_forwarder-&gt;OnCreateVideoSendStream(send_stream); &#125; UpdateAggregateNetworkState(); return send_stream;&#125;webrtc::VideoSendStream* Call::CreateVideoSendStream( webrtc::VideoSendStream::Config config, VideoEncoderConfig encoder_config) &#123; RTC_DCHECK_RUN_ON(worker_thread_); if (config_.fec_controller_factory) &#123; RTC_LOG(LS_INFO) &lt;&lt; "External FEC Controller will be used."; &#125; std::unique_ptr&lt;FecController&gt; fec_controller = config_.fec_controller_factory ? config_.fec_controller_factory-&gt;CreateFecController() : std::make_unique&lt;FecControllerDefault&gt;(clock_); return CreateVideoSendStream(std::move(config), std::move(encoder_config), std::move(fec_controller));&#125; 123456789.../webrtc/src/video/video_send_stream.ccvoid VideoSendStream::SetSource( rtc::VideoSourceInterface&lt;webrtc::VideoFrame&gt;* source, const DegradationPreference&amp; degradation_preference) &#123; RTC_DCHECK_RUN_ON(&amp;thread_checker_); video_stream_encoder_-&gt;SetSource(source, degradation_preference);&#125; addtrack 过程 会创建 transceiver ，transceiver 分别创建 audiochannel &amp; videochannel 后续channel 会针对 sdp 做解析（local/remote）, 协商出 音视频 传输的各种参数 ，参数会存储在 VideoSendStream中，这时候encoder 还没开始创建 VideoSendStream 构造函数 会 创建 VideoStreamEncoder VideoStreamEncoder 是后续 创建具体的 encode 以及 接收 videoframe 经过 encode 编码，接收EncodedImage 回调的 关键类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697.../webrtc/src/pc/rtp_transmission_manager.ccRTCErrorOr&lt;rtc::scoped_refptr&lt;RtpSenderInterface&gt;&gt;RtpTransmissionManager::AddTrack( rtc::scoped_refptr&lt;MediaStreamTrackInterface&gt; track, const std::vector&lt;std::string&gt;&amp; stream_ids, const std::vector&lt;RtpEncodingParameters&gt;* init_send_encodings) &#123; RTC_DCHECK_RUN_ON(signaling_thread()); return (IsUnifiedPlan() ? AddTrackUnifiedPlan(track, stream_ids, init_send_encodings) : AddTrackPlanB(track, stream_ids, init_send_encodings));&#125;RTCErrorOr&lt;rtc::scoped_refptr&lt;RtpSenderInterface&gt;&gt;RtpTransmissionManager::AddTrackUnifiedPlan( rtc::scoped_refptr&lt;MediaStreamTrackInterface&gt; track, const std::vector&lt;std::string&gt;&amp; stream_ids, const std::vector&lt;RtpEncodingParameters&gt;* init_send_encodings) &#123; auto transceiver = FindFirstTransceiverForAddedTrack(track, init_send_encodings); if (transceiver) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Reusing an existing " &lt;&lt; cricket::MediaTypeToString(transceiver-&gt;media_type()) &lt;&lt; " transceiver for AddTrack."; if (transceiver-&gt;stopping()) &#123; LOG_AND_RETURN_ERROR(RTCErrorType::INVALID_PARAMETER, "The existing transceiver is stopping."); &#125; if (transceiver-&gt;direction() == RtpTransceiverDirection::kRecvOnly) &#123; transceiver-&gt;internal()-&gt;set_direction( RtpTransceiverDirection::kSendRecv); &#125; else if (transceiver-&gt;direction() == RtpTransceiverDirection::kInactive) &#123; transceiver-&gt;internal()-&gt;set_direction( RtpTransceiverDirection::kSendOnly); &#125; transceiver-&gt;sender()-&gt;SetTrack(track.get()); transceiver-&gt;internal()-&gt;sender_internal()-&gt;set_stream_ids(stream_ids); transceiver-&gt;internal()-&gt;set_reused_for_addtrack(true); &#125; else &#123; cricket::MediaType media_type = (track-&gt;kind() == MediaStreamTrackInterface::kAudioKind ? cricket::MEDIA_TYPE_AUDIO : cricket::MEDIA_TYPE_VIDEO); RTC_LOG(LS_INFO) &lt;&lt; "Adding " &lt;&lt; cricket::MediaTypeToString(media_type) &lt;&lt; " transceiver in response to a call to AddTrack."; std::string sender_id = track-&gt;id(); // Avoid creating a sender with an existing ID by generating a random ID. // This can happen if this is the second time AddTrack has created a sender // for this track. if (FindSenderById(sender_id)) &#123; sender_id = rtc::CreateRandomUuid(); &#125; auto sender = CreateSender( media_type, sender_id, track, stream_ids, init_send_encodings ? *init_send_encodings : std::vector&lt;RtpEncodingParameters&gt;(1, RtpEncodingParameters&#123;&#125;)); auto receiver = CreateReceiver(media_type, rtc::CreateRandomUuid()); transceiver = CreateAndAddTransceiver(sender, receiver); transceiver-&gt;internal()-&gt;set_created_by_addtrack(true); transceiver-&gt;internal()-&gt;set_direction(RtpTransceiverDirection::kSendRecv); &#125; return transceiver-&gt;sender();&#125;rtc::scoped_refptr&lt;RtpSenderProxyWithInternal&lt;RtpSenderInternal&gt;&gt;RtpTransmissionManager::CreateSender( cricket::MediaType media_type, const std::string&amp; id, rtc::scoped_refptr&lt;MediaStreamTrackInterface&gt; track, const std::vector&lt;std::string&gt;&amp; stream_ids, const std::vector&lt;RtpEncodingParameters&gt;&amp; send_encodings) &#123; RTC_DCHECK_RUN_ON(signaling_thread()); rtc::scoped_refptr&lt;RtpSenderProxyWithInternal&lt;RtpSenderInternal&gt;&gt; sender; if (media_type == cricket::MEDIA_TYPE_AUDIO) &#123; RTC_DCHECK(!track || (track-&gt;kind() == MediaStreamTrackInterface::kAudioKind)); sender = RtpSenderProxyWithInternal&lt;RtpSenderInternal&gt;::Create( signaling_thread(), AudioRtpSender::Create(worker_thread(), id, legacy_stats_, this)); NoteUsageEvent(UsageEvent::AUDIO_ADDED); &#125; else &#123; RTC_DCHECK_EQ(media_type, cricket::MEDIA_TYPE_VIDEO); RTC_DCHECK(!track || (track-&gt;kind() == MediaStreamTrackInterface::kVideoKind)); sender = RtpSenderProxyWithInternal&lt;RtpSenderInternal&gt;::Create( signaling_thread(), VideoRtpSender::Create(worker_thread(), id, this)); NoteUsageEvent(UsageEvent::VIDEO_ADDED); &#125; bool set_track_succeeded = sender-&gt;SetTrack(track.get()); RTC_DCHECK(set_track_succeeded); sender-&gt;internal()-&gt;set_stream_ids(stream_ids); sender-&gt;internal()-&gt;set_init_send_encodings(send_encodings); return sender;&#125; RtpTransmissionManager::AddTrackUnifiedPlan -&gt; RtpSenderBase::SetTrack -&gt; VideoRtpSender::SetSend -&gt; WebRtcVideoChannel::WebRtcVideoSendStream::SetVideoSend -&gt; VideoSendStream::SetSource -&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071.../src/video/video_send_stream.ccVideoSendStream::VideoSendStream( Clock* clock, int num_cpu_cores, TaskQueueFactory* task_queue_factory, TaskQueueBase* network_queue, RtcpRttStats* call_stats, RtpTransportControllerSendInterface* transport, BitrateAllocatorInterface* bitrate_allocator, SendDelayStats* send_delay_stats, RtcEventLog* event_log, VideoSendStream::Config config, VideoEncoderConfig encoder_config, const std::map&lt;uint32_t, RtpState&gt;&amp; suspended_ssrcs, const std::map&lt;uint32_t, RtpPayloadState&gt;&amp; suspended_payload_states, std::unique_ptr&lt;FecController&gt; fec_controller, const FieldTrialsView&amp; field_trials) : rtp_transport_queue_(transport-&gt;GetWorkerQueue()), transport_(transport), stats_proxy_(clock, config, encoder_config.content_type, field_trials), config_(std::move(config)), content_type_(encoder_config.content_type), video_stream_encoder_(CreateVideoStreamEncoder( clock, num_cpu_cores, task_queue_factory, &amp;stats_proxy_, config_.encoder_settings, GetBitrateAllocationCallbackType(config_, field_trials), field_trials, config_.encoder_selector)), encoder_feedback_( clock, config_.rtp.ssrcs, video_stream_encoder_.get(), [this](uint32_t ssrc, const std::vector&lt;uint16_t&gt;&amp; seq_nums) &#123; return rtp_video_sender_-&gt;GetSentRtpPacketInfos(ssrc, seq_nums); &#125;), rtp_video_sender_( transport-&gt;CreateRtpVideoSender(suspended_ssrcs, suspended_payload_states, config_.rtp, config_.rtcp_report_interval_ms, config_.send_transport, CreateObservers(call_stats, &amp;encoder_feedback_, &amp;stats_proxy_, send_delay_stats), event_log, std::move(fec_controller), CreateFrameEncryptionConfig(&amp;config_), config_.frame_transformer)), send_stream_(clock, &amp;stats_proxy_, transport, bitrate_allocator, video_stream_encoder_.get(), &amp;config_, encoder_config.max_bitrate_bps, encoder_config.bitrate_priority, encoder_config.content_type, rtp_video_sender_, field_trials) &#123; RTC_DCHECK(config_.encoder_settings.encoder_factory); RTC_DCHECK(config_.encoder_settings.bitrate_allocator_factory); video_stream_encoder_-&gt;SetFecControllerOverride(rtp_video_sender_); ReconfigureVideoEncoder(std::move(encoder_config));&#125; send_stream_ 就是 VideoSendStreamImpl 继承了 VideoStreamEncoderInterface::EncoderSink 实现了 OnEncodedImage 方法 12345// Callback function which is called when an image has been encoded. virtual Result OnEncodedImage( const EncodedImage&amp; encoded_image, const CodecSpecificInfo* codec_specific_info) = 0; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134.../webrtc/src/video/video_stream_encoder.ccVideoStreamEncoder::VideoStreamEncoder( Clock* clock, uint32_t number_of_cores, VideoStreamEncoderObserver* encoder_stats_observer, const VideoStreamEncoderSettings&amp; settings, std::unique_ptr&lt;OveruseFrameDetector&gt; overuse_detector, std::unique_ptr&lt;FrameCadenceAdapterInterface&gt; frame_cadence_adapter, std::unique_ptr&lt;webrtc::TaskQueueBase, webrtc::TaskQueueDeleter&gt; encoder_queue, BitrateAllocationCallbackType allocation_cb_type, const FieldTrialsView&amp; field_trials, webrtc::VideoEncoderFactory::EncoderSelectorInterface* encoder_selector) : field_trials_(field_trials), worker_queue_(TaskQueueBase::Current()), number_of_cores_(number_of_cores), sink_(nullptr), settings_(settings), allocation_cb_type_(allocation_cb_type), rate_control_settings_(RateControlSettings::ParseFromFieldTrials()), encoder_selector_from_constructor_(encoder_selector), encoder_selector_from_factory_( encoder_selector_from_constructor_ ? nullptr : settings.encoder_factory-&gt;GetEncoderSelector()), encoder_selector_(encoder_selector_from_constructor_ ? encoder_selector_from_constructor_ : encoder_selector_from_factory_.get()), encoder_stats_observer_(encoder_stats_observer), cadence_callback_(*this), frame_cadence_adapter_(std::move(frame_cadence_adapter)), encoder_initialized_(false), max_framerate_(-1), pending_encoder_reconfiguration_(false), pending_encoder_creation_(false), crop_width_(0), crop_height_(0), encoder_target_bitrate_bps_(absl::nullopt), max_data_payload_length_(0), encoder_paused_and_dropped_frame_(false), was_encode_called_since_last_initialization_(false), encoder_failed_(false), clock_(clock), last_captured_timestamp_(0), delta_ntp_internal_ms_(clock_-&gt;CurrentNtpInMilliseconds() - clock_-&gt;TimeInMilliseconds()), last_frame_log_ms_(clock_-&gt;TimeInMilliseconds()), captured_frame_count_(0), dropped_frame_cwnd_pushback_count_(0), dropped_frame_encoder_block_count_(0), pending_frame_post_time_us_(0), accumulated_update_rect_&#123;0, 0, 0, 0&#125;, accumulated_update_rect_is_valid_(true), animation_start_time_(Timestamp::PlusInfinity()), cap_resolution_due_to_video_content_(false), expect_resize_state_(ExpectResizeState::kNoResize), fec_controller_override_(nullptr), force_disable_frame_dropper_(false), pending_frame_drops_(0), cwnd_frame_counter_(0), next_frame_types_(1, VideoFrameType::kVideoFrameDelta), frame_encode_metadata_writer_(this), experiment_groups_(GetExperimentGroups()), automatic_animation_detection_experiment_( ParseAutomatincAnimationDetectionFieldTrial()), input_state_provider_(encoder_stats_observer), video_stream_adapter_( std::make_unique&lt;VideoStreamAdapter&gt;(&amp;input_state_provider_, encoder_stats_observer, field_trials)), degradation_preference_manager_( std::make_unique&lt;DegradationPreferenceManager&gt;( video_stream_adapter_.get())), adaptation_constraints_(), stream_resource_manager_(&amp;input_state_provider_, encoder_stats_observer, clock_, settings_.experiment_cpu_load_estimator, std::move(overuse_detector), degradation_preference_manager_.get(), field_trials), video_source_sink_controller_(/*sink=*/frame_cadence_adapter_.get(), /*source=*/nullptr), default_limits_allowed_( !field_trials.IsEnabled("WebRTC-DefaultBitrateLimitsKillSwitch")), qp_parsing_allowed_( !field_trials.IsEnabled("WebRTC-QpParsingKillSwitch")), switch_encoder_on_init_failures_(!field_trials.IsDisabled( kSwitchEncoderOnInitializationFailuresFieldTrial)), vp9_low_tier_core_threshold_( ParseVp9LowTierCoreCountThreshold(field_trials)), encoder_queue_(std::move(encoder_queue)) &#123; TRACE_EVENT0("webrtc", "VideoStreamEncoder::VideoStreamEncoder"); RTC_DCHECK_RUN_ON(worker_queue_); RTC_DCHECK(encoder_stats_observer); RTC_DCHECK_GE(number_of_cores, 1); frame_cadence_adapter_-&gt;Initialize(&amp;cadence_callback_); stream_resource_manager_.Initialize(encoder_queue_.Get()); encoder_queue_.PostTask([this] &#123; RTC_DCHECK_RUN_ON(&amp;encoder_queue_); resource_adaptation_processor_ = std::make_unique&lt;ResourceAdaptationProcessor&gt;( video_stream_adapter_.get()); stream_resource_manager_.SetAdaptationProcessor( resource_adaptation_processor_.get(), video_stream_adapter_.get()); resource_adaptation_processor_-&gt;AddResourceLimitationsListener( &amp;stream_resource_manager_); video_stream_adapter_-&gt;AddRestrictionsListener(&amp;stream_resource_manager_); video_stream_adapter_-&gt;AddRestrictionsListener(this); stream_resource_manager_.MaybeInitializePixelLimitResource(); // Add the stream resource manager's resources to the processor. adaptation_constraints_ = stream_resource_manager_.AdaptationConstraints(); for (auto* constraint : adaptation_constraints_) &#123; video_stream_adapter_-&gt;AddAdaptationConstraint(constraint); &#125; &#125;);&#125;void VideoStreamEncoder::SetSink(EncoderSink* sink, bool rotation_applied) &#123; RTC_DCHECK_RUN_ON(worker_queue_); video_source_sink_controller_.SetRotationApplied(rotation_applied); video_source_sink_controller_.PushSourceSinkSettings(); encoder_queue_.PostTask([this, sink] &#123; RTC_DCHECK_RUN_ON(&amp;encoder_queue_); sink_ = sink; &#125;);&#125; videostreamencode 通过 video_source_sink_controller_ frame_cadence_adapter_ 注入到 videotrack sink ，进而接受 onframe 的回调12345cadence_callback_(*this),video_source_sink_controller_(/*sink=*/frame_cadence_adapter_.get(), /*source=*/nullptr), 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389void VideoStreamEncoder::OnFrame(Timestamp post_time, int frames_scheduled_for_processing, const VideoFrame&amp; video_frame) &#123; RTC_DCHECK_RUN_ON(&amp;encoder_queue_); VideoFrame incoming_frame = video_frame; // In some cases, e.g., when the frame from decoder is fed to encoder, // the timestamp may be set to the future. As the encoding pipeline assumes // capture time to be less than present time, we should reset the capture // timestamps here. Otherwise there may be issues with RTP send stream. if (incoming_frame.timestamp_us() &gt; post_time.us()) incoming_frame.set_timestamp_us(post_time.us()); // Capture time may come from clock with an offset and drift from clock_. int64_t capture_ntp_time_ms; if (video_frame.ntp_time_ms() &gt; 0) &#123; capture_ntp_time_ms = video_frame.ntp_time_ms(); &#125; else if (video_frame.render_time_ms() != 0) &#123; capture_ntp_time_ms = video_frame.render_time_ms() + delta_ntp_internal_ms_; &#125; else &#123; capture_ntp_time_ms = post_time.ms() + delta_ntp_internal_ms_; &#125; incoming_frame.set_ntp_time_ms(capture_ntp_time_ms); // Convert NTP time, in ms, to RTP timestamp. const int kMsToRtpTimestamp = 90; incoming_frame.set_timestamp( kMsToRtpTimestamp * static_cast&lt;uint32_t&gt;(incoming_frame.ntp_time_ms())); if (incoming_frame.ntp_time_ms() &lt;= last_captured_timestamp_) &#123; // We don't allow the same capture time for two frames, drop this one. RTC_LOG(LS_WARNING) &lt;&lt; "Same/old NTP timestamp (" &lt;&lt; incoming_frame.ntp_time_ms() &lt;&lt; " &lt;= " &lt;&lt; last_captured_timestamp_ &lt;&lt; ") for incoming frame. Dropping."; encoder_queue_.PostTask([this, incoming_frame]() &#123; RTC_DCHECK_RUN_ON(&amp;encoder_queue_); accumulated_update_rect_.Union(incoming_frame.update_rect()); accumulated_update_rect_is_valid_ &amp;= incoming_frame.has_update_rect(); &#125;); return; &#125; bool log_stats = false; if (post_time.ms() - last_frame_log_ms_ &gt; kFrameLogIntervalMs) &#123; last_frame_log_ms_ = post_time.ms(); log_stats = true; &#125; last_captured_timestamp_ = incoming_frame.ntp_time_ms(); encoder_stats_observer_-&gt;OnIncomingFrame(incoming_frame.width(), incoming_frame.height()); ++captured_frame_count_; CheckForAnimatedContent(incoming_frame, post_time.us()); bool cwnd_frame_drop = cwnd_frame_drop_interval_ &amp;&amp; (cwnd_frame_counter_++ % cwnd_frame_drop_interval_.value() == 0); if (frames_scheduled_for_processing == 1 &amp;&amp; !cwnd_frame_drop) &#123; MaybeEncodeVideoFrame(incoming_frame, post_time.us()); &#125; else &#123; if (cwnd_frame_drop) &#123; // Frame drop by congestion window pushback. Do not encode this // frame. ++dropped_frame_cwnd_pushback_count_; encoder_stats_observer_-&gt;OnFrameDropped( VideoStreamEncoderObserver::DropReason::kCongestionWindow); &#125; else &#123; // There is a newer frame in flight. Do not encode this frame. RTC_LOG(LS_VERBOSE) &lt;&lt; "Incoming frame dropped due to that the encoder is blocked."; ++dropped_frame_encoder_block_count_; encoder_stats_observer_-&gt;OnFrameDropped( VideoStreamEncoderObserver::DropReason::kEncoderQueue); &#125; accumulated_update_rect_.Union(incoming_frame.update_rect()); accumulated_update_rect_is_valid_ &amp;= incoming_frame.has_update_rect(); &#125; if (log_stats) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Number of frames: captured " &lt;&lt; captured_frame_count_ &lt;&lt; ", dropped (due to congestion window pushback) " &lt;&lt; dropped_frame_cwnd_pushback_count_ &lt;&lt; ", dropped (due to encoder blocked) " &lt;&lt; dropped_frame_encoder_block_count_ &lt;&lt; ", interval_ms " &lt;&lt; kFrameLogIntervalMs; captured_frame_count_ = 0; dropped_frame_cwnd_pushback_count_ = 0; dropped_frame_encoder_block_count_ = 0; &#125;&#125;void VideoStreamEncoder::MaybeEncodeVideoFrame(const VideoFrame&amp; video_frame, int64_t time_when_posted_us) &#123; RTC_DCHECK_RUN_ON(&amp;encoder_queue_); input_state_provider_.OnFrameSizeObserved(video_frame.size()); if (!last_frame_info_ || video_frame.width() != last_frame_info_-&gt;width || video_frame.height() != last_frame_info_-&gt;height || video_frame.is_texture() != last_frame_info_-&gt;is_texture) &#123; if ((!last_frame_info_ || video_frame.width() != last_frame_info_-&gt;width || video_frame.height() != last_frame_info_-&gt;height) &amp;&amp; settings_.encoder_switch_request_callback &amp;&amp; encoder_selector_) &#123; if (auto encoder = encoder_selector_-&gt;OnResolutionChange( &#123;video_frame.width(), video_frame.height()&#125;)) &#123; settings_.encoder_switch_request_callback-&gt;RequestEncoderSwitch( *encoder, /*allow_default_fallback=*/false); &#125; &#125; pending_encoder_reconfiguration_ = true; last_frame_info_ = VideoFrameInfo(video_frame.width(), video_frame.height(), video_frame.is_texture()); RTC_LOG(LS_INFO) &lt;&lt; "Video frame parameters changed: dimensions=" &lt;&lt; last_frame_info_-&gt;width &lt;&lt; "x" &lt;&lt; last_frame_info_-&gt;height &lt;&lt; ", texture=" &lt;&lt; last_frame_info_-&gt;is_texture &lt;&lt; "."; // Force full frame update, since resolution has changed. accumulated_update_rect_ = VideoFrame::UpdateRect&#123;0, 0, video_frame.width(), video_frame.height()&#125;; &#125; // We have to create the encoder before the frame drop logic, // because the latter depends on encoder_-&gt;GetScalingSettings. // According to the testcase // InitialFrameDropOffWhenEncoderDisabledScaling, the return value // from GetScalingSettings should enable or disable the frame drop. // Update input frame rate before we start using it. If we update it after // any potential frame drop we are going to artificially increase frame sizes. // Poll the rate before updating, otherwise we risk the rate being estimated // a little too high at the start of the call when then window is small. uint32_t framerate_fps = GetInputFramerateFps(); frame_cadence_adapter_-&gt;UpdateFrameRate(); int64_t now_ms = clock_-&gt;TimeInMilliseconds(); if (pending_encoder_reconfiguration_) &#123; ReconfigureEncoder(); last_parameters_update_ms_.emplace(now_ms); &#125; else if (!last_parameters_update_ms_ || now_ms - *last_parameters_update_ms_ &gt;= kParameterUpdateIntervalMs) &#123; if (last_encoder_rate_settings_) &#123; // Clone rate settings before update, so that SetEncoderRates() will // actually detect the change between the input and // `last_encoder_rate_setings_`, triggering the call to SetRate() on the // encoder. EncoderRateSettings new_rate_settings = *last_encoder_rate_settings_; new_rate_settings.rate_control.framerate_fps = static_cast&lt;double&gt;(framerate_fps); SetEncoderRates(UpdateBitrateAllocation(new_rate_settings)); &#125; last_parameters_update_ms_.emplace(now_ms); &#125; // Because pending frame will be dropped in any case, we need to // remember its updated region. if (pending_frame_) &#123; encoder_stats_observer_-&gt;OnFrameDropped( VideoStreamEncoderObserver::DropReason::kEncoderQueue); accumulated_update_rect_.Union(pending_frame_-&gt;update_rect()); accumulated_update_rect_is_valid_ &amp;= pending_frame_-&gt;has_update_rect(); &#125; if (DropDueToSize(video_frame.size())) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Dropping frame. Too large for target bitrate."; stream_resource_manager_.OnFrameDroppedDueToSize(); // Storing references to a native buffer risks blocking frame capture. if (video_frame.video_frame_buffer()-&gt;type() != VideoFrameBuffer::Type::kNative) &#123; pending_frame_ = video_frame; pending_frame_post_time_us_ = time_when_posted_us; &#125; else &#123; // Ensure that any previously stored frame is dropped. pending_frame_.reset(); accumulated_update_rect_.Union(video_frame.update_rect()); accumulated_update_rect_is_valid_ &amp;= video_frame.has_update_rect(); encoder_stats_observer_-&gt;OnFrameDropped( VideoStreamEncoderObserver::DropReason::kEncoderQueue); &#125; return; &#125; stream_resource_manager_.OnMaybeEncodeFrame(); if (EncoderPaused()) &#123; // Storing references to a native buffer risks blocking frame capture. if (video_frame.video_frame_buffer()-&gt;type() != VideoFrameBuffer::Type::kNative) &#123; if (pending_frame_) TraceFrameDropStart(); pending_frame_ = video_frame; pending_frame_post_time_us_ = time_when_posted_us; &#125; else &#123; // Ensure that any previously stored frame is dropped. pending_frame_.reset(); TraceFrameDropStart(); accumulated_update_rect_.Union(video_frame.update_rect()); accumulated_update_rect_is_valid_ &amp;= video_frame.has_update_rect(); encoder_stats_observer_-&gt;OnFrameDropped( VideoStreamEncoderObserver::DropReason::kEncoderQueue); &#125; return; &#125; pending_frame_.reset(); frame_dropper_.Leak(framerate_fps); // Frame dropping is enabled iff frame dropping is not force-disabled, and // rate controller is not trusted. const bool frame_dropping_enabled = !force_disable_frame_dropper_ &amp;&amp; !encoder_info_.has_trusted_rate_controller; frame_dropper_.Enable(frame_dropping_enabled); if (frame_dropping_enabled &amp;&amp; frame_dropper_.DropFrame()) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Drop Frame: " "target bitrate " &lt;&lt; (last_encoder_rate_settings_ ? last_encoder_rate_settings_-&gt;encoder_target.bps() : 0) &lt;&lt; ", input frame rate " &lt;&lt; framerate_fps; OnDroppedFrame( EncodedImageCallback::DropReason::kDroppedByMediaOptimizations); accumulated_update_rect_.Union(video_frame.update_rect()); accumulated_update_rect_is_valid_ &amp;= video_frame.has_update_rect(); return; &#125; EncodeVideoFrame(video_frame, time_when_posted_us);&#125;void VideoStreamEncoder::EncodeVideoFrame(const VideoFrame&amp; video_frame, int64_t time_when_posted_us) &#123; RTC_DCHECK_RUN_ON(&amp;encoder_queue_); RTC_LOG(LS_VERBOSE) &lt;&lt; __func__ &lt;&lt; " posted " &lt;&lt; time_when_posted_us &lt;&lt; " ntp time " &lt;&lt; video_frame.ntp_time_ms(); // If the encoder fail we can't continue to encode frames. When this happens // the WebrtcVideoSender is notified and the whole VideoSendStream is // recreated. if (encoder_failed_ || !encoder_initialized_) return; // It's possible that EncodeVideoFrame can be called after we've completed // a Stop() operation. Check if the encoder_ is set before continuing. // See: bugs.webrtc.org/12857 if (!encoder_) return; TraceFrameDropEnd(); // Encoder metadata needs to be updated before encode complete callback. VideoEncoder::EncoderInfo info = encoder_-&gt;GetEncoderInfo(); if (info.implementation_name != encoder_info_.implementation_name || info.is_hardware_accelerated != encoder_info_.is_hardware_accelerated) &#123; encoder_stats_observer_-&gt;OnEncoderImplementationChanged(&#123; .name = info.implementation_name, .is_hardware_accelerated = info.is_hardware_accelerated, &#125;); if (bitrate_adjuster_) &#123; // Encoder implementation changed, reset overshoot detector states. bitrate_adjuster_-&gt;Reset(); &#125; &#125; if (encoder_info_ != info) &#123; OnEncoderSettingsChanged(); stream_resource_manager_.ConfigureEncodeUsageResource(); // Re-configure scalers when encoder info changed. Consider two cases: // 1. When the status of the scaler changes from enabled to disabled, if we // don't do this CL, scaler will adapt up/down to trigger an unnecessary // full ReconfigureEncoder() when the scaler should be banned. // 2. When the status of the scaler changes from disabled to enabled, if we // don't do this CL, scaler will not work until some code trigger // ReconfigureEncoder(). In extreme cases, the scaler doesn't even work for // a long time when we expect that the scaler should work. stream_resource_manager_.ConfigureQualityScaler(info); stream_resource_manager_.ConfigureBandwidthQualityScaler(info); RTC_LOG(LS_INFO) &lt;&lt; "Encoder info changed to " &lt;&lt; info.ToString(); &#125; if (bitrate_adjuster_) &#123; for (size_t si = 0; si &lt; kMaxSpatialLayers; ++si) &#123; if (info.fps_allocation[si] != encoder_info_.fps_allocation[si]) &#123; bitrate_adjuster_-&gt;OnEncoderInfo(info); break; &#125; &#125; &#125; encoder_info_ = info; last_encode_info_ms_ = clock_-&gt;TimeInMilliseconds(); VideoFrame out_frame(video_frame); // Crop or scale the frame if needed. Dimension may be reduced to fit encoder // requirements, e.g. some encoders may require them to be divisible by 4. if ((crop_width_ &gt; 0 || crop_height_ &gt; 0) &amp;&amp; (out_frame.video_frame_buffer()-&gt;type() != VideoFrameBuffer::Type::kNative || !info.supports_native_handle)) &#123; int cropped_width = video_frame.width() - crop_width_; int cropped_height = video_frame.height() - crop_height_; rtc::scoped_refptr&lt;VideoFrameBuffer&gt; cropped_buffer; // TODO(ilnik): Remove scaling if cropping is too big, as it should never // happen after SinkWants signaled correctly from ReconfigureEncoder. VideoFrame::UpdateRect update_rect = video_frame.update_rect(); if (crop_width_ &lt; 4 &amp;&amp; crop_height_ &lt; 4) &#123; // The difference is small, crop without scaling. cropped_buffer = video_frame.video_frame_buffer()-&gt;CropAndScale( crop_width_ / 2, crop_height_ / 2, cropped_width, cropped_height, cropped_width, cropped_height); update_rect.offset_x -= crop_width_ / 2; update_rect.offset_y -= crop_height_ / 2; update_rect.Intersect( VideoFrame::UpdateRect&#123;0, 0, cropped_width, cropped_height&#125;); &#125; else &#123; // The difference is large, scale it. cropped_buffer = video_frame.video_frame_buffer()-&gt;Scale(cropped_width, cropped_height); if (!update_rect.IsEmpty()) &#123; // Since we can't reason about pixels after scaling, we invalidate whole // picture, if anything changed. update_rect = VideoFrame::UpdateRect&#123;0, 0, cropped_width, cropped_height&#125;; &#125; &#125; if (!cropped_buffer) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Cropping and scaling frame failed, dropping frame."; return; &#125; out_frame.set_video_frame_buffer(cropped_buffer); out_frame.set_update_rect(update_rect); out_frame.set_ntp_time_ms(video_frame.ntp_time_ms()); // Since accumulated_update_rect_ is constructed before cropping, // we can't trust it. If any changes were pending, we invalidate whole // frame here. if (!accumulated_update_rect_.IsEmpty()) &#123; accumulated_update_rect_ = VideoFrame::UpdateRect&#123;0, 0, out_frame.width(), out_frame.height()&#125;; accumulated_update_rect_is_valid_ = false; &#125; &#125; if (!accumulated_update_rect_is_valid_) &#123; out_frame.clear_update_rect(); &#125; else if (!accumulated_update_rect_.IsEmpty() &amp;&amp; out_frame.has_update_rect()) &#123; accumulated_update_rect_.Union(out_frame.update_rect()); accumulated_update_rect_.Intersect( VideoFrame::UpdateRect&#123;0, 0, out_frame.width(), out_frame.height()&#125;); out_frame.set_update_rect(accumulated_update_rect_); accumulated_update_rect_.MakeEmptyUpdate(); &#125; accumulated_update_rect_is_valid_ = true; TRACE_EVENT_ASYNC_STEP0("webrtc", "Video", video_frame.render_time_ms(), "Encode"); stream_resource_manager_.OnEncodeStarted(out_frame, time_when_posted_us); // The encoder should get the size that it expects. RTC_DCHECK(send_codec_.width &lt;= out_frame.width() &amp;&amp; send_codec_.height &lt;= out_frame.height()) &lt;&lt; "Encoder configured to " &lt;&lt; send_codec_.width &lt;&lt; "x" &lt;&lt; send_codec_.height &lt;&lt; " received a too small frame " &lt;&lt; out_frame.width() &lt;&lt; "x" &lt;&lt; out_frame.height(); TRACE_EVENT1("webrtc", "VCMGenericEncoder::Encode", "timestamp", out_frame.timestamp()); frame_encode_metadata_writer_.OnEncodeStarted(out_frame); const int32_t encode_status = encoder_-&gt;Encode(out_frame, &amp;next_frame_types_); was_encode_called_since_last_initialization_ = true; if (encode_status &lt; 0) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Encoder failed, failing encoder format: " &lt;&lt; encoder_config_.video_format.ToString(); RequestEncoderSwitch(); return; &#125; for (auto&amp; it : next_frame_types_) &#123; it = VideoFrameType::kVideoFrameDelta; &#125;&#125; ReconfigureEncoder() 中根据 encodefactory 创建 encoder encoder_ = settings_.encoder_factory-&gt;CreateVideoEncoder(encoder_config_.video_format); 然后注册 encoder 的 回调 encoder_-&gt;RegisterEncodeCompleteCallback(this); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798EncodedImageCallback::Result VideoStreamEncoder::OnEncodedImage( const EncodedImage&amp; encoded_image, const CodecSpecificInfo* codec_specific_info) &#123; TRACE_EVENT_INSTANT1("webrtc", "VCMEncodedFrameCallback::Encoded", "timestamp", encoded_image.Timestamp()); // TODO(bugs.webrtc.org/10520): Signal the simulcast id explicitly. const size_t spatial_idx = encoded_image.SpatialIndex().value_or(0); const VideoCodecType codec_type = codec_specific_info ? codec_specific_info-&gt;codecType : VideoCodecType::kVideoCodecGeneric; EncodedImage image_copy = AugmentEncodedImage(encoded_image, codec_specific_info); // Post a task because `send_codec_` requires `encoder_queue_` lock and we // need to update on quality convergence. unsigned int image_width = image_copy._encodedWidth; unsigned int image_height = image_copy._encodedHeight; encoder_queue_.PostTask([this, codec_type, image_width, image_height, spatial_idx, at_target_quality = image_copy.IsAtTargetQuality()] &#123; RTC_DCHECK_RUN_ON(&amp;encoder_queue_); // Let the frame cadence adapter know about quality convergence. if (frame_cadence_adapter_) frame_cadence_adapter_-&gt;UpdateLayerQualityConvergence(spatial_idx, at_target_quality); // Currently, the internal quality scaler is used for VP9 instead of the // webrtc qp scaler (in the no-svc case or if only a single spatial layer is // encoded). It has to be explicitly detected and reported to adaptation // metrics. if (codec_type == VideoCodecType::kVideoCodecVP9 &amp;&amp; send_codec_.VP9()-&gt;automaticResizeOn) &#123; unsigned int expected_width = send_codec_.width; unsigned int expected_height = send_codec_.height; int num_active_layers = 0; for (int i = 0; i &lt; send_codec_.VP9()-&gt;numberOfSpatialLayers; ++i) &#123; if (send_codec_.spatialLayers[i].active) &#123; ++num_active_layers; expected_width = send_codec_.spatialLayers[i].width; expected_height = send_codec_.spatialLayers[i].height; &#125; &#125; RTC_DCHECK_LE(num_active_layers, 1) &lt;&lt; "VP9 quality scaling is enabled for " "SVC with several active layers."; encoder_stats_observer_-&gt;OnEncoderInternalScalerUpdate( image_width &lt; expected_width || image_height &lt; expected_height); &#125; &#125;); // Encoded is called on whatever thread the real encoder implementation run // on. In the case of hardware encoders, there might be several encoders // running in parallel on different threads. encoder_stats_observer_-&gt;OnSendEncodedImage(image_copy, codec_specific_info); EncodedImageCallback::Result result = sink_-&gt;OnEncodedImage(image_copy, codec_specific_info); // We are only interested in propagating the meta-data about the image, not // encoded data itself, to the post encode function. Since we cannot be sure // the pointer will still be valid when run on the task queue, set it to null. DataSize frame_size = DataSize::Bytes(image_copy.size()); image_copy.ClearEncodedData(); int temporal_index = 0; if (codec_specific_info) &#123; if (codec_specific_info-&gt;codecType == kVideoCodecVP9) &#123; temporal_index = codec_specific_info-&gt;codecSpecific.VP9.temporal_idx; &#125; else if (codec_specific_info-&gt;codecType == kVideoCodecVP8) &#123; temporal_index = codec_specific_info-&gt;codecSpecific.VP8.temporalIdx; &#125; &#125; if (temporal_index == kNoTemporalIdx) &#123; temporal_index = 0; &#125; RunPostEncode(image_copy, clock_-&gt;CurrentTime().us(), temporal_index, frame_size); if (result.error == Result::OK) &#123; // In case of an internal encoder running on a separate thread, the // decision to drop a frame might be a frame late and signaled via // atomic flag. This is because we can't easily wait for the worker thread // without risking deadlocks, eg during shutdown when the worker thread // might be waiting for the internal encoder threads to stop. if (pending_frame_drops_.load() &gt; 0) &#123; int pending_drops = pending_frame_drops_.fetch_sub(1); RTC_DCHECK_GT(pending_drops, 0); result.drop_next_frame = true; &#125; &#125; return result;&#125; sink_-&gt;OnEncodedImage(image_copy, codec_specific_info); 进到了 VideoSendStreamImpl的OnEncodedImage处理逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091VideoSendStreamImpl::VideoSendStreamImpl( Clock* clock, SendStatisticsProxy* stats_proxy, RtpTransportControllerSendInterface* transport, BitrateAllocatorInterface* bitrate_allocator, VideoStreamEncoderInterface* video_stream_encoder, const VideoSendStream::Config* config, int initial_encoder_max_bitrate, double initial_encoder_bitrate_priority, VideoEncoderConfig::ContentType content_type, RtpVideoSenderInterface* rtp_video_sender, const FieldTrialsView&amp; field_trials) : clock_(clock), has_alr_probing_(config-&gt;periodic_alr_bandwidth_probing || GetAlrSettings(content_type)), pacing_config_(PacingConfig(field_trials)), stats_proxy_(stats_proxy), config_(config), rtp_transport_queue_(transport-&gt;GetWorkerQueue()), timed_out_(false), transport_(transport), bitrate_allocator_(bitrate_allocator), disable_padding_(true), max_padding_bitrate_(0), encoder_min_bitrate_bps_(0), encoder_max_bitrate_bps_( GetInitialEncoderMaxBitrate(initial_encoder_max_bitrate)), encoder_target_rate_bps_(0), encoder_bitrate_priority_(initial_encoder_bitrate_priority), video_stream_encoder_(video_stream_encoder), bandwidth_observer_(transport-&gt;GetBandwidthObserver()), rtp_video_sender_(rtp_video_sender), configured_pacing_factor_( GetConfiguredPacingFactor(*config_, content_type, pacing_config_)) &#123; RTC_DCHECK_GE(config_-&gt;rtp.payload_type, 0); RTC_DCHECK_LE(config_-&gt;rtp.payload_type, 127); RTC_DCHECK(!config_-&gt;rtp.ssrcs.empty()); RTC_DCHECK(transport_); RTC_DCHECK_NE(initial_encoder_max_bitrate, 0); RTC_LOG(LS_INFO) &lt;&lt; "VideoSendStreamImpl: " &lt;&lt; config_-&gt;ToString(); RTC_CHECK(AlrExperimentSettings::MaxOneFieldTrialEnabled()); // Only request rotation at the source when we positively know that the remote // side doesn't support the rotation extension. This allows us to prepare the // encoder in the expectation that rotation is supported - which is the common // case. bool rotation_applied = absl::c_none_of( config_-&gt;rtp.extensions, [](const RtpExtension&amp; extension) &#123; return extension.uri == RtpExtension::kVideoRotationUri; &#125;); video_stream_encoder_-&gt;SetSink(this, rotation_applied); absl::optional&lt;bool&gt; enable_alr_bw_probing; // If send-side BWE is enabled, check if we should apply updated probing and // pacing settings. if (configured_pacing_factor_) &#123; absl::optional&lt;AlrExperimentSettings&gt; alr_settings = GetAlrSettings(content_type); int queue_time_limit_ms; if (alr_settings) &#123; enable_alr_bw_probing = true; queue_time_limit_ms = alr_settings-&gt;max_paced_queue_time; &#125; else &#123; RateControlSettings rate_control_settings = RateControlSettings::ParseFromFieldTrials(); enable_alr_bw_probing = rate_control_settings.UseAlrProbing(); queue_time_limit_ms = pacing_config_.max_pacing_delay.Get().ms(); &#125; transport-&gt;SetQueueTimeLimit(queue_time_limit_ms); &#125; if (config_-&gt;periodic_alr_bandwidth_probing) &#123; enable_alr_bw_probing = config_-&gt;periodic_alr_bandwidth_probing; &#125; if (enable_alr_bw_probing) &#123; transport-&gt;EnablePeriodicAlrProbing(*enable_alr_bw_probing); &#125; rtp_transport_queue_-&gt;RunOrPost(SafeTask(transport_queue_safety_, [this] &#123; if (configured_pacing_factor_) transport_-&gt;SetPacingFactor(*configured_pacing_factor_); video_stream_encoder_-&gt;SetStartBitrate( bitrate_allocator_-&gt;GetStartBitrate(this)); &#125;));&#125; video_stream_encoder_-&gt;SetSink(this, rotation_applied); VideoSendStreamImpl 作为 VideoStreamEncoder 的sink， 接收经过编码之后的图像数据 1234567891011121314151617181920212223242526272829303132.../webrtc/src/video/video_send_stream_impl.ccEncodedImageCallback::Result VideoSendStreamImpl::OnEncodedImage( const EncodedImage&amp; encoded_image, const CodecSpecificInfo* codec_specific_info) &#123; // Encoded is called on whatever thread the real encoder implementation run // on. In the case of hardware encoders, there might be several encoders // running in parallel on different threads. // Indicate that there still is activity going on. activity_ = true; RTC_DCHECK(!rtp_transport_queue_-&gt;IsCurrent()); auto task_to_run_on_worker = [this]() &#123; RTC_DCHECK_RUN_ON(rtp_transport_queue_); if (disable_padding_) &#123; disable_padding_ = false; // To ensure that padding bitrate is propagated to the bitrate allocator. SignalEncoderActive(); &#125; // Check if there's a throttled VideoBitrateAllocation that we should try // sending. auto&amp; context = video_bitrate_allocation_context_; if (context &amp;&amp; context-&gt;throttled_allocation) &#123; OnBitrateAllocationUpdated(*context-&gt;throttled_allocation); &#125; &#125;; rtp_transport_queue_-&gt;TaskQueueForPost()-&gt;PostTask( SafeTask(transport_queue_safety_, std::move(task_to_run_on_worker))); return rtp_video_sender_-&gt;OnEncodedImage(encoded_image, codec_specific_info);&#125; send123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314.../webrtc/src/call/rtp_video_sender.ccRtpVideoSender::RtpVideoSender( Clock* clock, const std::map&lt;uint32_t, RtpState&gt;&amp; suspended_ssrcs, const std::map&lt;uint32_t, RtpPayloadState&gt;&amp; states, const RtpConfig&amp; rtp_config, int rtcp_report_interval_ms, Transport* send_transport, const RtpSenderObservers&amp; observers, RtpTransportControllerSendInterface* transport, RtcEventLog* event_log, RateLimiter* retransmission_limiter, std::unique_ptr&lt;FecController&gt; fec_controller, FrameEncryptorInterface* frame_encryptor, const CryptoOptions&amp; crypto_options, rtc::scoped_refptr&lt;FrameTransformerInterface&gt; frame_transformer, const FieldTrialsView&amp; field_trials, TaskQueueFactory* task_queue_factory) : field_trials_(field_trials), use_frame_rate_for_overhead_(absl::StartsWith( field_trials_.Lookup("WebRTC-Video-UseFrameRateForOverhead"), "Enabled")), has_packet_feedback_(TransportSeqNumExtensionConfigured(rtp_config)), active_(false), fec_controller_(std::move(fec_controller)), fec_allowed_(true), rtp_streams_(CreateRtpStreamSenders(clock, rtp_config, observers, rtcp_report_interval_ms, send_transport, transport-&gt;GetBandwidthObserver(), transport, suspended_ssrcs, event_log, retransmission_limiter, frame_encryptor, crypto_options, std::move(frame_transformer), field_trials_, task_queue_factory)), rtp_config_(rtp_config), codec_type_(GetVideoCodecType(rtp_config)), transport_(transport), transport_overhead_bytes_per_packet_(0), encoder_target_rate_bps_(0), frame_counts_(rtp_config.ssrcs.size()), frame_count_observer_(observers.frame_count_observer) &#123; transport_checker_.Detach(); RTC_DCHECK_EQ(rtp_config_.ssrcs.size(), rtp_streams_.size()); if (has_packet_feedback_) transport_-&gt;IncludeOverheadInPacedSender(); // SSRCs are assumed to be sorted in the same order as `rtp_modules`. for (uint32_t ssrc : rtp_config_.ssrcs) &#123; // Restore state if it previously existed. const RtpPayloadState* state = nullptr; auto it = states.find(ssrc); if (it != states.end()) &#123; state = &amp;it-&gt;second; shared_frame_id_ = std::max(shared_frame_id_, state-&gt;shared_frame_id); &#125; params_.push_back(RtpPayloadParams(ssrc, state, field_trials_)); &#125; // RTP/RTCP initialization. for (size_t i = 0; i &lt; rtp_config_.extensions.size(); ++i) &#123; const std::string&amp; extension = rtp_config_.extensions[i].uri; int id = rtp_config_.extensions[i].id; RTC_DCHECK(RtpExtension::IsSupportedForVideo(extension)); for (const RtpStreamSender&amp; stream : rtp_streams_) &#123; stream.rtp_rtcp-&gt;RegisterRtpHeaderExtension(extension, id); &#125; &#125; ConfigureSsrcs(suspended_ssrcs); if (!rtp_config_.mid.empty()) &#123; for (const RtpStreamSender&amp; stream : rtp_streams_) &#123; stream.rtp_rtcp-&gt;SetMid(rtp_config_.mid); &#125; &#125; bool fec_enabled = false; for (const RtpStreamSender&amp; stream : rtp_streams_) &#123; // Simulcast has one module for each layer. Set the CNAME on all modules. stream.rtp_rtcp-&gt;SetCNAME(rtp_config_.c_name.c_str()); stream.rtp_rtcp-&gt;SetMaxRtpPacketSize(rtp_config_.max_packet_size); stream.rtp_rtcp-&gt;RegisterSendPayloadFrequency(rtp_config_.payload_type, kVideoPayloadTypeFrequency); if (stream.fec_generator != nullptr) &#123; fec_enabled = true; &#125; &#125; // Currently, both ULPFEC and FlexFEC use the same FEC rate calculation logic, // so enable that logic if either of those FEC schemes are enabled. fec_controller_-&gt;SetProtectionMethod(fec_enabled, NackEnabled()); fec_controller_-&gt;SetProtectionCallback(this); // Construction happens on the worker thread (see Call::CreateVideoSendStream) // but subseqeuent calls to the RTP state will happen on one of two threads: // * The pacer thread for actually sending packets. // * The transport thread when tearing down and quering GetRtpState(). // Detach thread checkers. for (const RtpStreamSender&amp; stream : rtp_streams_) &#123; stream.rtp_rtcp-&gt;OnPacketSendingThreadSwitched(); &#125;&#125;std::vector&lt;RtpStreamSender&gt; CreateRtpStreamSenders( Clock* clock, const RtpConfig&amp; rtp_config, const RtpSenderObservers&amp; observers, int rtcp_report_interval_ms, Transport* send_transport, RtcpBandwidthObserver* bandwidth_callback, RtpTransportControllerSendInterface* transport, const std::map&lt;uint32_t, RtpState&gt;&amp; suspended_ssrcs, RtcEventLog* event_log, RateLimiter* retransmission_rate_limiter, FrameEncryptorInterface* frame_encryptor, const CryptoOptions&amp; crypto_options, rtc::scoped_refptr&lt;FrameTransformerInterface&gt; frame_transformer, const FieldTrialsView&amp; trials, TaskQueueFactory* task_queue_factory) &#123; RTC_DCHECK_GT(rtp_config.ssrcs.size(), 0); RTC_DCHECK(task_queue_factory); RtpRtcpInterface::Configuration configuration; configuration.clock = clock; configuration.audio = false; configuration.receiver_only = false; configuration.outgoing_transport = send_transport; configuration.intra_frame_callback = observers.intra_frame_callback; configuration.rtcp_loss_notification_observer = observers.rtcp_loss_notification_observer; configuration.bandwidth_callback = bandwidth_callback; configuration.network_state_estimate_observer = transport-&gt;network_state_estimate_observer(); configuration.transport_feedback_callback = transport-&gt;transport_feedback_observer(); configuration.rtt_stats = observers.rtcp_rtt_stats; configuration.rtcp_packet_type_counter_observer = observers.rtcp_type_observer; configuration.report_block_data_observer = observers.report_block_data_observer; configuration.paced_sender = transport-&gt;packet_sender(); configuration.send_bitrate_observer = observers.bitrate_observer; configuration.send_side_delay_observer = observers.send_delay_observer; configuration.send_packet_observer = observers.send_packet_observer; configuration.event_log = event_log; configuration.retransmission_rate_limiter = retransmission_rate_limiter; configuration.rtp_stats_callback = observers.rtp_stats; configuration.frame_encryptor = frame_encryptor; configuration.require_frame_encryption = crypto_options.sframe.require_frame_encryption; configuration.extmap_allow_mixed = rtp_config.extmap_allow_mixed; configuration.rtcp_report_interval_ms = rtcp_report_interval_ms; configuration.field_trials = &amp;trials; std::vector&lt;RtpStreamSender&gt; rtp_streams; RTC_DCHECK(rtp_config.rtx.ssrcs.empty() || rtp_config.rtx.ssrcs.size() == rtp_config.ssrcs.size()); // Some streams could have been disabled, but the rids are still there. // This will occur when simulcast has been disabled for a codec (e.g. VP9) RTC_DCHECK(rtp_config.rids.empty() || rtp_config.rids.size() &gt;= rtp_config.ssrcs.size()); for (size_t i = 0; i &lt; rtp_config.ssrcs.size(); ++i) &#123; RTPSenderVideo::Config video_config; configuration.local_media_ssrc = rtp_config.ssrcs[i]; std::unique_ptr&lt;VideoFecGenerator&gt; fec_generator = MaybeCreateFecGenerator(clock, rtp_config, suspended_ssrcs, i, trials); configuration.fec_generator = fec_generator.get(); configuration.rtx_send_ssrc = rtp_config.GetRtxSsrcAssociatedWithMediaSsrc(rtp_config.ssrcs[i]); RTC_DCHECK_EQ(configuration.rtx_send_ssrc.has_value(), !rtp_config.rtx.ssrcs.empty()); configuration.rid = (i &lt; rtp_config.rids.size()) ? rtp_config.rids[i] : ""; configuration.need_rtp_packet_infos = rtp_config.lntf.enabled; std::unique_ptr&lt;ModuleRtpRtcpImpl2&gt; rtp_rtcp( ModuleRtpRtcpImpl2::Create(configuration)); rtp_rtcp-&gt;SetSendingStatus(false); rtp_rtcp-&gt;SetSendingMediaStatus(false); rtp_rtcp-&gt;SetRTCPStatus(RtcpMode::kCompound); // Set NACK. rtp_rtcp-&gt;SetStorePacketsStatus(true, kMinSendSidePacketHistorySize); video_config.clock = configuration.clock; video_config.rtp_sender = rtp_rtcp-&gt;RtpSender(); video_config.frame_encryptor = frame_encryptor; video_config.require_frame_encryption = crypto_options.sframe.require_frame_encryption; video_config.enable_retransmit_all_layers = false; video_config.field_trials = &amp;trials; const bool using_flexfec = fec_generator &amp;&amp; fec_generator-&gt;GetFecType() == VideoFecGenerator::FecType::kFlexFec; const bool should_disable_red_and_ulpfec = ShouldDisableRedAndUlpfec(using_flexfec, rtp_config, trials); if (!should_disable_red_and_ulpfec &amp;&amp; rtp_config.ulpfec.red_payload_type != -1) &#123; video_config.red_payload_type = rtp_config.ulpfec.red_payload_type; &#125; if (fec_generator) &#123; video_config.fec_type = fec_generator-&gt;GetFecType(); video_config.fec_overhead_bytes = fec_generator-&gt;MaxPacketOverhead(); &#125; video_config.frame_transformer = frame_transformer; video_config.task_queue_factory = task_queue_factory; auto sender_video = std::make_unique&lt;RTPSenderVideo&gt;(video_config); rtp_streams.emplace_back(std::move(rtp_rtcp), std::move(sender_video), std::move(fec_generator)); &#125; return rtp_streams;&#125;EncodedImageCallback::Result RtpVideoSender::OnEncodedImage( const EncodedImage&amp; encoded_image, const CodecSpecificInfo* codec_specific_info) &#123; fec_controller_-&gt;UpdateWithEncodedData(encoded_image.size(), encoded_image._frameType); MutexLock lock(&amp;mutex_); RTC_DCHECK(!rtp_streams_.empty()); if (!active_) return Result(Result::ERROR_SEND_FAILED); shared_frame_id_++; size_t stream_index = 0; if (codec_specific_info &amp;&amp; (codec_specific_info-&gt;codecType == kVideoCodecVP8 || codec_specific_info-&gt;codecType == kVideoCodecH264 || codec_specific_info-&gt;codecType == kVideoCodecGeneric)) &#123; // Map spatial index to simulcast. stream_index = encoded_image.SpatialIndex().value_or(0); &#125; RTC_DCHECK_LT(stream_index, rtp_streams_.size()); uint32_t rtp_timestamp = encoded_image.Timestamp() + rtp_streams_[stream_index].rtp_rtcp-&gt;StartTimestamp(); // RTCPSender has it's own copy of the timestamp offset, added in // RTCPSender::BuildSR, hence we must not add the in the offset for this call. // TODO(nisse): Delete RTCPSender:timestamp_offset_, and see if we can confine // knowledge of the offset to a single place. if (!rtp_streams_[stream_index].rtp_rtcp-&gt;OnSendingRtpFrame( encoded_image.Timestamp(), encoded_image.capture_time_ms_, rtp_config_.payload_type, encoded_image._frameType == VideoFrameType::kVideoFrameKey)) &#123; // The payload router could be active but this module isn't sending. return Result(Result::ERROR_SEND_FAILED); &#125; absl::optional&lt;int64_t&gt; expected_retransmission_time_ms; if (encoded_image.RetransmissionAllowed()) &#123; expected_retransmission_time_ms = rtp_streams_[stream_index].rtp_rtcp-&gt;ExpectedRetransmissionTimeMs(); &#125; if (IsFirstFrameOfACodedVideoSequence(encoded_image, codec_specific_info)) &#123; // In order to use the dependency descriptor RTP header extension: // - Pass along any `FrameDependencyStructure` templates produced by the // encoder adapter. // - If none were produced the `RtpPayloadParams::*ToGeneric` for the // particular codec have simulated a dependency structure, so provide a // minimal set of templates. // - Otherwise, don't pass along any templates at all which will disable // the generation of a dependency descriptor. RTPSenderVideo&amp; sender_video = *rtp_streams_[stream_index].sender_video; if (codec_specific_info &amp;&amp; codec_specific_info-&gt;template_structure) &#123; sender_video.SetVideoStructure(&amp;*codec_specific_info-&gt;template_structure); &#125; else if (absl::optional&lt;FrameDependencyStructure&gt; structure = params_[stream_index].GenericStructure( codec_specific_info)) &#123; sender_video.SetVideoStructure(&amp;*structure); &#125; else &#123; sender_video.SetVideoStructure(nullptr); &#125; &#125; bool send_result = rtp_streams_[stream_index].sender_video-&gt;SendEncodedImage( rtp_config_.payload_type, codec_type_, rtp_timestamp, encoded_image, params_[stream_index].GetRtpVideoHeader( encoded_image, codec_specific_info, shared_frame_id_), expected_retransmission_time_ms); if (frame_count_observer_) &#123; FrameCounts&amp; counts = frame_counts_[stream_index]; if (encoded_image._frameType == VideoFrameType::kVideoFrameKey) &#123; ++counts.key_frames; &#125; else if (encoded_image._frameType == VideoFrameType::kVideoFrameDelta) &#123; ++counts.delta_frames; &#125; else &#123; RTC_DCHECK(encoded_image._frameType == VideoFrameType::kEmptyFrame); &#125; frame_count_observer_-&gt;FrameCountUpdated(counts, rtp_config_.ssrcs[stream_index]); &#125; if (!send_result) return Result(Result::ERROR_SEND_FAILED); return Result(Result::OK, rtp_timestamp);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382.../webrtc/src/modules/rtp_rtcp/source/rtp_sender_video.ccRTPSenderVideo::RTPSenderVideo(const Config&amp; config) : rtp_sender_(config.rtp_sender), clock_(config.clock), retransmission_settings_( config.enable_retransmit_all_layers ? kRetransmitAllLayers : (kRetransmitBaseLayer | kConditionallyRetransmitHigherLayers)), last_rotation_(kVideoRotation_0), transmit_color_space_next_frame_(false), send_allocation_(SendVideoLayersAllocation::kDontSend), current_playout_delay_&#123;-1, -1&#125;, playout_delay_pending_(false), forced_playout_delay_(LoadVideoPlayoutDelayOverride(config.field_trials)), red_payload_type_(config.red_payload_type), fec_type_(config.fec_type), fec_overhead_bytes_(config.fec_overhead_bytes), packetization_overhead_bitrate_(1000, RateStatistics::kBpsScale), frame_encryptor_(config.frame_encryptor), require_frame_encryption_(config.require_frame_encryption), generic_descriptor_auth_experiment_(!absl::StartsWith( config.field_trials-&gt;Lookup("WebRTC-GenericDescriptorAuth"), "Disabled")), absolute_capture_time_sender_(config.clock), frame_transformer_delegate_( config.frame_transformer ? rtc::make_ref_counted&lt;RTPSenderVideoFrameTransformerDelegate&gt;( this, config.frame_transformer, rtp_sender_-&gt;SSRC(), rtp_sender_-&gt;Csrcs(), config.task_queue_factory) : nullptr), include_capture_clock_offset_(!absl::StartsWith( config.field_trials-&gt;Lookup(kIncludeCaptureClockOffset), "Disabled")) &#123; if (frame_transformer_delegate_) frame_transformer_delegate_-&gt;Init();&#125;bool RTPSenderVideo::SendEncodedImage( int payload_type, absl::optional&lt;VideoCodecType&gt; codec_type, uint32_t rtp_timestamp, const EncodedImage&amp; encoded_image, RTPVideoHeader video_header, absl::optional&lt;int64_t&gt; expected_retransmission_time_ms) &#123; if (frame_transformer_delegate_) &#123; // The frame will be sent async once transformed. return frame_transformer_delegate_-&gt;TransformFrame( payload_type, codec_type, rtp_timestamp, encoded_image, video_header, expected_retransmission_time_ms); &#125; return SendVideo(payload_type, codec_type, rtp_timestamp, encoded_image.capture_time_ms_, encoded_image, video_header, expected_retransmission_time_ms, rtp_sender_-&gt;Csrcs());&#125;bool RTPSenderVideo::SendVideo( int payload_type, absl::optional&lt;VideoCodecType&gt; codec_type, uint32_t rtp_timestamp, int64_t capture_time_ms, rtc::ArrayView&lt;const uint8_t&gt; payload, RTPVideoHeader video_header, absl::optional&lt;int64_t&gt; expected_retransmission_time_ms, std::vector&lt;uint32_t&gt; csrcs) &#123; TRACE_EVENT_ASYNC_STEP1("webrtc", "Video", capture_time_ms, "Send", "type", FrameTypeToString(video_header.frame_type)); RTC_CHECK_RUNS_SERIALIZED(&amp;send_checker_); if (video_header.frame_type == VideoFrameType::kEmptyFrame) return true; if (payload.empty()) return false; if (!rtp_sender_-&gt;SendingMedia()) &#123; return false; &#125; int32_t retransmission_settings = retransmission_settings_; if (codec_type == VideoCodecType::kVideoCodecH264) &#123; // Backward compatibility for older receivers without temporal layer logic. retransmission_settings = kRetransmitBaseLayer | kRetransmitHigherLayers; &#125; MaybeUpdateCurrentPlayoutDelay(video_header); if (video_header.frame_type == VideoFrameType::kVideoFrameKey) &#123; if (!IsNoopDelay(current_playout_delay_)) &#123; // Force playout delay on key-frames, if set. playout_delay_pending_ = true; &#125; if (allocation_) &#123; // Send the bitrate allocation on every key frame. send_allocation_ = SendVideoLayersAllocation::kSendWithResolution; &#125; &#125; if (video_structure_ != nullptr &amp;&amp; video_header.generic) &#123; active_decode_targets_tracker_.OnFrame( video_structure_-&gt;decode_target_protected_by_chain, video_header.generic-&gt;active_decode_targets, video_header.frame_type == VideoFrameType::kVideoFrameKey, video_header.generic-&gt;frame_id, video_header.generic-&gt;chain_diffs); &#125; const uint8_t temporal_id = GetTemporalId(video_header); // No FEC protection for upper temporal layers, if used. const bool use_fec = fec_type_.has_value() &amp;&amp; (temporal_id == 0 || temporal_id == kNoTemporalIdx); // Maximum size of packet including rtp headers. // Extra space left in case packet will be resent using fec or rtx. int packet_capacity = rtp_sender_-&gt;MaxRtpPacketSize() - (use_fec ? FecPacketOverhead() : 0) - (rtp_sender_-&gt;RtxStatus() ? kRtxHeaderSize : 0); absl::optional&lt;Timestamp&gt; capture_time; if (capture_time_ms &gt; 0) &#123; capture_time = Timestamp::Millis(capture_time_ms); &#125; rtp_sender_-&gt;SetCsrcs(std::move(csrcs)); std::unique_ptr&lt;RtpPacketToSend&gt; single_packet = rtp_sender_-&gt;AllocatePacket(); RTC_DCHECK_LE(packet_capacity, single_packet-&gt;capacity()); single_packet-&gt;SetPayloadType(payload_type); single_packet-&gt;SetTimestamp(rtp_timestamp); if (capture_time) single_packet-&gt;set_capture_time(*capture_time); // Construct the absolute capture time extension if not provided. if (!video_header.absolute_capture_time.has_value() &amp;&amp; capture_time.has_value()) &#123; video_header.absolute_capture_time.emplace(); video_header.absolute_capture_time-&gt;absolute_capture_timestamp = Int64MsToUQ32x32( clock_-&gt;ConvertTimestampToNtpTime(*capture_time).ToMs()); if (include_capture_clock_offset_) &#123; video_header.absolute_capture_time-&gt;estimated_capture_clock_offset = 0; &#125; &#125; // Let `absolute_capture_time_sender_` decide if the extension should be sent. if (video_header.absolute_capture_time.has_value()) &#123; video_header.absolute_capture_time = absolute_capture_time_sender_.OnSendPacket( AbsoluteCaptureTimeSender::GetSource(single_packet-&gt;Ssrc(), single_packet-&gt;Csrcs()), single_packet-&gt;Timestamp(), kVideoPayloadTypeFrequency, video_header.absolute_capture_time-&gt;absolute_capture_timestamp, video_header.absolute_capture_time-&gt;estimated_capture_clock_offset); &#125; auto first_packet = std::make_unique&lt;RtpPacketToSend&gt;(*single_packet); auto middle_packet = std::make_unique&lt;RtpPacketToSend&gt;(*single_packet); auto last_packet = std::make_unique&lt;RtpPacketToSend&gt;(*single_packet); // Simplest way to estimate how much extensions would occupy is to set them. AddRtpHeaderExtensions(video_header, /*first_packet=*/true, /*last_packet=*/true, single_packet.get()); if (video_structure_ != nullptr &amp;&amp; single_packet-&gt;IsRegistered&lt;RtpDependencyDescriptorExtension&gt;() &amp;&amp; !single_packet-&gt;HasExtension&lt;RtpDependencyDescriptorExtension&gt;()) &#123; RTC_DCHECK_EQ(video_header.frame_type, VideoFrameType::kVideoFrameKey); // Disable attaching dependency descriptor to delta packets (including // non-first packet of a key frame) when it wasn't attached to a key frame, // as dependency descriptor can't be usable in such case. RTC_LOG(LS_WARNING) &lt;&lt; "Disable dependency descriptor because failed to " "attach it to a key frame."; video_structure_ = nullptr; &#125; AddRtpHeaderExtensions(video_header, /*first_packet=*/true, /*last_packet=*/false, first_packet.get()); AddRtpHeaderExtensions(video_header, /*first_packet=*/false, /*last_packet=*/false, middle_packet.get()); AddRtpHeaderExtensions(video_header, /*first_packet=*/false, /*last_packet=*/true, last_packet.get()); RTC_DCHECK_GT(packet_capacity, single_packet-&gt;headers_size()); RTC_DCHECK_GT(packet_capacity, first_packet-&gt;headers_size()); RTC_DCHECK_GT(packet_capacity, middle_packet-&gt;headers_size()); RTC_DCHECK_GT(packet_capacity, last_packet-&gt;headers_size()); RtpPacketizer::PayloadSizeLimits limits; limits.max_payload_len = packet_capacity - middle_packet-&gt;headers_size(); RTC_DCHECK_GE(single_packet-&gt;headers_size(), middle_packet-&gt;headers_size()); limits.single_packet_reduction_len = single_packet-&gt;headers_size() - middle_packet-&gt;headers_size(); RTC_DCHECK_GE(first_packet-&gt;headers_size(), middle_packet-&gt;headers_size()); limits.first_packet_reduction_len = first_packet-&gt;headers_size() - middle_packet-&gt;headers_size(); RTC_DCHECK_GE(last_packet-&gt;headers_size(), middle_packet-&gt;headers_size()); limits.last_packet_reduction_len = last_packet-&gt;headers_size() - middle_packet-&gt;headers_size(); bool has_generic_descriptor = first_packet-&gt;HasExtension&lt;RtpGenericFrameDescriptorExtension00&gt;() || first_packet-&gt;HasExtension&lt;RtpDependencyDescriptorExtension&gt;(); // Minimization of the vp8 descriptor may erase temporal_id, so use // `temporal_id` rather than reference `video_header` beyond this point. if (has_generic_descriptor) &#123; MinimizeDescriptor(&amp;video_header); &#125; // TODO(benwright@webrtc.org) - Allocate enough to always encrypt inline. rtc::Buffer encrypted_video_payload; if (frame_encryptor_ != nullptr) &#123; const size_t max_ciphertext_size = frame_encryptor_-&gt;GetMaxCiphertextByteSize(cricket::MEDIA_TYPE_VIDEO, payload.size()); encrypted_video_payload.SetSize(max_ciphertext_size); size_t bytes_written = 0; // Enable header authentication if the field trial isn't disabled. std::vector&lt;uint8_t&gt; additional_data; if (generic_descriptor_auth_experiment_) &#123; additional_data = RtpDescriptorAuthentication(video_header); &#125; if (frame_encryptor_-&gt;Encrypt( cricket::MEDIA_TYPE_VIDEO, first_packet-&gt;Ssrc(), additional_data, payload, encrypted_video_payload, &amp;bytes_written) != 0) &#123; return false; &#125; encrypted_video_payload.SetSize(bytes_written); payload = encrypted_video_payload; &#125; else if (require_frame_encryption_) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "No FrameEncryptor is attached to this video sending stream but " "one is required since require_frame_encryptor is set"; &#125; std::unique_ptr&lt;RtpPacketizer&gt; packetizer = RtpPacketizer::Create(codec_type, payload, limits, video_header); // TODO(bugs.webrtc.org/10714): retransmission_settings_ should generally be // replaced by expected_retransmission_time_ms.has_value(). For now, though, // only VP8 with an injected frame buffer controller actually controls it. const bool allow_retransmission = expected_retransmission_time_ms.has_value() ? AllowRetransmission(temporal_id, retransmission_settings, expected_retransmission_time_ms.value()) : false; const size_t num_packets = packetizer-&gt;NumPackets(); if (num_packets == 0) return false; bool first_frame = first_frame_sent_(); std::vector&lt;std::unique_ptr&lt;RtpPacketToSend&gt;&gt; rtp_packets; for (size_t i = 0; i &lt; num_packets; ++i) &#123; std::unique_ptr&lt;RtpPacketToSend&gt; packet; int expected_payload_capacity; // Choose right packet template: if (num_packets == 1) &#123; packet = std::move(single_packet); expected_payload_capacity = limits.max_payload_len - limits.single_packet_reduction_len; &#125; else if (i == 0) &#123; packet = std::move(first_packet); expected_payload_capacity = limits.max_payload_len - limits.first_packet_reduction_len; &#125; else if (i == num_packets - 1) &#123; packet = std::move(last_packet); expected_payload_capacity = limits.max_payload_len - limits.last_packet_reduction_len; &#125; else &#123; packet = std::make_unique&lt;RtpPacketToSend&gt;(*middle_packet); expected_payload_capacity = limits.max_payload_len; &#125; packet-&gt;set_first_packet_of_frame(i == 0); if (!packetizer-&gt;NextPacket(packet.get())) return false; RTC_DCHECK_LE(packet-&gt;payload_size(), expected_payload_capacity); packet-&gt;set_allow_retransmission(allow_retransmission); packet-&gt;set_is_key_frame(video_header.frame_type == VideoFrameType::kVideoFrameKey); // Put packetization finish timestamp into extension. if (packet-&gt;HasExtension&lt;VideoTimingExtension&gt;()) &#123; packet-&gt;set_packetization_finish_time(clock_-&gt;CurrentTime()); &#125; packet-&gt;set_fec_protect_packet(use_fec); if (red_enabled()) &#123; // TODO(sprang): Consider packetizing directly into packets with the RED // header already in place, to avoid this copy. std::unique_ptr&lt;RtpPacketToSend&gt; red_packet(new RtpPacketToSend(*packet)); BuildRedPayload(*packet, red_packet.get()); red_packet-&gt;SetPayloadType(*red_payload_type_); red_packet-&gt;set_is_red(true); // Append `red_packet` instead of `packet` to output. red_packet-&gt;set_packet_type(RtpPacketMediaType::kVideo); red_packet-&gt;set_allow_retransmission(packet-&gt;allow_retransmission()); rtp_packets.emplace_back(std::move(red_packet)); &#125; else &#123; packet-&gt;set_packet_type(RtpPacketMediaType::kVideo); rtp_packets.emplace_back(std::move(packet)); &#125; if (first_frame) &#123; if (i == 0) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Sent first RTP packet of the first video frame (pre-pacer)"; &#125; if (i == num_packets - 1) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Sent last RTP packet of the first video frame (pre-pacer)"; &#125; &#125; &#125; LogAndSendToNetwork(std::move(rtp_packets), payload.size()); // Update details about the last sent frame. last_rotation_ = video_header.rotation; if (video_header.color_space != last_color_space_) &#123; last_color_space_ = video_header.color_space; transmit_color_space_next_frame_ = !IsBaseLayer(video_header); &#125; else &#123; transmit_color_space_next_frame_ = transmit_color_space_next_frame_ ? !IsBaseLayer(video_header) : false; &#125; if (video_header.frame_type == VideoFrameType::kVideoFrameKey || PacketWillLikelyBeRequestedForRestransmitionIfLost(video_header)) &#123; // This frame will likely be delivered, no need to populate playout // delay extensions until it changes again. playout_delay_pending_ = false; if (send_allocation_ == SendVideoLayersAllocation::kSendWithResolution) &#123; last_full_sent_allocation_ = allocation_; &#125; send_allocation_ = SendVideoLayersAllocation::kDontSend; &#125; TRACE_EVENT_ASYNC_END1("webrtc", "Video", capture_time_ms, "timestamp", rtp_timestamp); return true;&#125;void RTPSenderVideo::LogAndSendToNetwork( std::vector&lt;std::unique_ptr&lt;RtpPacketToSend&gt;&gt; packets, size_t unpacketized_payload_size) &#123; &#123; MutexLock lock(&amp;stats_mutex_); size_t packetized_payload_size = 0; for (const auto&amp; packet : packets) &#123; if (*packet-&gt;packet_type() == RtpPacketMediaType::kVideo) &#123; packetized_payload_size += packet-&gt;payload_size(); &#125; &#125; // AV1 and H264 packetizers may produce less packetized bytes than // unpacketized. if (packetized_payload_size &gt;= unpacketized_payload_size) &#123; packetization_overhead_bitrate_.Update( packetized_payload_size - unpacketized_payload_size, clock_-&gt;TimeInMilliseconds()); &#125; &#125; rtp_sender_-&gt;EnqueuePackets(std::move(packets));&#125; 1234567891011121314151617.../webrtc/src/modules/rtp_rtcp/source/rtp_sender.ccvoid RTPSender::EnqueuePackets( std::vector&lt;std::unique_ptr&lt;RtpPacketToSend&gt;&gt; packets) &#123; RTC_DCHECK(!packets.empty()); Timestamp now = clock_-&gt;CurrentTime(); for (auto&amp; packet : packets) &#123; RTC_DCHECK(packet); RTC_CHECK(packet-&gt;packet_type().has_value()) &lt;&lt; "Packet type must be set before sending."; if (packet-&gt;capture_time() &lt;= Timestamp::Zero()) &#123; packet-&gt;set_capture_time(now); &#125; &#125; paced_sender_-&gt;EnqueuePackets(std::move(packets));&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758.../webrtc/src/modules/rtp_rtcp/source/rtp_rtcp_impl2.ccModuleRtpRtcpImpl2::ModuleRtpRtcpImpl2(const Configuration&amp; configuration) : worker_queue_(TaskQueueBase::Current()), rtcp_sender_(AddRtcpSendEvaluationCallback( RTCPSender::Configuration::FromRtpRtcpConfiguration(configuration), [this](TimeDelta duration) &#123; ScheduleRtcpSendEvaluation(duration); &#125;)), rtcp_receiver_(configuration, this), clock_(configuration.clock), packet_overhead_(28), // IPV4 UDP. nack_last_time_sent_full_ms_(0), nack_last_seq_number_sent_(0), rtt_stats_(configuration.rtt_stats), rtt_ms_(0) &#123; RTC_DCHECK(worker_queue_); rtcp_thread_checker_.Detach(); if (!configuration.receiver_only) &#123; rtp_sender_ = std::make_unique&lt;RtpSenderContext&gt;(configuration); rtp_sender_-&gt;sequencing_checker.Detach(); // Make sure rtcp sender use same timestamp offset as rtp sender. rtcp_sender_.SetTimestampOffset( rtp_sender_-&gt;packet_generator.TimestampOffset()); rtp_sender_-&gt;packet_sender.SetTimestampOffset( rtp_sender_-&gt;packet_generator.TimestampOffset()); &#125; // Set default packet size limit. // TODO(nisse): Kind-of duplicates // webrtc::VideoSendStream::Config::Rtp::kDefaultMaxPacketSize. const size_t kTcpOverIpv4HeaderSize = 40; SetMaxRtpPacketSize(IP_PACKET_SIZE - kTcpOverIpv4HeaderSize); rtt_update_task_ = RepeatingTaskHandle::DelayedStart( worker_queue_, kRttUpdateInterval, [this]() &#123; PeriodicUpdate(); return kRttUpdateInterval; &#125;);&#125;ModuleRtpRtcpImpl2::RtpSenderContext::RtpSenderContext( const RtpRtcpInterface::Configuration&amp; config) : packet_history(config.clock, config.enable_rtx_padding_prioritization), sequencer(config.local_media_ssrc, config.rtx_send_ssrc, /*require_marker_before_media_padding=*/!config.audio, config.clock), packet_sender(config, &amp;packet_history), non_paced_sender(&amp;packet_sender, &amp;sequencer), packet_generator( config, &amp;packet_history, config.paced_sender ? config.paced_sender : &amp;non_paced_sender) &#123;&#125;RTPSender* ModuleRtpRtcpImpl2::RtpSender() &#123; return rtp_sender_ ? &amp;rtp_sender_-&gt;packet_generator : nullptr;&#125; paced_sender_ 是在 RtpTransportControllerSend中创建的 pacer_ 是 TaskQueuePacedSender 类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101.../src/modules/pacing/task_queue_paced_sender.ccvoid TaskQueuePacedSender::EnqueuePackets( std::vector&lt;std::unique_ptr&lt;RtpPacketToSend&gt;&gt; packets) &#123; task_queue_.TaskQueueForPost()-&gt;PostTask(task_queue_.MaybeSafeTask( safety_.flag(), [this, packets = std::move(packets)]() mutable &#123; RTC_DCHECK_RUN_ON(&amp;task_queue_); TRACE_EVENT0(TRACE_DISABLED_BY_DEFAULT("webrtc"), "TaskQueuePacedSender::EnqueuePackets"); for (auto&amp; packet : packets) &#123; TRACE_EVENT2(TRACE_DISABLED_BY_DEFAULT("webrtc"), "TaskQueuePacedSender::EnqueuePackets::Loop", "sequence_number", packet-&gt;SequenceNumber(), "rtp_timestamp", packet-&gt;Timestamp()); size_t packet_size = packet-&gt;payload_size() + packet-&gt;padding_size(); if (include_overhead_) &#123; packet_size += packet-&gt;headers_size(); &#125; packet_size_.Apply(1, packet_size); RTC_DCHECK_GE(packet-&gt;capture_time(), Timestamp::Zero()); pacing_controller_.EnqueuePacket(std::move(packet)); &#125; MaybeProcessPackets(Timestamp::MinusInfinity()); &#125;));&#125;void TaskQueuePacedSender::MaybeProcessPackets( Timestamp scheduled_process_time) &#123; RTC_DCHECK_RUN_ON(&amp;task_queue_); TRACE_EVENT0(TRACE_DISABLED_BY_DEFAULT("webrtc"), "TaskQueuePacedSender::MaybeProcessPackets"); if (is_shutdown_ || !is_started_) &#123; return; &#125; Timestamp next_send_time = pacing_controller_.NextSendTime(); RTC_DCHECK(next_send_time.IsFinite()); const Timestamp now = clock_-&gt;CurrentTime(); TimeDelta early_execute_margin = pacing_controller_.IsProbing() ? PacingController::kMaxEarlyProbeProcessing : TimeDelta::Zero(); // Process packets and update stats. while (next_send_time &lt;= now + early_execute_margin) &#123; pacing_controller_.ProcessPackets(); next_send_time = pacing_controller_.NextSendTime(); RTC_DCHECK(next_send_time.IsFinite()); // Probing state could change. Get margin after process packets. early_execute_margin = pacing_controller_.IsProbing() ? PacingController::kMaxEarlyProbeProcessing : TimeDelta::Zero(); &#125; UpdateStats(); // Ignore retired scheduled task, otherwise reset `next_process_time_`. if (scheduled_process_time.IsFinite()) &#123; if (scheduled_process_time != next_process_time_) &#123; return; &#125; next_process_time_ = Timestamp::MinusInfinity(); &#125; // Do not hold back in probing. TimeDelta hold_back_window = TimeDelta::Zero(); if (!pacing_controller_.IsProbing()) &#123; hold_back_window = max_hold_back_window_; DataRate pacing_rate = pacing_controller_.pacing_rate(); if (max_hold_back_window_in_packets_ != kNoPacketHoldback &amp;&amp; !pacing_rate.IsZero() &amp;&amp; packet_size_.filtered() != rtc::ExpFilter::kValueUndefined) &#123; TimeDelta avg_packet_send_time = DataSize::Bytes(packet_size_.filtered()) / pacing_rate; hold_back_window = std::min(hold_back_window, avg_packet_send_time * max_hold_back_window_in_packets_); &#125; &#125; // Calculate next process time. TimeDelta time_to_next_process = std::max(hold_back_window, next_send_time - now - early_execute_margin); next_send_time = now + time_to_next_process; // If no in flight task or in flight task is later than `next_send_time`, // schedule a new one. Previous in flight task will be retired. if (next_process_time_.IsMinusInfinity() || next_process_time_ &gt; next_send_time) &#123; // Prefer low precision if allowed and not probing. task_queue_.TaskQueueForDelayedTasks()-&gt;PostDelayedHighPrecisionTask( task_queue_.MaybeSafeTask( safety_.flag(), [this, next_send_time]() &#123; MaybeProcessPackets(next_send_time); &#125;), time_to_next_process.RoundUpTo(TimeDelta::Millis(1))); next_process_time_ = next_send_time; &#125;&#125; PacingController::ProcessPackets -&gt; PacketRouter::SendPacket -&gt; ModuleRtpRtcpImpl2::TrySendPacket -&gt; RtpSenderEgress::SendPacket -&gt; RtpSenderEgress::SendPacketToNetwork -&gt; DegradedCall::FakeNetworkPipeTransportAdapter::SendRtp -&gt; DegradedCall::FakeNetworkPipeOnTaskQueue::SendRtp -&gt; FakeNetworkPipe::DeliverNetworkPacket -&gt; WebRtcVideoChannel::SendRtp -&gt; MediaChannel::SendRtp -&gt; MediaChannel::DoSendPacket -&gt; BaseChannel::SendPacket -&gt; RtpTransport::SendPacket -&gt; rtp_transport_ 就是在 分析candidate文章中 创建的那个 。 P2PTransportChannel::SendPacket -&gt; ProxyConnection::Send -&gt; UDPPort::SendTo -&gt; Socket -&gt; send 到这里 图中的 左半部分 send的过程 结束了 receive…RtpTransport::OnReadPacket-&gt;RtpTransport::OnRtpPacketReceived -&gt;…BaseChannel::OnRtpPacket -&gt;MediaChannel::OnPacketReceived -&gt;… receive 就是 send 逆向的过程,不重复了 over ….]]></content>
      <categories>
        <category>RTC</category>
      </categories>
      <tags>
        <tag>ios</tag>
        <tag>webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc-candidate源码分析]]></title>
    <url>%2F2023%2F02%2F23%2Fwebrtc-candidate%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[webrtc 信令来自ChatGPT WebRTC（Web Real-Time Communication）是一种可以在浏览器之间实现实时音视频通信的技术。信令是 WebRTC 中的一部分，用于管理会话的建立、维护和结束。简单来说，信令是用于交换元数据以建立连接的过程。 在 WebRTC 中，需要进行一些信令交换步骤来建立连接，例如： 交换SDP（Session Description Protocol）：每个端点都需要将其本地的SDP发送给对方，并且解析对方发来的SDP。交换ICE（Interactive Connectivity Establishment）候选项：WebRTC 会在两个端点之间建立多个连接，并尝试找到最佳的连接。ICE候选项是一组网络地址和端口，用于在对等方之间建立连接。交换网络信息：包括网络类型、带宽和网络状况等信息，以帮助双方选择最佳的连接。以上这些步骤需要通过信令服务器来完成。信令服务器负责协调连接双方之间的通信，并确保会话能够顺利进行。 WebRTC 并没有规定信令协议，因此开发人员可以自行选择适合自己应用的协议，例如 SIP（Session Initiation Protocol）、XMPP（Extensible Messaging and Presence Protocol）或 WebSocket 等。 本文看下是如何获取ICE候选项 ice candidate 在 WebRTC 中，ICE（Interactive Connectivity Establishment）协议用于在两个端点之间建立多个连接，并尝试找到最佳的连接。ICE协议使用候选项（Candidate）来表示可以建立连接的网络地址和端口组合。 在 WebRTC 中，候选项由 ICE 代理服务器生成，并发送给对端。ICE 代理服务器可以是 STUN（Session Traversal Utilities for NAT）服务器、TURN（Traversal Using Relay NAT）服务器或者同时支持 STUN 和 TURN 功能的服务器。 当 WebRTC 客户端开始建立连接时，它会向 ICE 代理服务器发送一个请求，以获取可用的候选项。ICE 代理服务器会返回一组网络地址和端口组合，用于尝试建立连接。ICE 候选项可能包括以下几种类型： 主机候选项（Host Candidate）：本地计算机的网络地址和端口组合。 服务器反射候选项（Server Reflexive Candidate）：通过 STUN 服务器获取的公网 IP 地址和端口组合。 对称候选项（Symmetric Candidate）：使用对称 NAT 进行 NAT 穿透时获取的网络地址和端口组合。 中继候选项（Relay Candidate）：使用 TURN 服务器进行 NAT 穿透时获取的网络地址和端口组合。 ICE 候选项是根据网络状况动态生成的，当网络环境发生变化时，WebRTC 客户端可能会重新生成新的候选项并发送给对端。因此，在建立 WebRTC 连接时，确保正确获取和处理 ICE 候选项非常重要，以确保连接的稳定性和质量。 SdpOfferAnswerHandler在 crateoffer &amp;&amp; setlocalDescription 之后就需要获取 candidate了，setlocalDescription 从 PeerConnection 进到 SdpOfferAnswerHandler 123456789101112webrtc/src/pc/sdp_offer_answer.ccvoid SdpOfferAnswerHandler::DoSetLocalDescription(...) &#123; ... // MaybeStartGathering needs to be called after informing the observer so that // we don't signal any candidates before signaling that SetLocalDescription // completed. transport_controller_s()-&gt;MaybeStartGathering();&#125; 注释也说的很清楚了 transport_controller_s()-&gt;MaybeStartGathering(); 获取 candidates 的入口。 MaybeStartGathering123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127/Users/blackox626/webrtc/src/pc/peer_connection.ccPeerConnection::Initialize -&gt; PeerConnection::InitializeTransportController_nJsepTransportController* PeerConnection::InitializeTransportController_n( const RTCConfiguration&amp; configuration, const PeerConnectionDependencies&amp; dependencies) &#123; JsepTransportController::Config config; config.redetermine_role_on_ice_restart = configuration.redetermine_role_on_ice_restart; config.ssl_max_version = options_.ssl_max_version; config.disable_encryption = options_.disable_encryption; config.bundle_policy = configuration.bundle_policy; config.rtcp_mux_policy = configuration.rtcp_mux_policy; // TODO(bugs.webrtc.org/9891) - Remove options_.crypto_options then remove // this stub. config.crypto_options = configuration.crypto_options.has_value() ? *configuration.crypto_options : options_.crypto_options; config.transport_observer = this; config.rtcp_handler = InitializeRtcpCallback(); config.event_log = event_log_ptr_;#if defined(ENABLE_EXTERNAL_AUTH) config.enable_external_auth = true;#endif config.active_reset_srtp_params = configuration.active_reset_srtp_params; // DTLS has to be enabled to use SCTP. if (dtls_enabled_) &#123; config.sctp_factory = context_-&gt;sctp_transport_factory(); &#125; config.ice_transport_factory = ice_transport_factory_.get(); config.on_dtls_handshake_error_ = [weak_ptr = weak_factory_.GetWeakPtr()](rtc::SSLHandshakeError s) &#123; if (weak_ptr) &#123; weak_ptr-&gt;OnTransportControllerDtlsHandshakeError(s); &#125; &#125;; config.field_trials = trials_.get(); transport_controller_.reset( new JsepTransportController(network_thread(), port_allocator_.get(), async_dns_resolver_factory_.get(), config)); transport_controller_-&gt;SubscribeIceConnectionState( [this](cricket::IceConnectionState s) &#123; RTC_DCHECK_RUN_ON(network_thread()); if (s == cricket::kIceConnectionConnected) &#123; ReportTransportStats(); &#125; signaling_thread()-&gt;PostTask( SafeTask(signaling_thread_safety_.flag(), [this, s]() &#123; RTC_DCHECK_RUN_ON(signaling_thread()); OnTransportControllerConnectionState(s); &#125;)); &#125;); transport_controller_-&gt;SubscribeConnectionState( [this](PeerConnectionInterface::PeerConnectionState s) &#123; RTC_DCHECK_RUN_ON(network_thread()); signaling_thread()-&gt;PostTask( SafeTask(signaling_thread_safety_.flag(), [this, s]() &#123; RTC_DCHECK_RUN_ON(signaling_thread()); SetConnectionState(s); &#125;)); &#125;); transport_controller_-&gt;SubscribeStandardizedIceConnectionState( [this](PeerConnectionInterface::IceConnectionState s) &#123; RTC_DCHECK_RUN_ON(network_thread()); signaling_thread()-&gt;PostTask( SafeTask(signaling_thread_safety_.flag(), [this, s]() &#123; RTC_DCHECK_RUN_ON(signaling_thread()); SetStandardizedIceConnectionState(s); &#125;)); &#125;); transport_controller_-&gt;SubscribeIceGatheringState( [this](cricket::IceGatheringState s) &#123; RTC_DCHECK_RUN_ON(network_thread()); signaling_thread()-&gt;PostTask( SafeTask(signaling_thread_safety_.flag(), [this, s]() &#123; RTC_DCHECK_RUN_ON(signaling_thread()); OnTransportControllerGatheringState(s); &#125;)); &#125;); transport_controller_-&gt;SubscribeIceCandidateGathered( [this](const std::string&amp; transport, const std::vector&lt;cricket::Candidate&gt;&amp; candidates) &#123; RTC_DCHECK_RUN_ON(network_thread()); signaling_thread()-&gt;PostTask( SafeTask(signaling_thread_safety_.flag(), [this, t = transport, c = candidates]() &#123; RTC_DCHECK_RUN_ON(signaling_thread()); OnTransportControllerCandidatesGathered(t, c); &#125;)); &#125;); transport_controller_-&gt;SubscribeIceCandidateError( [this](const cricket::IceCandidateErrorEvent&amp; event) &#123; RTC_DCHECK_RUN_ON(network_thread()); signaling_thread()-&gt;PostTask( SafeTask(signaling_thread_safety_.flag(), [this, event = event]() &#123; RTC_DCHECK_RUN_ON(signaling_thread()); OnTransportControllerCandidateError(event); &#125;)); &#125;); transport_controller_-&gt;SubscribeIceCandidatesRemoved( [this](const std::vector&lt;cricket::Candidate&gt;&amp; c) &#123; RTC_DCHECK_RUN_ON(network_thread()); signaling_thread()-&gt;PostTask( SafeTask(signaling_thread_safety_.flag(), [this, c = c]() &#123; RTC_DCHECK_RUN_ON(signaling_thread()); OnTransportControllerCandidatesRemoved(c); &#125;)); &#125;); transport_controller_-&gt;SubscribeIceCandidatePairChanged( [this](const cricket::CandidatePairChangeEvent&amp; event) &#123; RTC_DCHECK_RUN_ON(network_thread()); signaling_thread()-&gt;PostTask( SafeTask(signaling_thread_safety_.flag(), [this, event = event]() &#123; RTC_DCHECK_RUN_ON(signaling_thread()); OnTransportControllerCandidateChanged(event); &#125;)); &#125;); transport_controller_-&gt;SetIceConfig(ParseIceConfig(configuration)); return transport_controller_.get();&#125; 注册了 candidate 获取的回调： transport_controller_-&gt;SubscribeIceCandidateGathered 先看下收集之后的行为 1234567891011121314151617181920212223242526272829303132void PeerConnection::OnTransportControllerCandidatesGathered( const std::string&amp; transport_name, const cricket::Candidates&amp; candidates) &#123; // TODO(bugs.webrtc.org/12427): Expect this to come in on the network thread // (not signaling as it currently does), handle appropriately. int sdp_mline_index; if (!GetLocalCandidateMediaIndex(transport_name, &amp;sdp_mline_index)) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "OnTransportControllerCandidatesGathered: content name " &lt;&lt; transport_name &lt;&lt; " not found"; return; &#125; for (cricket::Candidates::const_iterator citer = candidates.begin(); citer != candidates.end(); ++citer) &#123; // Use transport_name as the candidate media id. std::unique_ptr&lt;JsepIceCandidate&gt; candidate( new JsepIceCandidate(transport_name, sdp_mline_index, *citer)); sdp_handler_-&gt;AddLocalIceCandidate(candidate.get()); OnIceCandidate(std::move(candidate)); &#125;&#125;void PeerConnection::OnIceCandidate( std::unique_ptr&lt;IceCandidateInterface&gt; candidate) &#123; if (IsClosed()) &#123; return; &#125; ReportIceCandidateCollected(candidate-&gt;candidate()); ClearStatsCache(); Observer()-&gt;OnIceCandidate(candidate.get());&#125; 首先 sdp_handler_-&gt;AddLocalIceCandidate(candidate.get()); 设置本地ice candidate然后 通过 OnIceCandidate 事件回调 传递到上层app ，app通过 signal message 发送到webrtc server，webrtc server 转发给对端 app 同样也会收到 webrtc server转发过来的对端的 ice candidate 12345678910void PeerConnection::AddIceCandidate( std::unique_ptr&lt;IceCandidateInterface&gt; candidate, std::function&lt;void(RTCError)&gt; callback) &#123; RTC_DCHECK_RUN_ON(signaling_thread()); sdp_handler_-&gt;AddIceCandidate(std::move(candidate), [this, callback](webrtc::RTCError result) &#123; ClearStatsCache(); callback(result); &#125;);&#125; 通过 sdp_handler_-&gt;AddIceCandidate 设置对端的ice candidate； JsepTransportController JavaScript Session Establishment Protocol (JSEP) JavaScript 会话建立协议 123456789101112/webrtc/src/pc/jsep_transport_controller.ccvoid JsepTransportController::MaybeStartGathering() &#123; if (!network_thread_-&gt;IsCurrent()) &#123; network_thread_-&gt;BlockingCall([&amp;] &#123; MaybeStartGathering(); &#125;); return; &#125; for (auto&amp; dtls : GetDtlsTransports()) &#123; dtls-&gt;ice_transport()-&gt;MaybeStartGathering(); &#125;&#125; 先看下 DtlsTransports 的生产过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273SdpOfferAnswerHandler::DoSetLocalDescription -&gt;SdpOfferAnswerHandler::ApplyLocalDescription -&gt;SdpOfferAnswerHandler::PushdownTransportDescription -&gt; JsepTransportController::SetLocalDescription -&gt;JsepTransportController::ApplyDescription_n -&gt; RTCError JsepTransportController::ApplyDescription_n( bool local, SdpType type, const cricket::SessionDescription* description) &#123; TRACE_EVENT0("webrtc", "JsepTransportController::ApplyDescription_n"); RTC_DCHECK(description); if (local) &#123; local_desc_ = description; &#125; else &#123; remote_desc_ = description; &#125; RTCError error; error = ValidateAndMaybeUpdateBundleGroups(local, type, description); if (!error.ok()) &#123; return error; &#125; std::map&lt;const cricket::ContentGroup*, std::vector&lt;int&gt;&gt; merged_encrypted_extension_ids_by_bundle; if (!bundles_.bundle_groups().empty()) &#123; merged_encrypted_extension_ids_by_bundle = MergeEncryptedHeaderExtensionIdsForBundles(description); &#125; for (const cricket::ContentInfo&amp; content_info : description-&gt;contents()) &#123; // Don't create transports for rejected m-lines and bundled m-lines. if (content_info.rejected || !bundles_.IsFirstMidInGroup(content_info.name)) &#123; continue; &#125; error = MaybeCreateJsepTransport(local, content_info, *description); if (!error.ok()) &#123; return error; &#125; &#125; RTC_DCHECK(description-&gt;contents().size() == description-&gt;transport_infos().size()); for (size_t i = 0; i &lt; description-&gt;contents().size(); ++i) &#123; const cricket::ContentInfo&amp; content_info = description-&gt;contents()[i]; const cricket::TransportInfo&amp; transport_info = description-&gt;transport_infos()[i]; if (content_info.rejected) &#123; // This may cause groups to be removed from |bundles_.bundle_groups()|. HandleRejectedContent(content_info); continue; &#125; const cricket::ContentGroup* established_bundle_group = bundles_.LookupGroupByMid(content_info.name); // For bundle members that are not BUNDLE-tagged (not first in the group), // configure their transport to be the same as the BUNDLE-tagged transport. if (established_bundle_group &amp;&amp; content_info.name != *established_bundle_group-&gt;FirstContentName()) &#123; if (!HandleBundledContent(content_info, *established_bundle_group)) &#123; return RTCError(RTCErrorType::INVALID_PARAMETER, "Failed to process the bundled m= section with " "mid='" + content_info.name + "'."); &#125; continue; &#125; error = ValidateContent(content_info); if (!error.ok()) &#123; return error; &#125; std::vector&lt;int&gt; extension_ids; // Is BUNDLE-tagged (first in the group)? if (established_bundle_group &amp;&amp; content_info.name == *established_bundle_group-&gt;FirstContentName()) &#123; auto it = merged_encrypted_extension_ids_by_bundle.find( established_bundle_group); RTC_DCHECK(it != merged_encrypted_extension_ids_by_bundle.end()); extension_ids = it-&gt;second; &#125; else &#123; extension_ids = GetEncryptedHeaderExtensionIds(content_info); &#125; int rtp_abs_sendtime_extn_id = GetRtpAbsSendTimeHeaderExtensionId(content_info); cricket::JsepTransport* transport = GetJsepTransportForMid(content_info.name); RTC_DCHECK(transport); SetIceRole_n(DetermineIceRole(transport, transport_info, type, local)); cricket::JsepTransportDescription jsep_description = CreateJsepTransportDescription(content_info, transport_info, extension_ids, rtp_abs_sendtime_extn_id); if (local) &#123; error = transport-&gt;SetLocalJsepTransportDescription(jsep_description, type); &#125; else &#123; error = transport-&gt;SetRemoteJsepTransportDescription(jsep_description, type); &#125; if (!error.ok()) &#123; LOG_AND_RETURN_ERROR( RTCErrorType::INVALID_PARAMETER, "Failed to apply the description for m= section with mid='" + content_info.name + "': " + error.message()); &#125; &#125; if (type == SdpType::kAnswer) &#123; transports_.CommitTransports(); bundles_.Commit(); &#125; return RTCError::OK();&#125;RTCError JsepTransportController::MaybeCreateJsepTransport( bool local, const cricket::ContentInfo&amp; content_info, const cricket::SessionDescription&amp; description) &#123; cricket::JsepTransport* transport = GetJsepTransportByName(content_info.name); if (transport) &#123; return RTCError::OK(); &#125; const cricket::MediaContentDescription* content_desc = content_info.media_description(); if (certificate_ &amp;&amp; !content_desc-&gt;cryptos().empty()) &#123; return RTCError(RTCErrorType::INVALID_PARAMETER, "SDES and DTLS-SRTP cannot be enabled at the same time."); &#125; rtc::scoped_refptr&lt;webrtc::IceTransportInterface&gt; ice = CreateIceTransport(content_info.name, /*rtcp=*/false); std::unique_ptr&lt;cricket::DtlsTransportInternal&gt; rtp_dtls_transport = CreateDtlsTransport(content_info, ice-&gt;internal()); std::unique_ptr&lt;cricket::DtlsTransportInternal&gt; rtcp_dtls_transport; std::unique_ptr&lt;RtpTransport&gt; unencrypted_rtp_transport; std::unique_ptr&lt;SrtpTransport&gt; sdes_transport; std::unique_ptr&lt;DtlsSrtpTransport&gt; dtls_srtp_transport; rtc::scoped_refptr&lt;webrtc::IceTransportInterface&gt; rtcp_ice; if (config_.rtcp_mux_policy != PeerConnectionInterface::kRtcpMuxPolicyRequire &amp;&amp; content_info.type == cricket::MediaProtocolType::kRtp) &#123; rtcp_ice = CreateIceTransport(content_info.name, /*rtcp=*/true); rtcp_dtls_transport = CreateDtlsTransport(content_info, rtcp_ice-&gt;internal()); &#125; if (config_.disable_encryption) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Creating UnencryptedRtpTransport, becayse encryption is disabled."; unencrypted_rtp_transport = CreateUnencryptedRtpTransport( content_info.name, rtp_dtls_transport.get(), rtcp_dtls_transport.get()); &#125; else if (!content_desc-&gt;cryptos().empty()) &#123; sdes_transport = CreateSdesTransport( content_info.name, rtp_dtls_transport.get(), rtcp_dtls_transport.get()); RTC_LOG(LS_INFO) &lt;&lt; "Creating SdesTransport."; &#125; else &#123; RTC_LOG(LS_INFO) &lt;&lt; "Creating DtlsSrtpTransport."; dtls_srtp_transport = CreateDtlsSrtpTransport( content_info.name, rtp_dtls_transport.get(), rtcp_dtls_transport.get()); &#125; std::unique_ptr&lt;cricket::SctpTransportInternal&gt; sctp_transport; if (config_.sctp_factory) &#123; sctp_transport = config_.sctp_factory-&gt;CreateSctpTransport(rtp_dtls_transport.get()); &#125; std::unique_ptr&lt;cricket::JsepTransport&gt; jsep_transport = std::make_unique&lt;cricket::JsepTransport&gt;( content_info.name, certificate_, std::move(ice), std::move(rtcp_ice), std::move(unencrypted_rtp_transport), std::move(sdes_transport), std::move(dtls_srtp_transport), std::move(rtp_dtls_transport), std::move(rtcp_dtls_transport), std::move(sctp_transport), [&amp;]() &#123; RTC_DCHECK_RUN_ON(network_thread_); UpdateAggregateStates_n(); &#125;); jsep_transport-&gt;rtp_transport()-&gt;SignalRtcpPacketReceived.connect( this, &amp;JsepTransportController::OnRtcpPacketReceived_n); transports_.RegisterTransport(content_info.name, std::move(jsep_transport)); UpdateAggregateStates_n(); return RTCError::OK();&#125;rtc::scoped_refptr&lt;webrtc::IceTransportInterface&gt;JsepTransportController::CreateIceTransport(const std::string&amp; transport_name, bool rtcp) &#123; int component = rtcp ? cricket::ICE_CANDIDATE_COMPONENT_RTCP : cricket::ICE_CANDIDATE_COMPONENT_RTP; IceTransportInit init; init.set_port_allocator(port_allocator_); init.set_async_dns_resolver_factory(async_dns_resolver_factory_); init.set_event_log(config_.event_log); init.set_field_trials(config_.field_trials); auto transport = config_.ice_transport_factory-&gt;CreateIceTransport( transport_name, component, std::move(init)); RTC_DCHECK(transport); transport-&gt;internal()-&gt;SetIceRole(ice_role_); transport-&gt;internal()-&gt;SetIceTiebreaker(ice_tiebreaker_); transport-&gt;internal()-&gt;SetIceConfig(ice_config_); return transport;&#125;std::unique_ptr&lt;cricket::DtlsTransportInternal&gt;JsepTransportController::CreateDtlsTransport( const cricket::ContentInfo&amp; content_info, cricket::IceTransportInternal* ice) &#123; RTC_DCHECK_RUN_ON(network_thread_); std::unique_ptr&lt;cricket::DtlsTransportInternal&gt; dtls; if (config_.dtls_transport_factory) &#123; dtls = config_.dtls_transport_factory-&gt;CreateDtlsTransport( ice, config_.crypto_options, config_.ssl_max_version); &#125; else &#123; dtls = std::make_unique&lt;cricket::DtlsTransport&gt;(ice, config_.crypto_options, config_.event_log, config_.ssl_max_version); &#125; RTC_DCHECK(dtls); RTC_DCHECK_EQ(ice, dtls-&gt;ice_transport()); if (certificate_) &#123; bool set_cert_success = dtls-&gt;SetLocalCertificate(certificate_); RTC_DCHECK(set_cert_success); &#125; // Connect to signals offered by the DTLS and ICE transport. dtls-&gt;SignalWritableState.connect( this, &amp;JsepTransportController::OnTransportWritableState_n); dtls-&gt;SignalReceivingState.connect( this, &amp;JsepTransportController::OnTransportReceivingState_n); dtls-&gt;ice_transport()-&gt;SignalGatheringState.connect( this, &amp;JsepTransportController::OnTransportGatheringState_n); dtls-&gt;ice_transport()-&gt;SignalCandidateGathered.connect( this, &amp;JsepTransportController::OnTransportCandidateGathered_n); dtls-&gt;ice_transport()-&gt;SignalCandidateError.connect( this, &amp;JsepTransportController::OnTransportCandidateError_n); dtls-&gt;ice_transport()-&gt;SignalCandidatesRemoved.connect( this, &amp;JsepTransportController::OnTransportCandidatesRemoved_n); dtls-&gt;ice_transport()-&gt;SignalRoleConflict.connect( this, &amp;JsepTransportController::OnTransportRoleConflict_n); dtls-&gt;ice_transport()-&gt;SignalStateChanged.connect( this, &amp;JsepTransportController::OnTransportStateChanged_n); dtls-&gt;ice_transport()-&gt;SignalIceTransportStateChanged.connect( this, &amp;JsepTransportController::OnTransportStateChanged_n); dtls-&gt;ice_transport()-&gt;SignalCandidatePairChanged.connect( this, &amp;JsepTransportController::OnTransportCandidatePairChanged_n); dtls-&gt;SubscribeDtlsHandshakeError( [this](rtc::SSLHandshakeError error) &#123; OnDtlsHandshakeError(error); &#125;); return dtls;&#125; 12345678910111213.../webrtc/src/p2p/base/default_ice_transport_factory.ccrtc::scoped_refptr&lt;IceTransportInterface&gt;DefaultIceTransportFactory::CreateIceTransport( const std::string&amp; transport_name, int component, IceTransportInit init) &#123; BasicIceControllerFactory factory; init.set_ice_controller_factory(&amp;factory); return rtc::make_ref_counted&lt;DefaultIceTransport&gt;( cricket::P2PTransportChannel::Create(transport_name, component, std::move(init)));&#125; 在来看 dtls-&gt;ice_transport()-&gt;MaybeStartGathering() dtls-&gt;ice_transport() 就是 P2PTransportChannel 123456789101112131415161718192021dtls-&gt;ice_transport()-&gt;SignalCandidateGathered.connect( this, &amp;JsepTransportController::OnTransportCandidateGathered_n);void JsepTransportController::OnTransportCandidateGathered_n( cricket::IceTransportInternal* transport, const cricket::Candidate&amp; candidate) &#123; // We should never signal peer-reflexive candidates. if (candidate.type() == cricket::PRFLX_PORT_TYPE) &#123; RTC_DCHECK_NOTREACHED(); return; &#125; signal_ice_candidates_gathered_.Send( transport-&gt;transport_name(), std::vector&lt;cricket::Candidate&gt;&#123;candidate&#125;);&#125;void SubscribeIceCandidateGathered(F&amp;&amp; callback) &#123; RTC_DCHECK_RUN_ON(network_thread_); signal_ice_candidates_gathered_.AddReceiver(std::forward&lt;F&gt;(callback)); &#125; 这里就跟之前的peer_connection 中 SubscribeIceCandidateGathered 注册回调 对应上了 P2PTransportChannel1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980.../webrtc/src/p2p/base/p2p_transport_channel.ccvoid P2PTransportChannel::MaybeStartGathering() &#123; RTC_DCHECK_RUN_ON(network_thread_); // TODO(bugs.webrtc.org/14605): ensure tie_breaker_ is set. if (ice_parameters_.ufrag.empty() || ice_parameters_.pwd.empty()) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Cannot gather candidates because ICE parameters are empty" " ufrag: " &lt;&lt; ice_parameters_.ufrag &lt;&lt; " pwd: " &lt;&lt; ice_parameters_.pwd; return; &#125; // Start gathering if we never started before, or if an ICE restart occurred. if (allocator_sessions_.empty() || IceCredentialsChanged(allocator_sessions_.back()-&gt;ice_ufrag(), allocator_sessions_.back()-&gt;ice_pwd(), ice_parameters_.ufrag, ice_parameters_.pwd)) &#123; if (gathering_state_ != kIceGatheringGathering) &#123; gathering_state_ = kIceGatheringGathering; SignalGatheringState(this); &#125; if (!allocator_sessions_.empty()) &#123; IceRestartState state; if (writable()) &#123; state = IceRestartState::CONNECTED; &#125; else if (IsGettingPorts()) &#123; state = IceRestartState::CONNECTING; &#125; else &#123; state = IceRestartState::DISCONNECTED; &#125; RTC_HISTOGRAM_ENUMERATION("WebRTC.PeerConnection.IceRestartState", static_cast&lt;int&gt;(state), static_cast&lt;int&gt;(IceRestartState::MAX_VALUE)); &#125; for (const auto&amp; session : allocator_sessions_) &#123; if (session-&gt;IsStopped()) &#123; continue; &#125; session-&gt;StopGettingPorts(); &#125; // Time for a new allocator. std::unique_ptr&lt;PortAllocatorSession&gt; pooled_session = allocator_-&gt;TakePooledSession(transport_name(), component(), ice_parameters_.ufrag, ice_parameters_.pwd); if (pooled_session) &#123; pooled_session-&gt;set_ice_tiebreaker(tiebreaker_); AddAllocatorSession(std::move(pooled_session)); PortAllocatorSession* raw_pooled_session = allocator_sessions_.back().get(); // Process the pooled session's existing candidates/ports, if they exist. OnCandidatesReady(raw_pooled_session, raw_pooled_session-&gt;ReadyCandidates()); for (PortInterface* port : allocator_sessions_.back()-&gt;ReadyPorts()) &#123; OnPortReady(raw_pooled_session, port); &#125; if (allocator_sessions_.back()-&gt;CandidatesAllocationDone()) &#123; OnCandidatesAllocationDone(raw_pooled_session); &#125; &#125; else &#123; AddAllocatorSession(allocator_-&gt;CreateSession( transport_name(), component(), ice_parameters_.ufrag, ice_parameters_.pwd)); allocator_sessions_.back()-&gt;set_ice_tiebreaker(tiebreaker_); allocator_sessions_.back()-&gt;StartGettingPorts(); &#125; &#125;&#125;void P2PTransportChannel::OnCandidatesReady( PortAllocatorSession* session, const std::vector&lt;Candidate&gt;&amp; candidates) &#123; RTC_DCHECK_RUN_ON(network_thread_); for (size_t i = 0; i &lt; candidates.size(); ++i) &#123; SignalCandidateGathered(this, candidates[i]); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491.../webrtc/src/p2p/client/basic_port_allocator.cc// BasicPortAllocatorSessionBasicPortAllocatorSession::BasicPortAllocatorSession( BasicPortAllocator* allocator, absl::string_view content_name, int component, absl::string_view ice_ufrag, absl::string_view ice_pwd) : PortAllocatorSession(content_name, component, ice_ufrag, ice_pwd, allocator-&gt;flags()), allocator_(allocator), network_thread_(rtc::Thread::Current()), socket_factory_(allocator-&gt;socket_factory()), allocation_started_(false), network_manager_started_(false), allocation_sequences_created_(false), turn_port_prune_policy_(allocator-&gt;turn_port_prune_policy()) &#123; TRACE_EVENT0("webrtc", "BasicPortAllocatorSession::BasicPortAllocatorSession"); allocator_-&gt;network_manager()-&gt;SignalNetworksChanged.connect( this, &amp;BasicPortAllocatorSession::OnNetworksChanged); /// 收集networks BasicNetworkManager allocator_-&gt;network_manager()-&gt;StartUpdating();&#125;void BasicPortAllocatorSession::StartGettingPorts() &#123; RTC_DCHECK_RUN_ON(network_thread_); state_ = SessionState::GATHERING; network_thread_-&gt;PostTask( SafeTask(network_safety_.flag(), [this] &#123; GetPortConfigurations(); &#125;)); RTC_LOG(LS_INFO) &lt;&lt; "Start getting ports with turn_port_prune_policy " &lt;&lt; turn_port_prune_policy_;&#125;void BasicPortAllocatorSession::GetPortConfigurations() &#123; RTC_DCHECK_RUN_ON(network_thread_); auto config = std::make_unique&lt;PortConfiguration&gt;( allocator_-&gt;stun_servers(), username(), password(), allocator()-&gt;field_trials()); for (const RelayServerConfig&amp; turn_server : allocator_-&gt;turn_servers()) &#123; config-&gt;AddRelay(turn_server); &#125; ConfigReady(std::move(config));&#125;void BasicPortAllocatorSession::ConfigReady(PortConfiguration* config) &#123; RTC_DCHECK_RUN_ON(network_thread_); ConfigReady(absl::WrapUnique(config));&#125;void BasicPortAllocatorSession::ConfigReady( std::unique_ptr&lt;PortConfiguration&gt; config) &#123; RTC_DCHECK_RUN_ON(network_thread_); network_thread_-&gt;PostTask(SafeTask( network_safety_.flag(), [this, config = std::move(config)]() mutable &#123; OnConfigReady(std::move(config)); &#125;));&#125;// Adds a configuration to the list.void BasicPortAllocatorSession::OnConfigReady( std::unique_ptr&lt;PortConfiguration&gt; config) &#123; RTC_DCHECK_RUN_ON(network_thread_); if (config) configs_.push_back(std::move(config)); AllocatePorts();&#125;void BasicPortAllocatorSession::AllocatePorts() &#123; RTC_DCHECK_RUN_ON(network_thread_); network_thread_-&gt;PostTask(SafeTask( network_safety_.flag(), [this, allocation_epoch = allocation_epoch_] &#123; OnAllocate(allocation_epoch); &#125;));&#125;void BasicPortAllocatorSession::OnAllocate(int allocation_epoch) &#123; RTC_DCHECK_RUN_ON(network_thread_); if (allocation_epoch != allocation_epoch_) return; if (network_manager_started_ &amp;&amp; !IsStopped()) &#123; bool disable_equivalent_phases = true; DoAllocate(disable_equivalent_phases); &#125; allocation_started_ = true;&#125;void BasicPortAllocatorSession::DoAllocate(bool disable_equivalent) &#123; RTC_DCHECK_RUN_ON(network_thread_); bool done_signal_needed = false; std::vector&lt;const rtc::Network*&gt; networks = GetNetworks(); if (networks.empty()) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "Machine has no networks; no ports will be allocated"; done_signal_needed = true; &#125; else &#123; RTC_LOG(LS_INFO) &lt;&lt; "Allocate ports on " &lt;&lt; NetworksToString(networks); PortConfiguration* config = configs_.empty() ? nullptr : configs_.back().get(); for (uint32_t i = 0; i &lt; networks.size(); ++i) &#123; uint32_t sequence_flags = flags(); if ((sequence_flags &amp; DISABLE_ALL_PHASES) == DISABLE_ALL_PHASES) &#123; // If all the ports are disabled we should just fire the allocation // done event and return. done_signal_needed = true; break; &#125; if (!config || config-&gt;relays.empty()) &#123; // No relay ports specified in this config. sequence_flags |= PORTALLOCATOR_DISABLE_RELAY; &#125; if (!(sequence_flags &amp; PORTALLOCATOR_ENABLE_IPV6) &amp;&amp; networks[i]-&gt;GetBestIP().family() == AF_INET6) &#123; // Skip IPv6 networks unless the flag's been set. continue; &#125; if (!(sequence_flags &amp; PORTALLOCATOR_ENABLE_IPV6_ON_WIFI) &amp;&amp; networks[i]-&gt;GetBestIP().family() == AF_INET6 &amp;&amp; networks[i]-&gt;type() == rtc::ADAPTER_TYPE_WIFI) &#123; // Skip IPv6 Wi-Fi networks unless the flag's been set. continue; &#125; if (disable_equivalent) &#123; // Disable phases that would only create ports equivalent to // ones that we have already made. DisableEquivalentPhases(networks[i], config, &amp;sequence_flags); if ((sequence_flags &amp; DISABLE_ALL_PHASES) == DISABLE_ALL_PHASES) &#123; // New AllocationSequence would have nothing to do, so don't make it. continue; &#125; &#125; AllocationSequence* sequence = new AllocationSequence(this, networks[i], config, sequence_flags, [this, safety_flag = network_safety_.flag()] &#123; if (safety_flag-&gt;alive()) OnPortAllocationComplete(); &#125;); sequence-&gt;Init(); sequence-&gt;Start(); sequences_.push_back(sequence); done_signal_needed = true; &#125; &#125; if (done_signal_needed) &#123; network_thread_-&gt;PostTask(SafeTask(network_safety_.flag(), [this] &#123; OnAllocationSequenceObjectsCreated(); &#125;)); &#125;&#125;std::vector&lt;const rtc::Network*&gt; BasicPortAllocatorSession::GetNetworks() &#123; RTC_DCHECK_RUN_ON(network_thread_); std::vector&lt;const rtc::Network*&gt; networks; rtc::NetworkManager* network_manager = allocator_-&gt;network_manager(); RTC_DCHECK(network_manager != nullptr); // If the network permission state is BLOCKED, we just act as if the flag has // been passed in. if (network_manager-&gt;enumeration_permission() == rtc::NetworkManager::ENUMERATION_BLOCKED) &#123; set_flags(flags() | PORTALLOCATOR_DISABLE_ADAPTER_ENUMERATION); &#125; // If the adapter enumeration is disabled, we'll just bind to any address // instead of specific NIC. This is to ensure the same routing for http // traffic by OS is also used here to avoid any local or public IP leakage // during stun process. if (flags() &amp; PORTALLOCATOR_DISABLE_ADAPTER_ENUMERATION) &#123; networks = network_manager-&gt;GetAnyAddressNetworks(); &#125; else &#123; networks = network_manager-&gt;GetNetworks(); // If network enumeration fails, use the ANY address as a fallback, so we // can at least try gathering candidates using the default route chosen by // the OS. Or, if the PORTALLOCATOR_ENABLE_ANY_ADDRESS_PORTS flag is // set, we'll use ANY address candidates either way. if (networks.empty() || (flags() &amp; PORTALLOCATOR_ENABLE_ANY_ADDRESS_PORTS)) &#123; std::vector&lt;const rtc::Network*&gt; any_address_networks = network_manager-&gt;GetAnyAddressNetworks(); networks.insert(networks.end(), any_address_networks.begin(), any_address_networks.end()); &#125; &#125; // Filter out link-local networks if needed. if (flags() &amp; PORTALLOCATOR_DISABLE_LINK_LOCAL_NETWORKS) &#123; NetworkFilter link_local_filter( [](const rtc::Network* network) &#123; return IPIsLinkLocal(network-&gt;prefix()); &#125;, "link-local"); FilterNetworks(&amp;networks, link_local_filter); &#125; // Do some more filtering, depending on the network ignore mask and "disable // costly networks" flag. NetworkFilter ignored_filter( [this](const rtc::Network* network) &#123; return allocator_-&gt;GetNetworkIgnoreMask() &amp; network-&gt;type(); &#125;, "ignored"); FilterNetworks(&amp;networks, ignored_filter); if (flags() &amp; PORTALLOCATOR_DISABLE_COSTLY_NETWORKS) &#123; uint16_t lowest_cost = rtc::kNetworkCostMax; for (const rtc::Network* network : networks) &#123; // Don't determine the lowest cost from a link-local network. // On iOS, a device connected to the computer will get a link-local // network for communicating with the computer, however this network can't // be used to connect to a peer outside the network. if (rtc::IPIsLinkLocal(network-&gt;GetBestIP())) &#123; continue; &#125; lowest_cost = std::min&lt;uint16_t&gt;( lowest_cost, network-&gt;GetCost(*allocator()-&gt;field_trials())); &#125; NetworkFilter costly_filter( [lowest_cost, this](const rtc::Network* network) &#123; return network-&gt;GetCost(*allocator()-&gt;field_trials()) &gt; lowest_cost + rtc::kNetworkCostLow; &#125;, "costly"); FilterNetworks(&amp;networks, costly_filter); &#125; // Lastly, if we have a limit for the number of IPv6 network interfaces (by // default, it's 5), remove networks to ensure that limit is satisfied. // // TODO(deadbeef): Instead of just taking the first N arbitrary IPv6 // networks, we could try to choose a set that's "most likely to work". It's // hard to define what that means though; it's not just "lowest cost". // Alternatively, we could just focus on making our ICE pinging logic smarter // such that this filtering isn't necessary in the first place. const webrtc::FieldTrialsView* field_trials = allocator_-&gt;field_trials(); if (IsDiversifyIpv6InterfacesEnabled(field_trials)) &#123; std::vector&lt;const rtc::Network*&gt; ipv6_networks; for (auto it = networks.begin(); it != networks.end();) &#123; if ((*it)-&gt;prefix().family() == AF_INET6) &#123; ipv6_networks.push_back(*it); it = networks.erase(it); continue; &#125; ++it; &#125; ipv6_networks = SelectIPv6Networks(ipv6_networks, allocator_-&gt;max_ipv6_networks()); networks.insert(networks.end(), ipv6_networks.begin(), ipv6_networks.end()); &#125; else &#123; int ipv6_networks = 0; for (auto it = networks.begin(); it != networks.end();) &#123; if ((*it)-&gt;prefix().family() == AF_INET6) &#123; if (ipv6_networks &gt;= allocator_-&gt;max_ipv6_networks()) &#123; it = networks.erase(it); continue; &#125; else &#123; ++ipv6_networks; &#125; &#125; ++it; &#125; &#125; return networks;&#125;void AllocationSequence::Start() &#123; state_ = kRunning; session_-&gt;network_thread()-&gt;PostTask( SafeTask(safety_.flag(), [this, epoch = epoch_] &#123; Process(epoch); &#125;)); // Take a snapshot of the best IP, so that when DisableEquivalentPhases is // called next time, we enable all phases if the best IP has since changed. previous_best_ip_ = network_-&gt;GetBestIP();&#125;void AllocationSequence::Process(int epoch) &#123; RTC_DCHECK(rtc::Thread::Current() == session_-&gt;network_thread()); const char* const PHASE_NAMES[kNumPhases] = &#123;"Udp", "Relay", "Tcp"&#125;; if (epoch != epoch_) return; // Perform all of the phases in the current step. RTC_LOG(LS_INFO) &lt;&lt; network_-&gt;ToString() &lt;&lt; ": Allocation Phase=" &lt;&lt; PHASE_NAMES[phase_]; switch (phase_) &#123; case PHASE_UDP: CreateUDPPorts(); CreateStunPorts(); break; case PHASE_RELAY: CreateRelayPorts(); break; case PHASE_TCP: CreateTCPPorts(); state_ = kCompleted; break; default: RTC_DCHECK_NOTREACHED(); &#125; if (state() == kRunning) &#123; ++phase_; session_-&gt;network_thread()-&gt;PostDelayedTask( SafeTask(safety_.flag(), [this, epoch = epoch_] &#123; Process(epoch); &#125;), TimeDelta::Millis(session_-&gt;allocator()-&gt;step_delay())); &#125; else &#123; // No allocation steps needed further if all phases in AllocationSequence // are completed. Cause further Process calls in the previous epoch to be // ignored. ++epoch_; port_allocation_complete_callback_(); &#125;&#125;void AllocationSequence::CreateUDPPorts() &#123; if (IsFlagSet(PORTALLOCATOR_DISABLE_UDP)) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "AllocationSequence: UDP ports disabled, skipping."; return; &#125; // TODO(mallinath) - Remove UDPPort creating socket after shared socket // is enabled completely. std::unique_ptr&lt;UDPPort&gt; port; bool emit_local_candidate_for_anyaddress = !IsFlagSet(PORTALLOCATOR_DISABLE_DEFAULT_LOCAL_CANDIDATE); if (IsFlagSet(PORTALLOCATOR_ENABLE_SHARED_SOCKET) &amp;&amp; udp_socket_) &#123; port = UDPPort::Create( session_-&gt;network_thread(), session_-&gt;socket_factory(), network_, udp_socket_.get(), session_-&gt;username(), session_-&gt;password(), emit_local_candidate_for_anyaddress, session_-&gt;allocator()-&gt;stun_candidate_keepalive_interval(), session_-&gt;allocator()-&gt;field_trials()); &#125; else &#123; port = UDPPort::Create( session_-&gt;network_thread(), session_-&gt;socket_factory(), network_, session_-&gt;allocator()-&gt;min_port(), session_-&gt;allocator()-&gt;max_port(), session_-&gt;username(), session_-&gt;password(), emit_local_candidate_for_anyaddress, session_-&gt;allocator()-&gt;stun_candidate_keepalive_interval(), session_-&gt;allocator()-&gt;field_trials()); &#125; if (port) &#123; port-&gt;SetIceTiebreaker(session_-&gt;ice_tiebreaker()); // If shared socket is enabled, STUN candidate will be allocated by the // UDPPort. if (IsFlagSet(PORTALLOCATOR_ENABLE_SHARED_SOCKET)) &#123; udp_port_ = port.get(); port-&gt;SubscribePortDestroyed( [this](PortInterface* port) &#123; OnPortDestroyed(port); &#125;); // If STUN is not disabled, setting stun server address to port. if (!IsFlagSet(PORTALLOCATOR_DISABLE_STUN)) &#123; if (config_ &amp;&amp; !config_-&gt;StunServers().empty()) &#123; RTC_LOG(LS_INFO) &lt;&lt; "AllocationSequence: UDPPort will be handling the " "STUN candidate generation."; port-&gt;set_server_addresses(config_-&gt;StunServers()); &#125; &#125; &#125; session_-&gt;AddAllocatedPort(port.release(), this); &#125;&#125;void BasicPortAllocatorSession::AddAllocatedPort(Port* port, AllocationSequence* seq) &#123; RTC_DCHECK_RUN_ON(network_thread_); if (!port) return; RTC_LOG(LS_INFO) &lt;&lt; "Adding allocated port for " &lt;&lt; content_name(); port-&gt;set_content_name(content_name()); port-&gt;set_component(component()); port-&gt;set_generation(generation()); if (allocator_-&gt;proxy().type != rtc::PROXY_NONE) port-&gt;set_proxy(allocator_-&gt;user_agent(), allocator_-&gt;proxy()); port-&gt;set_send_retransmit_count_attribute( (flags() &amp; PORTALLOCATOR_ENABLE_STUN_RETRANSMIT_ATTRIBUTE) != 0); PortData data(port, seq); ports_.push_back(data); port-&gt;SignalCandidateReady.connect( this, &amp;BasicPortAllocatorSession::OnCandidateReady); port-&gt;SignalCandidateError.connect( this, &amp;BasicPortAllocatorSession::OnCandidateError); port-&gt;SignalPortComplete.connect(this, &amp;BasicPortAllocatorSession::OnPortComplete); port-&gt;SubscribePortDestroyed( [this](PortInterface* port) &#123; OnPortDestroyed(port); &#125;); port-&gt;SignalPortError.connect(this, &amp;BasicPortAllocatorSession::OnPortError); RTC_LOG(LS_INFO) &lt;&lt; port-&gt;ToString() &lt;&lt; ": Added port to allocator"; port-&gt;PrepareAddress();&#125;void BasicPortAllocatorSession::OnCandidateReady(Port* port, const Candidate&amp; c) &#123; RTC_DCHECK_RUN_ON(network_thread_); PortData* data = FindPort(port); RTC_DCHECK(data != NULL); RTC_LOG(LS_INFO) &lt;&lt; port-&gt;ToString() &lt;&lt; ": Gathered candidate: " &lt;&lt; c.ToSensitiveString(); // Discarding any candidate signal if port allocation status is // already done with gathering. if (!data-&gt;inprogress()) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "Discarding candidate because port is already done gathering."; return; &#125; // Mark that the port has a pairable candidate, either because we have a // usable candidate from the port, or simply because the port is bound to the // any address and therefore has no host candidate. This will trigger the port // to start creating candidate pairs (connections) and issue connectivity // checks. If port has already been marked as having a pairable candidate, // do nothing here. // Note: We should check whether any candidates may become ready after this // because there we will check whether the candidate is generated by the ready // ports, which may include this port. bool pruned = false; if (CandidatePairable(c, port) &amp;&amp; !data-&gt;has_pairable_candidate()) &#123; data-&gt;set_has_pairable_candidate(true); if (port-&gt;Type() == RELAY_PORT_TYPE) &#123; if (turn_port_prune_policy_ == webrtc::KEEP_FIRST_READY) &#123; pruned = PruneNewlyPairableTurnPort(data); &#125; else if (turn_port_prune_policy_ == webrtc::PRUNE_BASED_ON_PRIORITY) &#123; pruned = PruneTurnPorts(port); &#125; &#125; // If the current port is not pruned yet, SignalPortReady. if (!data-&gt;pruned()) &#123; RTC_LOG(LS_INFO) &lt;&lt; port-&gt;ToString() &lt;&lt; ": Port ready."; SignalPortReady(this, port); port-&gt;KeepAliveUntilPruned(); &#125; &#125; if (data-&gt;ready() &amp;&amp; CheckCandidateFilter(c)) &#123; std::vector&lt;Candidate&gt; candidates; candidates.push_back(allocator_-&gt;SanitizeCandidate(c)); SignalCandidatesReady(this, candidates); &#125; else &#123; RTC_LOG(LS_INFO) &lt;&lt; "Discarding candidate because it doesn't match filter."; &#125; // If we have pruned any port, maybe need to signal port allocation done. if (pruned) &#123; MaybeSignalCandidatesAllocationDone(); &#125;&#125;void BasicPortAllocatorSession::OnPortComplete(Port* port) &#123; RTC_DCHECK_RUN_ON(network_thread_); RTC_LOG(LS_INFO) &lt;&lt; port-&gt;ToString() &lt;&lt; ": Port completed gathering candidates."; PortData* data = FindPort(port); RTC_DCHECK(data != NULL); // Ignore any late signals. if (!data-&gt;inprogress()) &#123; return; &#125; // Moving to COMPLETE state. data-&gt;set_state(PortData::STATE_COMPLETE); // Send candidate allocation complete signal if this was the last port. MaybeSignalCandidatesAllocationDone();&#125; 通过 udp port 连接到 ice server ， 注册 OnCandidateReady 回调 就跟前面 p2p_transport_channel OnCandidateReady 对应上了 port123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218.../webrtc/src/p2p/base/stun_port.ccUDPPort::UDPPort(rtc::Thread* thread, rtc::PacketSocketFactory* factory, const rtc::Network* network, rtc::AsyncPacketSocket* socket, absl::string_view username, absl::string_view password, bool emit_local_for_anyaddress, const webrtc::FieldTrialsView* field_trials) : Port(thread, LOCAL_PORT_TYPE, factory, network, username, password, field_trials), request_manager_( thread, [this](const void* data, size_t size, StunRequest* request) &#123; OnSendPacket(data, size, request); &#125;), socket_(socket), error_(0), ready_(false), stun_keepalive_delay_(STUN_KEEPALIVE_INTERVAL), dscp_(rtc::DSCP_NO_CHANGE), emit_local_for_anyaddress_(emit_local_for_anyaddress) &#123;&#125;UDPPort::UDPPort(rtc::Thread* thread, rtc::PacketSocketFactory* factory, const rtc::Network* network, uint16_t min_port, uint16_t max_port, absl::string_view username, absl::string_view password, bool emit_local_for_anyaddress, const webrtc::FieldTrialsView* field_trials) : Port(thread, LOCAL_PORT_TYPE, factory, network, min_port, max_port, username, password, field_trials), request_manager_( thread, [this](const void* data, size_t size, StunRequest* request) &#123; OnSendPacket(data, size, request); &#125;), socket_(nullptr), error_(0), ready_(false), stun_keepalive_delay_(STUN_KEEPALIVE_INTERVAL), dscp_(rtc::DSCP_NO_CHANGE), emit_local_for_anyaddress_(emit_local_for_anyaddress) &#123;&#125;bool UDPPort::Init() &#123; stun_keepalive_lifetime_ = GetStunKeepaliveLifetime(); if (!SharedSocket()) &#123; RTC_DCHECK(socket_ == nullptr); socket_ = socket_factory()-&gt;CreateUdpSocket( rtc::SocketAddress(Network()-&gt;GetBestIP(), 0), min_port(), max_port()); if (!socket_) &#123; RTC_LOG(LS_WARNING) &lt;&lt; ToString() &lt;&lt; ": UDP socket creation failed"; return false; &#125; socket_-&gt;SignalReadPacket.connect(this, &amp;UDPPort::OnReadPacket); &#125; socket_-&gt;SignalSentPacket.connect(this, &amp;UDPPort::OnSentPacket); socket_-&gt;SignalReadyToSend.connect(this, &amp;UDPPort::OnReadyToSend); socket_-&gt;SignalAddressReady.connect(this, &amp;UDPPort::OnLocalAddressReady); return true;&#125;void UDPPort::PrepareAddress() &#123; RTC_DCHECK(request_manager_.empty()); if (socket_-&gt;GetState() == rtc::AsyncPacketSocket::STATE_BOUND) &#123; OnLocalAddressReady(socket_, socket_-&gt;GetLocalAddress()); &#125;&#125;void UDPPort::OnLocalAddressReady(rtc::AsyncPacketSocket* socket, const rtc::SocketAddress&amp; address) &#123; // When adapter enumeration is disabled and binding to the any address, the // default local address will be issued as a candidate instead if // `emit_local_for_anyaddress` is true. This is to allow connectivity for // applications which absolutely requires a HOST candidate. rtc::SocketAddress addr = address; // If MaybeSetDefaultLocalAddress fails, we keep the "any" IP so that at // least the port is listening. MaybeSetDefaultLocalAddress(&amp;addr); AddAddress(addr, addr, rtc::SocketAddress(), UDP_PROTOCOL_NAME, "", "", LOCAL_PORT_TYPE, ICE_TYPE_PREFERENCE_HOST, 0, "", false); MaybePrepareStunCandidate();&#125;void UDPPort::MaybePrepareStunCandidate() &#123; // Sending binding request to the STUN server if address is available to // prepare STUN candidate. if (!server_addresses_.empty()) &#123; SendStunBindingRequests(); &#125; else &#123; // Port is done allocating candidates. MaybeSetPortCompleteOrError(); &#125;&#125;void UDPPort::SendStunBindingRequests() &#123; // We will keep pinging the stun server to make sure our NAT pin-hole stays // open until the deadline (specified in SendStunBindingRequest). RTC_DCHECK(request_manager_.empty()); for (ServerAddresses::const_iterator it = server_addresses_.begin(); it != server_addresses_.end();) &#123; // sending a STUN binding request may cause the current SocketAddress to be // erased from the set, invalidating the loop iterator before it is // incremented (even if the SocketAddress itself still exists). So make a // copy of the loop iterator, which may be safely invalidated. ServerAddresses::const_iterator addr = it++; SendStunBindingRequest(*addr); &#125;&#125;void UDPPort::SendStunBindingRequest(const rtc::SocketAddress&amp; stun_addr) &#123; if (stun_addr.IsUnresolvedIP()) &#123; ResolveStunAddress(stun_addr); &#125; else if (socket_-&gt;GetState() == rtc::AsyncPacketSocket::STATE_BOUND) &#123; // Check if `server_addr_` is compatible with the port's ip. if (IsCompatibleAddress(stun_addr)) &#123; request_manager_.Send( new StunBindingRequest(this, stun_addr, rtc::TimeMillis())); &#125; else &#123; // Since we can't send stun messages to the server, we should mark this // port ready. const char* reason = "STUN server address is incompatible."; RTC_LOG(LS_WARNING) &lt;&lt; reason; OnStunBindingOrResolveRequestFailed(stun_addr, SERVER_NOT_REACHABLE_ERROR, reason); &#125; &#125;&#125;StunBindingRequest(UDPPort* port, const rtc::SocketAddress&amp; addr, int64_t start_time) : StunRequest(port-&gt;request_manager(), std::make_unique&lt;StunMessage&gt;(STUN_BINDING_REQUEST)), port_(port), server_addr_(addr), start_time_(start_time) &#123;&#125; const rtc::SocketAddress&amp; server_addr() const &#123; return server_addr_; &#125; void OnResponse(StunMessage* response) override &#123; const StunAddressAttribute* addr_attr = response-&gt;GetAddress(STUN_ATTR_MAPPED_ADDRESS); if (!addr_attr) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Binding response missing mapped address."; &#125; else if (addr_attr-&gt;family() != STUN_ADDRESS_IPV4 &amp;&amp; addr_attr-&gt;family() != STUN_ADDRESS_IPV6) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Binding address has bad family"; &#125; else &#123; rtc::SocketAddress addr(addr_attr-&gt;ipaddr(), addr_attr-&gt;port()); port_-&gt;OnStunBindingRequestSucceeded(this-&gt;Elapsed(), server_addr_, addr); &#125; // The keep-alive requests will be stopped after its lifetime has passed. if (WithinLifetime(rtc::TimeMillis())) &#123; port_-&gt;request_manager_.SendDelayed( new StunBindingRequest(port_, server_addr_, start_time_), port_-&gt;stun_keepalive_delay()); &#125; &#125;void UDPPort::OnStunBindingRequestSucceeded( int rtt_ms, const rtc::SocketAddress&amp; stun_server_addr, const rtc::SocketAddress&amp; stun_reflected_addr) &#123; RTC_DCHECK(stats_.stun_binding_responses_received &lt; stats_.stun_binding_requests_sent); stats_.stun_binding_responses_received++; stats_.stun_binding_rtt_ms_total += rtt_ms; stats_.stun_binding_rtt_ms_squared_total += rtt_ms * rtt_ms; if (bind_request_succeeded_servers_.find(stun_server_addr) != bind_request_succeeded_servers_.end()) &#123; return; &#125; bind_request_succeeded_servers_.insert(stun_server_addr); // If socket is shared and `stun_reflected_addr` is equal to local socket // address and mDNS obfuscation is not enabled, or if the same address has // been added by another STUN server, then discarding the stun address. // For STUN, related address is the local socket address. if ((!SharedSocket() || stun_reflected_addr != socket_-&gt;GetLocalAddress() || Network()-&gt;GetMdnsResponder() != nullptr) &amp;&amp; !HasStunCandidateWithAddress(stun_reflected_addr)) &#123; rtc::SocketAddress related_address = socket_-&gt;GetLocalAddress(); // If we can't stamp the related address correctly, empty it to avoid leak. if (!MaybeSetDefaultLocalAddress(&amp;related_address)) &#123; related_address = rtc::EmptySocketAddressWithFamily(related_address.family()); &#125; rtc::StringBuilder url; url &lt;&lt; "stun:" &lt;&lt; stun_server_addr.hostname() &lt;&lt; ":" &lt;&lt; stun_server_addr.port(); AddAddress(stun_reflected_addr, socket_-&gt;GetLocalAddress(), related_address, UDP_PROTOCOL_NAME, "", "", STUN_PORT_TYPE, ICE_TYPE_PREFERENCE_SRFLX, 0, url.str(), false); &#125; MaybeSetPortCompleteOrError();&#125; udp port 跟 stun/turn server socket连接 发送 SendStunBindingRequest，拿到 related_address。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051.../webrtc/src/p2p/base/port.ccvoid Port::AddAddress(const rtc::SocketAddress&amp; address, const rtc::SocketAddress&amp; base_address, const rtc::SocketAddress&amp; related_address, absl::string_view protocol, absl::string_view relay_protocol, absl::string_view tcptype, absl::string_view type, uint32_t type_preference, uint32_t relay_preference, absl::string_view url, bool is_final) &#123; RTC_DCHECK_RUN_ON(thread_); if (protocol == TCP_PROTOCOL_NAME &amp;&amp; type == LOCAL_PORT_TYPE) &#123; RTC_DCHECK(!tcptype.empty()); &#125; std::string foundation = ComputeFoundation(type, protocol, relay_protocol, base_address); Candidate c(component_, protocol, address, 0U, username_fragment(), password_, type, generation_, foundation, network_-&gt;id(), network_cost_); c.set_priority( c.GetPriority(type_preference, network_-&gt;preference(), relay_preference)); c.set_relay_protocol(relay_protocol); c.set_tcptype(tcptype); c.set_network_name(network_-&gt;name()); c.set_network_type(network_-&gt;type()); c.set_underlying_type_for_vpn(network_-&gt;underlying_type_for_vpn()); c.set_url(url); c.set_related_address(related_address); bool pending = MaybeObfuscateAddress(&amp;c, type, is_final); if (!pending) &#123; FinishAddingAddress(c, is_final); &#125;&#125;void Port::FinishAddingAddress(const Candidate&amp; c, bool is_final) &#123; candidates_.push_back(c); SignalCandidateReady(this, c); PostAddAddress(is_final);&#125;void Port::PostAddAddress(bool is_final) &#123; if (is_final) &#123; SignalPortComplete(this); &#125;&#125; AddAddress 创建 Candidate，通过 SignalCandidateReady 层层回调返回 回去…. conclusion整个 Candidate 的 收集过程就走完了 ~ OVER ~ 在 WebRTC 中，ICE 协议使用 ICE Candidate 信息来描述设备的网络地址。ICE Candidate 包含以下信息： 媒体类型（音频或视频）协议类型（UDP 或 TCP）IP 地址端口号套接字类型（IPv4 或 IPv6）优先级基础地址类型（服务器反射地址、对称 NAT 地址、中继地址等）根据 ICE Candidate 中的基础地址类型，可以将 ICE Candidate 分为以下四种类型： 主机候选（host candidate）：主机候选是指设备的本地地址，即通过 STUN 服务器获取的本地 IP 地址和端口号。主机候选可以直接用于通信，是 ICE 协议中优先级最高的候选类型。服务器反射候选（server reflexive candidate）：服务器反射候选是指通过 STUN 服务器获取的公网 IP 地址和端口号。服务器反射候选可以用于 NAT 环境下的通信，优先级次于主机候选。（stun bingding request）中继候选（relay candidate）：中继候选是指通过 TURN 服务器获取的 IP 地址和端口号。中继候选可以用于 NAT 环境下的通信，但通信质量可能会较差，优先级最低。 连通性测试过程中，在来自对方的数据报文里看到的地址（peer reflexive，缩写为prflx） （Connection Request） referenceJSEP]]></content>
      <categories>
        <category>RTC</category>
      </categories>
      <tags>
        <tag>ios</tag>
        <tag>webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OWT-client-ios信令交互源码分析]]></title>
    <url>%2F2023%2F02%2F15%2FOWT-client-ios%E4%BF%A1%E4%BB%A4%E4%BA%A4%E4%BA%92%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[OWTOWT 信令交互过程: A POST /tokens/A SocketIO connectA SocketIO loginA SocketIO publishA SocketIO soac offerA SocketIO soac candidatePortal SocketIO soac answer B POST /tokens/B SocketIO connectB SocketIO loginB SocketIO subscribeB SocketIO soac offerB SocketIO soac candidatePortal SocketIO soac answer SocketIO logout token12345678910111213141516-(void)getTokenFromBasicSample:(NSString *)basicServer roomId:(NSString *)roomId onSuccess:(void (^)(NSString *))onSuccess onFailure:(void (^)())onFailure&#123; AFHTTPRequestOperationManager *manager = [AFHTTPRequestOperationManager manager]; manager.requestSerializer = [AFJSONRequestSerializer serializer]; [manager.requestSerializer setValue:@"*/*" forHTTPHeaderField:@"Accept"]; [manager.requestSerializer setValue:@"application/json" forHTTPHeaderField:@"Content-Type"]; manager.responseSerializer = [AFHTTPResponseSerializer serializer]; manager.securityPolicy.allowInvalidCertificates=YES; manager.securityPolicy.validatesDomainName=NO; NSDictionary *params = [[NSDictionary alloc]initWithObjectsAndKeys:roomId, @"room", @"user", @"username", @"presenter", @"role", nil]; [manager POST:[basicServer stringByAppendingString:@"createToken/"] parameters:params success:^(AFHTTPRequestOperation *operation, id responseObject) &#123; NSData* data=[[NSData alloc]initWithData:responseObject]; onSuccess([[NSString alloc]initWithData:data encoding:NSUTF8StringEncoding]); &#125; failure:^(AFHTTPRequestOperation *operation, NSError *error) &#123; NSLog(@"Error: %@", error); &#125;];&#125; data base64 decode之后123456&#123; "tokenId":"63ec8bb0e20782d930e787bb", "host":"172.19.35.107:8080", "secure":true, "signature":"NWE0YjQzM2M1Zjk3NDMwMTRkOGM3Nzg5Zjk3MGQ1YWZkM2I2YmI1MWNmZDk2NzM5NGJiYjBhNWRkMDA2NGE2OQ=="&#125; connect上一步 token 的 response， host 就是 socket链接的 url 地址。 connect就是与server建议长连接（socketIO）,OWT.framework对外暴露的OWTConferenceClient中并没有提供connnect的方法，connect的过程其实是在join方法中 join12345678910111213141516171819202122232425262728293031../src/talk/owt/sdk/conference/objc/OWTConferenceClient.mm- (void)joinWithToken:(NSString*)token onSuccess:(void (^)(OWTConferenceInfo*))onSuccess onFailure:(void (^)(NSError*))onFailure &#123; if (token == nil) &#123; if (onFailure != nil) &#123; NSError* err = [[NSError alloc] initWithDomain:OWTErrorDomain code:OWTConferenceErrorUnknown userInfo:[[NSDictionary alloc] initWithObjectsAndKeys:@"Token cannot be nil.", NSLocalizedDescriptionKey, nil]]; onFailure(err); &#125; return; &#125; const std::string nativeToken = [token UTF8String]; __weak OWTConferenceClient *weakSelf = self; _nativeConferenceClient-&gt;Join( nativeToken, [=](std::shared_ptr&lt;owt::conference::ConferenceInfo&gt; info) &#123; if (onSuccess != nil) onSuccess([[OWTConferenceInfo alloc] initWithNativeInfo:info]); &#125;, [=](std::unique_ptr&lt;owt::base::Exception&gt; e) &#123; [weakSelf triggerOnFailure:onFailure withException:(std::move(e))]; &#125;);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122../src/talk/owt/sdk/conference/conferenceclient.ccvoid ConferenceClient::Join( const std::string&amp; token, std::function&lt;void(std::shared_ptr&lt;ConferenceInfo&gt;)&gt; on_success, std::function&lt;void(std::unique_ptr&lt;Exception&gt;)&gt; on_failure) &#123; if (signaling_channel_connected_) &#123; if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure]() &#123; std::unique_ptr&lt;Exception&gt; e( new Exception(ExceptionType::kConferenceUnknown, "Already connected to conference server.")); on_failure(std::move(e)); &#125;); &#125; return; &#125; std::string token_base64(token); if (!StringUtils::IsBase64EncodedString(token)) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "Passing token with Base64 decoded is deprecated, " "please pass it without modification."; token_base64 = rtc::Base64::Encode(token); &#125; signaling_channel_-&gt;Connect( token_base64, [=](sio::message::ptr info) &#123; signaling_channel_connected_ = true; // Get current user's participantId, user ID and role and fill in the // ConferenceInfo. std::string participant_id, user_id, role; if (info-&gt;get_map()["id"]-&gt;get_flag() != sio::message::flag_string || info-&gt;get_map()["user"]-&gt;get_flag() != sio::message::flag_string || info-&gt;get_map()["role"]-&gt;get_flag() != sio::message::flag_string) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Room info doesn't contain participant's ID/uerID/role."; if (on_failure) &#123; event_queue_-&gt;PostTask([on_failure]() &#123; std::unique_ptr&lt;Exception&gt; e( new Exception(ExceptionType::kConferenceUnknown, "Received invalid user info from MCU.")); on_failure(std::move(e)); &#125;); &#125; return; &#125; else &#123; participant_id = info-&gt;get_map()["id"]-&gt;get_string(); user_id = info-&gt;get_map()["user"]-&gt;get_string(); role = info-&gt;get_map()["role"]-&gt;get_string(); const std::lock_guard&lt;std::mutex&gt; lock(conference_info_mutex_); if (current_conference_info_.get()) &#123; current_conference_info_.reset(); &#125; current_conference_info_.reset(new ConferenceInfo); current_conference_info_-&gt;self_.reset( new Participant(participant_id, role, user_id)); &#125; auto room_info = info-&gt;get_map()["room"]; if (room_info == nullptr || room_info-&gt;get_flag() != sio::message::flag_object) &#123; RTC_DCHECK(false); return; &#125; if (room_info-&gt;get_map()["id"]-&gt;get_flag() != sio::message::flag_string) &#123; RTC_DCHECK(false); return; &#125; else &#123; current_conference_info_-&gt;id_ = room_info-&gt;get_map()["id"]-&gt;get_string(); &#125; // Trigger OnUserJoin for existed users, and also fill in the // ConferenceInfo. if (room_info-&gt;get_map()["participants"]-&gt;get_flag() != sio::message::flag_array) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "Room info doesn't contain valid users."; &#125; else &#123; auto users = room_info-&gt;get_map()["participants"]-&gt;get_vector(); // Make sure |on_success| is triggered before any other events because // OnUserJoined and OnStreamAdded should be triggered after join a // conference. for (auto it = users.begin(); it != users.end(); ++it) &#123; TriggerOnUserJoined(*it, true); &#125; &#125; // Trigger OnStreamAdded for existed remote streams, and also fill in // the ConferenceInfo. if (room_info-&gt;get_map()["streams"]-&gt;get_flag() != sio::message::flag_array) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "Room info doesn't contain valid streams."; &#125; else &#123; auto streams = room_info-&gt;get_map()["streams"]-&gt;get_vector(); for (auto it = streams.begin(); it != streams.end(); ++it) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Find streams in the conference."; TriggerOnStreamAdded(*it, true); &#125; &#125;#ifdef OWT_ENABLE_QUIC auto webtransport_token = info-&gt;get_map()["webTransportToken"]; if (webtransport_token != nullptr &amp;&amp; webtransport_token-&gt;get_flag() == sio::message::flag_string) &#123; // Base64 encoded webTransportToken with format: // &#123;tokenId, transportId, participantId, issueTime&#125;. Parse the transportId // and save it. webtransport_token_ = info-&gt;get_map()["webTransportToken"]-&gt;get_string(); bool transport_id_get = ParseWebTransportToken(); // If server provides WebTransport channel, prepare the QUIC client as // well. No underlying webtransport connection setup at this phase. if (transport_id_get) InitializeQuicClientIfSupported(token_base64); &#125;#endif // Invoke the success callback before trigger any participant join or // stream added message. if (on_success) &#123; event_queue_-&gt;PostTask( [on_success, this]() &#123; on_success(current_conference_info_); &#125;); &#125; &#125;, on_failure);&#125; 可以看到 先进行了 token 的校验，然后 进行 signaling_channel_-&gt;Connect，本质上 join 其实就是 执行connect。 publish12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455-(void)doPublish&#123; if (_localStream == nil) &#123;#if TARGET_IPHONE_SIMULATOR NSLog(@"Camera is not supported on simulator"); OWTStreamConstraints* constraints=[[OWTStreamConstraints alloc]init]; constraints.audio=YES; constraints.video=nil;#else /* Create LocalStream with constraints */ OWTStreamConstraints* constraints=[[OWTStreamConstraints alloc] init]; constraints.audio=YES; constraints.video=[[OWTVideoTrackConstraints alloc] init]; constraints.video.frameRate=24; constraints.video.resolution=CGSizeMake(640,480); constraints.video.devicePosition=AVCaptureDevicePositionFront;#endif RTCMediaStream *localRTCStream = [self createLocalSenderStream:constraints]; OWTStreamSourceInfo *sourceinfo = [[OWTStreamSourceInfo alloc] init]; sourceinfo.audio = OWTAudioSourceInfoMic; sourceinfo.video = OWTVideoSourceInfoCamera; _localStream=[[OWTLocalStream alloc] initWithMediaStream:localRTCStream source:sourceinfo];#if TARGET_IPHONE_SIMULATOR NSLog(@"Stream does not have video track.");#else dispatch_async(dispatch_get_main_queue(), ^&#123; [((SFUStreamView *)self.view).localVideoView setCaptureSession:[_capturer captureSession] ]; &#125;);#endif OWTPublishOptions* options=[[OWTPublishOptions alloc] init]; OWTAudioCodecParameters* opusParameters=[[OWTAudioCodecParameters alloc] init]; opusParameters.name=OWTAudioCodecOpus; OWTAudioEncodingParameters *audioParameters=[[OWTAudioEncodingParameters alloc] init]; audioParameters.codec=opusParameters; options.audio=[NSArray arrayWithObjects:audioParameters, nil]; OWTVideoCodecParameters *h264Parameters=[[OWTVideoCodecParameters alloc] init]; h264Parameters.name=OWTVideoCodecH264; OWTVideoEncodingParameters *videoParameters=[[OWTVideoEncodingParameters alloc]init]; videoParameters.codec=h264Parameters; options.video=[NSArray arrayWithObjects:videoParameters, nil]; [_conferenceClient publish:_localStream withOptions:options onSuccess:^(OWTConferencePublication* p) &#123; NSLog(@"[ZSPDEBUG Function:%s Line:%d] publish success! OWTConferencePublication:%@ id:%@", __FUNCTION__,__LINE__,p,p.publicationId); _publication=p; _publication.delegate=self; [self mixToCommonView:p]; &#125; onFailure:^(NSError* err) &#123; NSLog(@"publish failure!"); [self showMsg:[err localizedFailureReason]]; &#125;]; _screenStream=appDelegate.screenStream; _remoteStream=appDelegate.mixedStream; [self subscribe]; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940../src/talk/owt/sdk/conference/objc/OWTConferenceClient.mm- (void)publish:(OWTLocalStream*)stream withOptions:(OWTPublishOptions*)options onSuccess:(void (^)(OWTConferencePublication*))onSuccess onFailure:(void (^)(NSError*))onFailure &#123; RTC_CHECK(stream); auto nativeStreamRefPtr = [stream nativeStream]; std::shared_ptr&lt;owt::base::LocalStream&gt; nativeStream( std::static_pointer_cast&lt;owt::base::LocalStream&gt;(nativeStreamRefPtr)); __weak OWTConferenceClient *weakSelf = self; if (options == nil) &#123; _nativeConferenceClient-&gt;Publish( nativeStream, [=](std::shared_ptr&lt;owt::conference::ConferencePublication&gt; publication) &#123; [_publishedStreams setObject:stream forKey:[stream streamId]]; if (onSuccess != nil) onSuccess([[OWTConferencePublication alloc] initWithNativePublication:publication]); &#125;, [=](std::unique_ptr&lt;owt::base::Exception&gt; e) &#123; [weakSelf triggerOnFailure:onFailure withException:(std::move(e))]; &#125;); &#125; else &#123; _nativeConferenceClient-&gt;Publish( nativeStream, *[options nativePublishOptions].get(), [=](std::shared_ptr&lt;owt::conference::ConferencePublication&gt; publication) &#123; [_publishedStreams setObject:stream forKey:[stream streamId]]; if (onSuccess != nil) onSuccess([[OWTConferencePublication alloc] initWithNativePublication:publication]); &#125;, [=](std::unique_ptr&lt;owt::base::Exception&gt; e) &#123; [weakSelf triggerOnFailure:onFailure withException:(std::move(e))]; &#125;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115../src/talk/owt/sdk/conference/conferenceclient.ccvoid ConferenceClient::Publish( std::shared_ptr&lt;LocalStream&gt; stream, const PublishOptions&amp; options, std::function&lt;void(std::shared_ptr&lt;ConferencePublication&gt;)&gt; on_success, std::function&lt;void(std::unique_ptr&lt;Exception&gt;)&gt; on_failure) &#123; if (!CheckNullPointer((uintptr_t)stream.get(), on_failure)) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Local stream cannot be nullptr."; return; &#125; if (!CheckSignalingChannelOnline(on_failure)) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Signaling channel disconnected."; return; &#125;#ifdef OWT_ENABLE_QUIC if (stream-&gt;DataEnabled()) &#123; if (web_transport_channel_ &amp;&amp; web_transport_channel_connected_) &#123; std::weak_ptr&lt;ConferenceClient&gt; weak_this = shared_from_this(); web_transport_channel_-&gt;Publish( stream, [stream, on_success, weak_this](std::string session_id, std::string transport_id) &#123; auto that = weak_this.lock(); if (!that) return; // map current pcc if (on_success != nullptr) &#123; // For QUIC stream we use session_id as stream_id for publication. RTC_LOG(LS_INFO) &lt;&lt; "Publication succeed. Returning session id/transport id:" &lt;&lt; session_id; std::shared_ptr&lt;ConferencePublication&gt; cp( new ConferencePublication(that, session_id, session_id)); &#123; std::lock_guard&lt;std::mutex&gt; lock(that-&gt;quic_publications_mutex_); that-&gt;quic_publications_[session_id] = cp; &#125; RTC_LOG(LS_INFO) &lt;&lt; "Writting session id for stream auth:" &lt;&lt; session_id; // Convert to hex16 and write for stream auth. uint8_t* stream_uuid = new uint8_t[16]; ConvertUUID(session_id.c_str(), stream_uuid); stream-&gt;Stream()-&gt;Write(stream_uuid, 16); delete []stream_uuid; on_success(cp); &#125; &#125;, on_failure); &#125; else &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Cannot publish a quic stream without quic client connected."; std::string failure_message( "Publishing quic stream without quic client connected"); if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure, failure_message]() &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceUnknown, failure_message)); on_failure(std::move(e)); &#125;); &#125; &#125; return; &#125;#endif if (!CheckNullPointer((uintptr_t)(stream-&gt;MediaStream()), on_failure)) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Cannot publish a local stream without media stream."; return; &#125; if (stream-&gt;MediaStream()-&gt;GetAudioTracks().size() == 0 &amp;&amp; stream-&gt;MediaStream()-&gt;GetVideoTracks().size() == 0) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Cannot publish a local stream without audio &amp; video"; std::string failure_message( "Publishing local stream with neither audio nor video."); if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure, failure_message]() &#123; std::unique_ptr&lt;Exception&gt; e( new Exception(ExceptionType::kConferenceUnknown, failure_message)); on_failure(std::move(e)); &#125;); &#125; return; &#125; // Reorder SDP according to perference list. PeerConnectionChannelConfiguration config = GetPeerConnectionChannelConfiguration(); for (auto codec : options.video) &#123; config.video.push_back(VideoEncodingParameters(codec)); &#125; for (auto codec : options.audio) &#123; config.audio.push_back(AudioEncodingParameters(codec)); &#125; std::shared_ptr&lt;ConferencePeerConnectionChannel&gt; pcc( new ConferencePeerConnectionChannel(config, signaling_channel_, event_queue_)); pcc-&gt;AddObserver(*this); &#123; std::lock_guard&lt;std::mutex&gt; lock(publish_pcs_mutex_); publish_pcs_.push_back(pcc); &#125; std::weak_ptr&lt;ConferenceClient&gt; weak_this = shared_from_this(); std::string stream_id = stream-&gt;Id(); pcc-&gt;Publish(stream, [on_success, weak_this, stream_id](std::string session_id) &#123; auto that = weak_this.lock(); if (!that) return; // map current pcc if (on_success != nullptr) &#123; std::shared_ptr&lt;ConferencePublication&gt; cp( new ConferencePublication(that, session_id, stream_id)); on_success(cp); &#125; &#125;, on_failure);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159../src/talk/owt/sdk/conference/ConferencePeerConnectionChannel.ccvoid ConferencePeerConnectionChannel::Publish( std::shared_ptr&lt;LocalStream&gt; stream, std::function&lt;void(std::string)&gt; on_success, std::function&lt;void(std::unique_ptr&lt;Exception&gt;)&gt; on_failure) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Publish a local stream."; published_stream_ = stream; if ((!CheckNullPointer((uintptr_t)stream.get(), on_failure)) || (!CheckNullPointer((uintptr_t)stream-&gt;MediaStream(), on_failure))) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Local stream cannot be nullptr."; &#125; if (IsMediaStreamEnded(stream-&gt;MediaStream())) &#123; if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure]() &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceUnknown, "Cannot publish ended stream.")); on_failure(std::move(e)); &#125;); &#125; return; &#125; int audio_track_count = 0, video_track_count = 0; audio_track_count = stream-&gt;MediaStream()-&gt;GetAudioTracks().size(); video_track_count = stream-&gt;MediaStream()-&gt;GetVideoTracks().size(); if (audio_track_count == 0 &amp;&amp; video_track_count == 0) &#123; if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure]() &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceUnknown, "Cannot publish media stream without any tracks.")); on_failure(std::move(e)); &#125;); &#125; return; &#125; publish_success_callback_ = on_success; failure_callback_ = on_failure; audio_transceiver_direction_=webrtc::RtpTransceiverDirection::kSendOnly; video_transceiver_direction_=webrtc::RtpTransceiverDirection::kSendOnly; sio::message::ptr options = sio::object_message::create(); // attributes sio::message::ptr attributes_ptr = sio::object_message::create(); for (auto const&amp; attr : stream-&gt;Attributes()) &#123; attributes_ptr-&gt;get_map()[attr.first] = sio::string_message::create(attr.second); &#125; options-&gt;get_map()[kStreamOptionAttributesKey] = attributes_ptr; // TODO(jianlin): Currently we fix mid to 0/1. Need // to update the flow to set local desc for retrieving the mid. // See https://github.com/open-webrtc-toolkit/owt-client-native/issues/459 // for more details. sio::message::ptr media_ptr = sio::object_message::create(); sio::message::ptr tracks_ptr = sio::array_message::create(); if (audio_track_count != 0) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Adding audio tracks for publish."; sio::message::ptr audio_options = sio::object_message::create(); audio_options-&gt;get_map()["type"] = sio::string_message::create("audio"); audio_options-&gt;get_map()["mid"] = sio::string_message::create("0"); if (stream-&gt;Source().audio == owt::base::AudioSourceInfo::kScreenCast) &#123; audio_options-&gt;get_map()["source"] = sio::string_message::create("screen-cast"); &#125; else &#123; audio_options-&gt;get_map()["source"] = sio::string_message::create("mic"); &#125; tracks_ptr-&gt;get_vector().push_back(audio_options); &#125; if (video_track_count != 0) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Adding video tracks for publish."; sio::message::ptr video_options = sio::object_message::create(); video_options-&gt;get_map()["type"] = sio::string_message::create("video"); if (audio_track_count == 0) &#123; video_options-&gt;get_map()["mid"] = sio::string_message::create("0"); &#125; else &#123; video_options-&gt;get_map()["mid"] = sio::string_message::create("1"); &#125; if (stream-&gt;Source().video == owt::base::VideoSourceInfo::kScreenCast) &#123; video_options-&gt;get_map()["source"] = sio::string_message::create("screen-cast"); &#125; else &#123; video_options-&gt;get_map()["source"] = sio::string_message::create("camera"); &#125; tracks_ptr-&gt;get_vector().push_back(video_options); &#125; media_ptr-&gt;get_map()["tracks"] = tracks_ptr; options-&gt;get_map()["media"] = media_ptr; sio::message::ptr transport_ptr = sio::object_message::create(); transport_ptr-&gt;get_map()["type"] = sio::string_message::create("webrtc"); options-&gt;get_map()["transport"] = transport_ptr; SendPublishMessage(options, stream, on_failure);&#125;void ConferencePeerConnectionChannel::SendPublishMessage( sio::message::ptr options, std::shared_ptr&lt;LocalStream&gt; stream, std::function&lt;void(std::unique_ptr&lt;Exception&gt;)&gt; on_failure) &#123; signaling_channel_-&gt;SendInitializationMessage( options, stream-&gt;MediaStream()-&gt;id(), "", [stream, this](std::string session_id, std::string transport_id) &#123; SetSessionId(session_id); for (const auto&amp; track : stream-&gt;MediaStream()-&gt;GetAudioTracks()) &#123; webrtc::RtpTransceiverInit transceiver_init; transceiver_init.stream_ids.push_back(stream-&gt;MediaStream()-&gt;id()); transceiver_init.direction = webrtc::RtpTransceiverDirection::kSendOnly; AddTransceiver(track, transceiver_init); &#125; for (const auto&amp; track : stream-&gt;MediaStream()-&gt;GetVideoTracks()) &#123; webrtc::RtpTransceiverInit transceiver_init; transceiver_init.stream_ids.push_back(stream-&gt;MediaStream()-&gt;id()); transceiver_init.direction = webrtc::RtpTransceiverDirection::kSendOnly; if (configuration_.video.size() &gt; 0 &amp;&amp; configuration_.video[0].rtp_encoding_parameters.size() != 0) &#123; for (auto encoding : configuration_.video[0].rtp_encoding_parameters) &#123; webrtc::RtpEncodingParameters param; if (encoding.rid != "") param.rid = encoding.rid; if (encoding.max_bitrate_bps != 0) param.max_bitrate_bps = encoding.max_bitrate_bps; if (encoding.max_framerate != 0) param.max_framerate = encoding.max_framerate; if (encoding.scale_resolution_down_by &gt; 0) param.scale_resolution_down_by = encoding.scale_resolution_down_by; if (encoding.num_temporal_layers &gt; 0 &amp;&amp; encoding.num_temporal_layers &lt;= 4) &#123; param.num_temporal_layers = encoding.num_temporal_layers; &#125; if (encoding.priority != owt::base::NetworkPriority::kDefault) &#123; switch (encoding.priority) &#123; case owt::base::NetworkPriority::kVeryLow: param.network_priority = webrtc::Priority::kVeryLow; break; case owt::base::NetworkPriority::kLow: param.network_priority = webrtc::Priority::kLow; break; case owt::base::NetworkPriority::kMedium: param.network_priority = webrtc::Priority::kMedium; break; case owt::base::NetworkPriority::kHigh: param.network_priority = webrtc::Priority::kHigh; break; default: break; &#125; &#125; param.active = encoding.active; transceiver_init.send_encodings.push_back(param); &#125; &#125; AddTransceiver(track, transceiver_init); &#125; CreateOffer(); &#125;, on_failure);&#125; const std::string kEventNamePublish = “publish”; publish_stream_label 不为空，发送publish message 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475void ConferenceSocketSignalingChannel::SendInitializationMessage( sio::message::ptr options, std::string publish_stream_label, std::string subscribe_stream_label, std::function&lt;void(std::string, std::string)&gt; on_success, std::function&lt;void(std::unique_ptr&lt;Exception&gt;)&gt; on_failure) &#123; sio::message::list message_list; message_list.push(options); std::string event_name; if (publish_stream_label != "") event_name = kEventNamePublish; else if (subscribe_stream_label != "") event_name = kEventNameSubscribe; Emit(event_name, message_list, [=](sio::message::list const&amp; msg) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Received ack from server."; if (on_success == nullptr) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "Does not implement success callback. Make sure " "it is what you want."; return; &#125; sio::message::ptr message = msg.at(0); if (message-&gt;get_flag() != sio::message::flag_string) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "The first element of publish ack is not a string."; if (on_failure) &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceInvalidParam, "Received unkown message from server.")); on_failure(std::move(e)); &#125; return; &#125; if (message-&gt;get_string() == "ok") &#123; if (msg.at(1)-&gt;get_flag() != sio::message::flag_object) &#123; RTC_DCHECK(false); return; &#125; // TODO: Spec returns &#123;transportId, publication/subscriptionId&#125; while server impl // is currently returning id and transportId. RTC_LOG(LS_ERROR) &lt;&lt; "Fetching transport ID:"; std::string session_id = msg.at(1)-&gt;get_map()["id"]-&gt;get_string(); std::string transport_id(""); auto transport_id_obj = msg.at(1)-&gt;get_map()["transportId"]; if (transport_id_obj != nullptr &amp;&amp; transport_id_obj-&gt;get_flag() == sio::message::flag_string) &#123; transport_id = transport_id_obj-&gt;get_string(); &#125; RTC_LOG(LS_ERROR) &lt;&lt; "Session ID:" &lt;&lt; session_id &lt;&lt; ", TransportID:" &lt;&lt; transport_id; if (event_name == kEventNamePublish || event_name == kEventNameSubscribe) &#123; on_success(session_id, transport_id); return; &#125; return; &#125; else if (message-&gt;get_string() == "error" &amp;&amp; msg.at(1) != nullptr &amp;&amp; msg.at(1)-&gt;get_flag() == sio::message::flag_string) &#123; if (on_failure) &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceNotSupported, msg.at(1)-&gt;get_string())); on_failure(std::move(e)); &#125; &#125; else &#123; if (on_failure) &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceInvalidParam, "Ack for initializing message is not expected.")); on_failure(std::move(e)); &#125; return; &#125; &#125;, on_failure);&#125; 12345678../src/talk/owt/sdk/base/PeerConnectionChannel.ccvoid PeerConnectionChannel::AddTransceiver( cricket::MediaType media_type, const webrtc::RtpTransceiverInit&amp; init) &#123; peer_connection_-&gt;AddTransceiver(media_type, init);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110.../src/third_party/webrtc/pc/peer_connection.ccRTCErrorOr&lt;rtc::scoped_refptr&lt;RtpTransceiverInterface&gt;&gt;PeerConnection::AddTransceiver( cricket::MediaType media_type, rtc::scoped_refptr&lt;MediaStreamTrackInterface&gt; track, const RtpTransceiverInit&amp; init, bool update_negotiation_needed) &#123; RTC_DCHECK_RUN_ON(signaling_thread()); if (!ConfiguredForMedia()) &#123; LOG_AND_RETURN_ERROR(RTCErrorType::UNSUPPORTED_OPERATION, "Not configured for media"); &#125; RTC_DCHECK((media_type == cricket::MEDIA_TYPE_AUDIO || media_type == cricket::MEDIA_TYPE_VIDEO)); if (track) &#123; RTC_DCHECK_EQ(media_type, (track-&gt;kind() == MediaStreamTrackInterface::kAudioKind ? cricket::MEDIA_TYPE_AUDIO : cricket::MEDIA_TYPE_VIDEO)); &#125; RTC_HISTOGRAM_COUNTS_LINEAR(kSimulcastNumberOfEncodings, init.send_encodings.size(), 0, 7, 8); size_t num_rids = absl::c_count_if(init.send_encodings, [](const RtpEncodingParameters&amp; encoding) &#123; return !encoding.rid.empty(); &#125;); if (num_rids &gt; 0 &amp;&amp; num_rids != init.send_encodings.size()) &#123; LOG_AND_RETURN_ERROR( RTCErrorType::INVALID_PARAMETER, "RIDs must be provided for either all or none of the send encodings."); &#125; if (num_rids &gt; 0 &amp;&amp; absl::c_any_of(init.send_encodings, [](const RtpEncodingParameters&amp; encoding) &#123; return !IsLegalRsidName(encoding.rid); &#125;)) &#123; LOG_AND_RETURN_ERROR(RTCErrorType::INVALID_PARAMETER, "Invalid RID value provided."); &#125; if (absl::c_any_of(init.send_encodings, [](const RtpEncodingParameters&amp; encoding) &#123; return encoding.ssrc.has_value(); &#125;)) &#123; LOG_AND_RETURN_ERROR( RTCErrorType::UNSUPPORTED_PARAMETER, "Attempted to set an unimplemented parameter of RtpParameters."); &#125; RtpParameters parameters; parameters.encodings = init.send_encodings; // Encodings are dropped from the tail if too many are provided. size_t max_simulcast_streams = media_type == cricket::MEDIA_TYPE_VIDEO ? kMaxSimulcastStreams : 1u; if (parameters.encodings.size() &gt; max_simulcast_streams) &#123; parameters.encodings.erase( parameters.encodings.begin() + max_simulcast_streams, parameters.encodings.end()); &#125; // Single RID should be removed. if (parameters.encodings.size() == 1 &amp;&amp; !parameters.encodings[0].rid.empty()) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Removing RID: " &lt;&lt; parameters.encodings[0].rid &lt;&lt; "."; parameters.encodings[0].rid.clear(); &#125; // If RIDs were not provided, they are generated for simulcast scenario. if (parameters.encodings.size() &gt; 1 &amp;&amp; num_rids == 0) &#123; rtc::UniqueStringGenerator rid_generator; for (RtpEncodingParameters&amp; encoding : parameters.encodings) &#123; encoding.rid = rid_generator(); &#125; &#125; if (UnimplementedRtpParameterHasValue(parameters)) &#123; LOG_AND_RETURN_ERROR( RTCErrorType::UNSUPPORTED_PARAMETER, "Attempted to set an unimplemented parameter of RtpParameters."); &#125; auto result = cricket::CheckRtpParametersValues(parameters); if (!result.ok()) &#123; LOG_AND_RETURN_ERROR(result.type(), result.message()); &#125; RTC_LOG(LS_INFO) &lt;&lt; "Adding " &lt;&lt; cricket::MediaTypeToString(media_type) &lt;&lt; " transceiver in response to a call to AddTransceiver."; // Set the sender ID equal to the track ID if the track is specified unless // that sender ID is already in use. std::string sender_id = (track &amp;&amp; !rtp_manager()-&gt;FindSenderById(track-&gt;id()) ? track-&gt;id() : rtc::CreateRandomUuid()); auto sender = rtp_manager()-&gt;CreateSender( media_type, sender_id, track, init.stream_ids, parameters.encodings); auto receiver = rtp_manager()-&gt;CreateReceiver(media_type, rtc::CreateRandomUuid()); auto transceiver = rtp_manager()-&gt;CreateAndAddTransceiver(sender, receiver); transceiver-&gt;internal()-&gt;set_direction(init.direction); if (update_negotiation_needed) &#123; sdp_handler_-&gt;UpdateNegotiationNeeded(); &#125; return rtc::scoped_refptr&lt;RtpTransceiverInterface&gt;(transceiver);&#125; 从 mediastream 获取 tracks 的信息，audio &amp; video 构造publish message, 通过 signaling_channel_ 发送到 OWT server。 publish的过程最终还是进到了 webrtc里面 peer_connection的 AddTransceiver方法。AddTransceiver 创建 transceiver ，Transceiver表示的是收发相同mid的receiver和sender的一个组合体 ，负责收发媒体数据，以Track为载体。 offerpublish 消息发送成功之后，就构造offer message , SetLocalDescription 后通过 signaling_channel_ 发送到 OWT server。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172../src/talk/owt/sdk/conference/ConferencePeerConnectionChannel.ccvoid ConferencePeerConnectionChannel::CreateOffer() &#123; RTC_LOG(LS_INFO) &lt;&lt; "Create offer."; scoped_refptr&lt;FunctionalCreateSessionDescriptionObserver&gt; observer = FunctionalCreateSessionDescriptionObserver::Create( std::bind(&amp;ConferencePeerConnectionChannel:: OnCreateSessionDescriptionSuccess, this, std::placeholders::_1), std::bind(&amp;ConferencePeerConnectionChannel:: OnCreateSessionDescriptionFailure, this, std::placeholders::_1)); bool rtp_no_mux = webrtc::field_trial::IsEnabled("OWT-IceUnbundle"); auto offer_answer_options = webrtc::PeerConnectionInterface::RTCOfferAnswerOptions(); offer_answer_options.use_rtp_mux = !rtp_no_mux; peer_connection_-&gt;CreateOffer(observer.get(), offer_answer_options);&#125;void ConferencePeerConnectionChannel::OnCreateSessionDescriptionSuccess( webrtc::SessionDescriptionInterface* desc) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Create sdp success."; scoped_refptr&lt;FunctionalSetSessionDescriptionObserver&gt; observer = FunctionalSetSessionDescriptionObserver::Create( std::bind(&amp;ConferencePeerConnectionChannel:: OnSetLocalSessionDescriptionSuccess, this), std::bind(&amp;ConferencePeerConnectionChannel:: OnSetLocalSessionDescriptionFailure, this, std::placeholders::_1)); std::string sdp_string; if (!desc-&gt;ToString(&amp;sdp_string)) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Error parsing local description."; RTC_DCHECK(false); &#125; std::vector&lt;AudioCodec&gt; audio_codecs; for (auto&amp; audio_enc_param : configuration_.audio) &#123; audio_codecs.push_back(audio_enc_param.codec.name); &#125; sdp_string = SdpUtils::SetPreferAudioCodecs(sdp_string, audio_codecs); std::vector&lt;VideoCodec&gt; video_codecs; for (auto&amp; video_enc_param : configuration_.video) &#123; video_codecs.push_back(video_enc_param.codec.name); &#125; bool is_screen = published_stream_.get() ? (published_stream_-&gt;Source().video == owt::base::VideoSourceInfo::kScreenCast) : (subscribed_stream_.get() ? (subscribed_stream_-&gt;Source().video == owt::base::VideoSourceInfo::kScreenCast) : false); sdp_string = SdpUtils::SetPreferVideoCodecs(sdp_string, video_codecs, is_screen); webrtc::SessionDescriptionInterface* new_desc( webrtc::CreateSessionDescription(desc-&gt;type(), sdp_string, nullptr)); peer_connection_-&gt;SetLocalDescription(observer.get(), new_desc);&#125;void ConferencePeerConnectionChannel::OnSetLocalSessionDescriptionSuccess() &#123; RTC_LOG(LS_INFO) &lt;&lt; "Set local sdp success."; // For conference, it's now OK to set bandwidth ApplyBitrateSettings(); auto desc = LocalDescription(); string sdp; desc-&gt;ToString(&amp;sdp); sio::message::ptr message = sio::object_message::create(); message-&gt;get_map()["id"] = sio::string_message::create(session_id_); sio::message::ptr sdp_message = sio::object_message::create(); sdp_message-&gt;get_map()["type"] = sio::string_message::create(desc-&gt;type()); sdp_message-&gt;get_map()["sdp"] = sio::string_message::create(sdp); message-&gt;get_map()["signaling"] = sdp_message; signaling_channel_-&gt;SendSdp(message, nullptr, nullptr);&#125; 1234567891011121314151617181920../src/talk/owt/sdk/conference/ConferenceSocketSignalingChannel.ccvoid ConferenceSocketSignalingChannel::SendSdp( sio::message::ptr message, std::function&lt;void()&gt; on_success, std::function&lt;void(std::unique_ptr&lt;Exception&gt;)&gt; on_failure) &#123; std::weak_ptr&lt;ConferenceSocketSignalingChannel&gt; weak_this = shared_from_this(); sio::message::list message_list(message); // Add a null message for |to_to_deprecated|. Don't know its meaning. message_list.push(sio::null_message::create()); Emit(kEventNameSignalingMessage, message_list, [weak_this, on_success, on_failure](sio::message::list const&amp; msg) &#123; if (auto that = weak_this.lock()) &#123; that-&gt;OnEmitAck(msg, on_success, on_failure); &#125; &#125;, on_failure);&#125; answer先看下 ConferenceSocketSignalingChannel channel 方法中 这段逻辑 123456789101112../src/talk/owt/sdk/conference/ConferenceSocketSignalingChannel.ccfor (const std::string&amp; notification_name : &#123;kEventNameStreamMessage, kEventNameTextMessage, kEventNameOnUserPresence, kEventNameOnSignalingMessage, kEventNameOnDrop&#125;) &#123; socket_client_-&gt;socket()-&gt;on( notification_name, sio::socket::event_listener_aux(std::bind( &amp;ConferenceSocketSignalingChannel::OnNotificationFromServer, this, std::placeholders::_1, std::placeholders::_2))); &#125; const std::string kEventNameSignalingMessage = “soac”; //only for soac messageconst std::string kEventNameOnSignalingMessage = “progress”; 发送offer 是 kEventNameSignalingMessage soac 事件， OWT server 收到 offer 之后，会通过 progress事件 返回 answer， 看下 OnNotificationFromServer 逻辑 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108../src/talk/owt/sdk/conference/ConferenceSocketSignalingChannel.ccvoid ConferenceSocketSignalingChannel::OnNotificationFromServer( const std::string&amp; name, sio::message::ptr const&amp; data) &#123; if (name == kEventNameStreamMessage) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Received stream event."; if (data-&gt;get_map()["status"] != nullptr &amp;&amp; data-&gt;get_map()["status"]-&gt;get_flag() == sio::message::flag_string &amp;&amp; data-&gt;get_map()["id"] != nullptr &amp;&amp; data-&gt;get_map()["id"]-&gt;get_flag() == sio::message::flag_string) &#123; std::string stream_status = data-&gt;get_map()["status"]-&gt;get_string(); std::string stream_id = data-&gt;get_map()["id"]-&gt;get_string(); if (stream_status == "add") &#123; auto stream_info = data-&gt;get_map()["data"]; if (stream_info != nullptr &amp;&amp; stream_info-&gt;get_flag() == sio::message::flag_object) &#123; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnStreamAdded(stream_info); &#125; &#125; &#125; else if (stream_status == "update") &#123; sio::message::ptr update_message = sio::object_message::create(); update_message-&gt;get_map()["id"] = sio::string_message::create(stream_id); auto stream_update = data-&gt;get_map()["data"]; if (stream_update != nullptr &amp;&amp; stream_update-&gt;get_flag() == sio::message::flag_object) &#123; update_message-&gt;get_map()["event"] = stream_update; &#125; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnStreamUpdated(update_message); &#125; &#125; else if (stream_status == "remove") &#123; sio::message::ptr remove_message = sio::object_message::create(); remove_message-&gt;get_map()["id"] = sio::string_message::create(stream_id); std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnStreamRemoved(remove_message); &#125; &#125; &#125; &#125; else if (name == kEventNameTextMessage) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Received custom message."; std::string from = data-&gt;get_map()["from"]-&gt;get_string(); std::string message = data-&gt;get_map()["message"]-&gt;get_string(); std::string to = "me"; auto target = data-&gt;get_map()["to"]; if (target != nullptr &amp;&amp; target-&gt;get_flag() == sio::message::flag_string) &#123; to = target-&gt;get_string(); &#125; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnCustomMessage(from, message, to); &#125; &#125; else if (name == kEventNameOnUserPresence) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Received user join/leave message."; if (data == nullptr || data-&gt;get_flag() != sio::message::flag_object || data-&gt;get_map()["action"] == nullptr || data-&gt;get_map()["action"]-&gt;get_flag() != sio::message::flag_string) &#123; RTC_DCHECK(false); return; &#125; auto participant_action = data-&gt;get_map()["action"]-&gt;get_string(); if (participant_action == "join") &#123; // Get the pariticipant ID from data; auto participant_info = data-&gt;get_map()["data"]; if (participant_info != nullptr &amp;&amp; participant_info-&gt;get_flag() == sio::message::flag_object &amp;&amp; participant_info-&gt;get_map()["id"] != nullptr &amp;&amp; participant_info-&gt;get_map()["id"]-&gt;get_flag() == sio::message::flag_string) &#123; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnUserJoined(participant_info); &#125; &#125; &#125; else if (participant_action == "leave") &#123; auto participant_info = data-&gt;get_map()["data"]; if (participant_info != nullptr &amp;&amp; participant_info-&gt;get_flag() == sio::message::flag_string) &#123; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnUserLeft(participant_info); &#125; &#125; &#125; else &#123; RTC_DCHECK_NOTREACHED(); &#125; &#125; else if (name == kEventNameOnSignalingMessage) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Received signaling message from server."; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnSignalingMessage(data); &#125; &#125; else if (name == kEventNameOnDrop) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Received drop message."; socket_client_-&gt;set_reconnect_attempts(0); std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnServerDisconnected(); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105.../src/talk/owt/sdk/conference/conferencepeerconnectionchannel.ccvoid ConferencePeerConnectionChannel::OnSignalingMessage( sio::message::ptr message) &#123; if (message == nullptr) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Ignore empty signaling message"; return; &#125; if (message-&gt;get_flag() == sio::message::flag_string) &#123; if (message-&gt;get_string() == "success") &#123; std::weak_ptr&lt;ConferencePeerConnectionChannel&gt; weak_this = shared_from_this(); if (publish_success_callback_) &#123; event_queue_-&gt;PostTask([weak_this] &#123; auto that = weak_this.lock(); std::lock_guard&lt;std::mutex&gt; lock(that-&gt;callback_mutex_); if (!that || !that-&gt;publish_success_callback_) return; that-&gt;publish_success_callback_(that-&gt;GetSessionId()); that-&gt;ResetCallbacks(); &#125;); &#125; else if (subscribe_success_callback_) &#123; bool stream_added = false; &#123; std::lock_guard&lt;std::mutex&gt; lock(sub_stream_added_mutex_); stream_added = sub_stream_added_; sub_server_ready_ = true; if (stream_added) &#123; event_queue_-&gt;PostTask([weak_this] &#123; auto that = weak_this.lock(); std::lock_guard&lt;std::mutex&gt; lock(that-&gt;callback_mutex_); if (!that || !that-&gt;subscribe_success_callback_) return; that-&gt;subscribe_success_callback_(that-&gt;GetSessionId()); that-&gt;ResetCallbacks(); &#125;); sub_server_ready_ = false; sub_stream_added_ = false; &#125; &#125; &#125; return; &#125; else if (message-&gt;get_string() == "failure") &#123; if (!connected_ &amp;&amp; failure_callback_) &#123; std::weak_ptr&lt;ConferencePeerConnectionChannel&gt; weak_this = shared_from_this(); event_queue_-&gt;PostTask([weak_this] &#123; auto that = weak_this.lock(); std::lock_guard&lt;std::mutex&gt; lock(that-&gt;callback_mutex_); if (!that || !that-&gt;failure_callback_) return; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceUnknown, "Server internal error during connection establishment.")); that-&gt;failure_callback_(std::move(e)); that-&gt;ResetCallbacks(); &#125;); &#125; &#125; return; &#125; else if (message-&gt;get_flag() != sio::message::flag_object) &#123; RTC_LOG(LS_WARNING) &lt;&lt; "Ignore invalid signaling message from server."; return; &#125; // Since trickle ICE from server is not supported, we parse the message as // SOAC message, not Canddiate message. if (message-&gt;get_map().find("type") == message-&gt;get_map().end()) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Ignore message without type from server."; return; &#125; if (message-&gt;get_map()["type"]-&gt;get_flag() != sio::message::flag_string || message-&gt;get_map()["sdp"] == nullptr || message-&gt;get_map()["sdp"]-&gt;get_flag() != sio::message::flag_string) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Invalid signaling message"; return; &#125; const std::string type = message-&gt;get_map()["type"]-&gt;get_string(); RTC_LOG(LS_INFO) &lt;&lt; "On signaling message: " &lt;&lt; type; if (type == "answer") &#123; const std::string sdp = message-&gt;get_map()["sdp"]-&gt;get_string(); SetRemoteDescription(type, sdp); &#125; else &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Ignoring signaling message from server other than answer."; &#125;&#125;void ConferencePeerConnectionChannel::SetRemoteDescription( const std::string&amp; type, const std::string&amp; sdp) &#123; std::unique_ptr&lt;webrtc::SessionDescriptionInterface&gt; desc( webrtc::CreateSessionDescription( "answer", sdp, nullptr)); // TODO(jianjun): change answer to type.toLowerCase. if (!desc) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Failed to create session description."; return; &#125; scoped_refptr&lt;FunctionalSetRemoteDescriptionObserver&gt; observer = FunctionalSetRemoteDescriptionObserver::Create(std::bind( &amp;ConferencePeerConnectionChannel::OnSetRemoteDescriptionComplete, this, std::placeholders::_1)); peer_connection_-&gt;SetRemoteDescription(std::move(desc), observer);&#125; 123456789101112131415161718.../src/talk/owt/sdk/base/peerconnectionchannel.ccvoid PeerConnectionChannel::OnSetRemoteDescriptionComplete( webrtc::RTCError error) &#123; if (error.ok()) &#123; OnSetRemoteSessionDescriptionSuccess(); &#125; else &#123; OnSetRemoteSessionDescriptionFailure(error.message()); &#125;&#125;void PeerConnectionChannel::OnSetRemoteSessionDescriptionSuccess() &#123; RTC_LOG(LS_INFO) &lt;&lt; "Set remote sdp success."; if (peer_connection_-&gt;remote_description() &amp;&amp; peer_connection_-&gt;remote_description()-&gt;type() == "offer") &#123; CreateAnswer(); &#125;&#125; 如果收到对方发的offer，才需要创建answer，如果是SFU模式，client 都是直接跟 owt server 交互，是不需要创建answer，如果是 MCU模式，就需要 CreateAnswer。 12345678910111213141516171819.../src/talk/owt/sdk/conference/conferencepeerconnectionchannel.ccvoid ConferencePeerConnectionChannel::CreateAnswer() &#123; RTC_LOG(LS_INFO) &lt;&lt; "Create answer."; scoped_refptr&lt;FunctionalCreateSessionDescriptionObserver&gt; observer = FunctionalCreateSessionDescriptionObserver::Create( std::bind(&amp;ConferencePeerConnectionChannel:: OnCreateSessionDescriptionSuccess, this, std::placeholders::_1), std::bind(&amp;ConferencePeerConnectionChannel:: OnCreateSessionDescriptionFailure, this, std::placeholders::_1)); bool rtp_no_mux = webrtc::field_trial::IsEnabled("OWT-IceUnbundle"); auto offer_answer_options = webrtc::PeerConnectionInterface::RTCOfferAnswerOptions(); offer_answer_options.use_rtp_mux = !rtp_no_mux; peer_connection_-&gt;CreateAnswer(observer.get(), offer_answer_options);&#125; CreateAnswer 回调 跟 createoffer 的 回调逻辑处理是一样的 OnCreateSessionDescriptionSuccess，不多赘述 最后也会通过signaling_channel_发给owt server subscribe1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 OWTConferenceSubscribeOptions* subOption = [[OWTConferenceSubscribeOptions alloc] init]; subOption.video=[[OWTConferenceVideoSubscriptionConstraints alloc]init]; OWTVideoCodecParameters* h264Codec = [[OWTVideoCodecParameters alloc] init]; h264Codec.name = OWTVideoCodecH264; h264Codec.profile = @"M"; subOption.video.codecs = [NSArray arrayWithObjects:h264Codec, nil]; subOption.audio = [[OWTConferenceAudioSubscriptionConstraints alloc]init];// OWTAudioCodecParameters* pcmCodec = [[OWTAudioCodecParameters alloc] init];// pcmCodec.name = OWTAudioCodecPcma;// subOption.audio.codecs = [NSArray arrayWithObjects:pcmCodec, nil];// subOption.video.bitrateMultiplier = 2.0f; int width = INT_MAX; int height = INT_MAX; for (NSValue* value in appDelegate.mixedStream.capabilities.video.resolutions) &#123; CGSize resolution=[value CGSizeValue]; if (resolution.width == 640 &amp;&amp; resolution.height == 480) &#123; width = resolution.width; height = resolution.height; break; &#125; if (resolution.width &lt; width &amp;&amp; resolution.height != 0) &#123; width = resolution.width; height = resolution.height; &#125; &#125; [[AVAudioSession sharedInstance] overrideOutputAudioPort:AVAudioSessionPortOverrideSpeaker error:nil]; [_conferenceClient subscribe:appDelegate.mixedStream withOptions:subOption onSuccess:^(OWTConferenceSubscription* subscription) &#123; _subscription=subscription; _subscription.delegate=self; _getStatsTimer = [NSTimer timerWithTimeInterval:1.0 target:self selector:@selector(printStats) userInfo:nil repeats:YES]; [[NSRunLoop mainRunLoop] addTimer:_getStatsTimer forMode:NSDefaultRunLoopMode]; dispatch_async(dispatch_get_main_queue(), ^&#123; _remoteStream = appDelegate.mixedStream; NSLog(@"Subscribe stream success."); //[_remoteStream attach:((SFUStreamView*)self.view).remoteVideoView]; UIView&lt;RTCVideoRenderer&gt; *videoView = [_streamView addRemoteRenderer:_remoteStream]; [_remoteStream attach:videoView]; [_streamView.act stopAnimating]; _subscribedMix = YES; &#125;); &#125; onFailure:^(NSError* err) &#123; NSLog(@"Subscribe stream failed. %@", [err localizedDescription]); &#125;]; 12345678910111213141516171819202122232425262728293031323334353637383940414243.../src/talk/owt/sdk/conference/objc/OWTConferenceClient.mm- (void)subscribe:(OWTRemoteStream*)stream withOptions:(OWTConferenceSubscribeOptions*)options onSuccess:(void (^)(OWTConferenceSubscription*))onSuccess onFailure:(void (^)(NSError*))onFailure &#123; RTC_CHECK(stream); auto nativeStreamRefPtr = [stream nativeStream]; std::shared_ptr&lt;owt::base::RemoteStream&gt; nativeStream( std::static_pointer_cast&lt;owt::base::RemoteStream&gt;(nativeStreamRefPtr)); __weak OWTConferenceClient *weakSelf = self; if (options == nil) &#123; _nativeConferenceClient-&gt;Subscribe( nativeStream, [=](std::shared_ptr&lt;owt::conference::ConferenceSubscription&gt; subscription) &#123; OWTConferenceSubscription* sub = [[OWTConferenceSubscription alloc] initWithNativeSubscription:subscription]; [stream setNativeStream:nativeStream]; if (onSuccess != nil) &#123; onSuccess(sub); &#125; &#125;, [=](std::unique_ptr&lt;owt::base::Exception&gt; e) &#123; [weakSelf triggerOnFailure:onFailure withException:(std::move(e))]; &#125;); &#125; else &#123; _nativeConferenceClient-&gt;Subscribe( nativeStream, *[options nativeSubscribeOptions].get(), [=](std::shared_ptr&lt;owt::conference::ConferenceSubscription&gt; subscription) &#123; OWTConferenceSubscription* sub = [[OWTConferenceSubscription alloc] initWithNativeSubscription:subscription]; [stream setNativeStream:nativeStream]; if (onSuccess != nil) &#123; onSuccess(sub); &#125; &#125;, [=](std::unique_ptr&lt;owt::base::Exception&gt; e) &#123; [weakSelf triggerOnFailure:onFailure withException:(std::move(e))]; &#125;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149.../src/talk/owt/sdk/conference/conferenceclient.ccvoid ConferenceClient::Subscribe( std::shared_ptr&lt;RemoteStream&gt; stream, const SubscribeOptions&amp; options, std::function&lt;void(std::shared_ptr&lt;ConferenceSubscription&gt;)&gt; on_success, std::function&lt;void(std::unique_ptr&lt;Exception&gt;)&gt; on_failure) &#123; if (!CheckSignalingChannelOnline(on_failure)) &#123; return; &#125;#ifdef OWT_ENABLE_QUIC if (stream-&gt;DataEnabled()) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Requesting subscibe of quic stream."; if (web_transport_channel_ &amp;&amp; web_transport_channel_connected_) &#123; std::weak_ptr&lt;ConferenceClient&gt; weak_this = shared_from_this(); web_transport_channel_-&gt;Subscribe( stream, [on_success, weak_this](std::string session_id) &#123; auto that = weak_this.lock(); if (!that) return; if (on_success != nullptr) &#123; // For QUIC stream we use session_id as stream_id for publication. std::shared_ptr&lt;ConferenceSubscription&gt; cp( new ConferenceSubscription(that, session_id, session_id)); &#123; std::lock_guard&lt;std::mutex&gt; lock( that-&gt;quic_subscriptions_mutex_); that-&gt;quic_subscriptions_[session_id] = cp; &#125; // Check if any pending stream for this to be attached. &#123; std::lock_guard&lt;std::mutex&gt; stream_lock( that-&gt;pending_quic_streams_mutex_); if (that-&gt;pending_incoming_streams_.find(session_id) != that-&gt;pending_incoming_streams_.end()) &#123; that-&gt;TriggerOnIncomingStream( session_id, that-&gt;pending_incoming_streams_[session_id]); that-&gt;pending_incoming_streams_.erase(session_id); &#125; &#125; on_success(cp); &#125; &#125;, on_failure); &#125; else &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Cannot subscribe a quic stream without quic client connected."; std::string failure_message( "Subscribing quic stream without quic client connected"); if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure, failure_message]() &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceUnknown, failure_message)); on_failure(std::move(e)); &#125;); &#125; &#125; return; &#125;#endif if (!CheckNullPointer((uintptr_t)stream.get(), on_failure)) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Remote stream cannot be nullptr."; return; &#125; if (added_stream_type_.find(stream-&gt;Id()) == added_stream_type_.end()) &#123; std::string failure_message( "Subscribing an invalid stream. Please check whether this stream is " "removed."); if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure, failure_message]() &#123; std::unique_ptr&lt;Exception&gt; e( new Exception(ExceptionType::kConferenceUnknown, failure_message)); on_failure(std::move(e)); &#125;); &#125; return; &#125; if (options.video.disabled &amp;&amp; options.audio.disabled) &#123; std::string failure_message( "Subscribing with both audio and video disabled is not allowed."); if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure, failure_message]() &#123; std::unique_ptr&lt;Exception&gt; e( new Exception(ExceptionType::kConferenceUnknown, failure_message)); on_failure(std::move(e)); &#125;); &#125; return; &#125; // Avoid subscribing the same stream twice. &#123; std::lock_guard&lt;std::mutex&gt; lock(subscribe_pcs_mutex_); // Search subscirbe pcs auto it = std::find_if( subscribe_pcs_.begin(), subscribe_pcs_.end(), [&amp;](std::shared_ptr&lt;ConferencePeerConnectionChannel&gt; o) -&gt; bool &#123; return o-&gt;GetSubStreamId() == stream-&gt;Id(); &#125;); if (it != subscribe_pcs_.end()) &#123; std::string failure_message( "The same remote stream has already been subscribed. Subcribe after " "it is unsubscribed"); if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure, failure_message]() &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceUnknown, failure_message)); on_failure(std::move(e)); &#125;); &#125; return; &#125; &#125; // Reorder SDP according to perference list. PeerConnectionChannelConfiguration config = GetPeerConnectionChannelConfiguration(); for (auto codec : options.video.codecs) &#123; config.video.push_back(VideoEncodingParameters(codec, 0, false)); &#125; for (auto codec : options.audio.codecs) &#123; config.audio.push_back(AudioEncodingParameters(codec, 0)); &#125; std::shared_ptr&lt;ConferencePeerConnectionChannel&gt; pcc( new ConferencePeerConnectionChannel(config, signaling_channel_, event_queue_)); pcc-&gt;AddObserver(*this); &#123; std::lock_guard&lt;std::mutex&gt; lock(subscribe_pcs_mutex_); subscribe_pcs_.push_back(pcc); &#125; std::weak_ptr&lt;ConferenceClient&gt; weak_this = shared_from_this(); std::string stream_id = stream-&gt;Id(); pcc-&gt;Subscribe( stream, options, [on_success, weak_this, stream_id](std::string session_id) &#123; auto that = weak_this.lock(); if (!that) return; // map current pcc if (on_success != nullptr) &#123; std::shared_ptr&lt;ConferenceSubscription&gt; cp( new ConferenceSubscription(that, session_id, stream_id)); on_success(cp); &#125; &#125;, on_failure);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139.../src/talk/owt/sdk/conference/conferencepeerconnectionchannel.ccvoid ConferencePeerConnectionChannel::Subscribe( std::shared_ptr&lt;RemoteStream&gt; stream, const SubscribeOptions&amp; subscribe_options, std::function&lt;void(std::string)&gt; on_success, std::function&lt;void(std::unique_ptr&lt;Exception&gt;)&gt; on_failure) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Subscribe a remote stream. It has audio? " &lt;&lt; stream-&gt;has_audio_ &lt;&lt; ", has video? " &lt;&lt; stream-&gt;has_video_; if (!SubOptionAllowed(subscribe_options, stream-&gt;Settings(), stream-&gt;Capabilities())) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Subscribe option mismatch with stream subcription capabilities."; if (on_failure != nullptr) &#123; event_queue_-&gt;PostTask([on_failure]() &#123; std::unique_ptr&lt;Exception&gt; e( new Exception(ExceptionType::kConferenceUnknown, "Unsupported subscribe option.")); on_failure(std::move(e)); &#125;); &#125; return; &#125; if (!CheckNullPointer((uintptr_t)stream.get(), on_failure)) &#123; RTC_LOG(LS_ERROR) &lt;&lt; "Remote stream cannot be nullptr."; return; &#125; if (subscribe_success_callback_) &#123; if (on_failure) &#123; event_queue_-&gt;PostTask([on_failure]() &#123; std::unique_ptr&lt;Exception&gt; e(new Exception( ExceptionType::kConferenceUnknown, "Subscribing this stream.")); on_failure(std::move(e)); &#125;); &#125; &#125; subscribe_success_callback_ = on_success; failure_callback_ = on_failure; int audio_track_count = 0, video_track_count = 0; if (stream-&gt;has_audio_ &amp;&amp; !subscribe_options.audio.disabled) &#123; webrtc::RtpTransceiverInit transceiver_init; transceiver_init.direction = webrtc::RtpTransceiverDirection::kRecvOnly; AddTransceiver(cricket::MediaType::MEDIA_TYPE_AUDIO, transceiver_init); audio_track_count = 1; &#125; if (stream-&gt;has_video_ &amp;&amp; !subscribe_options.video.disabled) &#123; webrtc::RtpTransceiverInit transceiver_init; transceiver_init.direction = webrtc::RtpTransceiverDirection::kRecvOnly; AddTransceiver(cricket::MediaType::MEDIA_TYPE_VIDEO, transceiver_init); video_track_count = 1; &#125; sio::message::ptr sio_options = sio::object_message::create(); sio::message::ptr media_options = sio::object_message::create(); sio::message::ptr tracks_options = sio::array_message::create(); if (audio_track_count &gt; 0) &#123; sio::message::ptr audio_options = sio::object_message::create(); audio_options-&gt;get_map()["type"] = sio::string_message::create("audio"); audio_options-&gt;get_map()["mid"] = sio::string_message::create("0"); audio_options-&gt;get_map()["from"] = sio::string_message::create(stream-&gt;Id()); tracks_options-&gt;get_vector().push_back(audio_options); &#125; if (video_track_count &gt; 0) &#123; sio::message::ptr video_options = sio::object_message::create(); video_options-&gt;get_map()["type"] = sio::string_message::create("video"); if (audio_track_count == 0) &#123; video_options-&gt;get_map()["mid"] = sio::string_message::create("0"); &#125; else &#123; video_options-&gt;get_map()["mid"] = sio::string_message::create("1"); &#125; auto publication_settings = stream-&gt;Settings(); if (subscribe_options.video.rid != "") &#123; for (auto video_setting : publication_settings.video) &#123; if (video_setting.rid == subscribe_options.video.rid) &#123; std::string track_id = video_setting.track_id; video_options-&gt;get_map()["from"] = sio::string_message::create(track_id); break; &#125; &#125; &#125; else &#123; video_options-&gt;get_map()["from"] = sio::string_message::create(stream-&gt;Id()); &#125; sio::message::ptr video_spec = sio::object_message::create(); sio::message::ptr resolution_options = sio::object_message::create(); if (subscribe_options.video.resolution.width != 0 &amp;&amp; subscribe_options.video.resolution.height != 0) &#123; resolution_options-&gt;get_map()["width"] = sio::int_message::create(subscribe_options.video.resolution.width); resolution_options-&gt;get_map()["height"] = sio::int_message::create(subscribe_options.video.resolution.height); video_spec-&gt;get_map()["resolution"] = resolution_options; &#125; // If bitrateMultiplier is not specified, do not include it in video spec. std::string quality_level("x1.0"); if (subscribe_options.video.bitrateMultiplier != 0) &#123; quality_level = "x" + std::to_string(subscribe_options.video.bitrateMultiplier) .substr(0, 3); &#125; if (quality_level != "x1.0") &#123; sio::message::ptr quality_options = sio::string_message::create(quality_level); video_spec-&gt;get_map()["bitrate"] = quality_options; &#125; if (subscribe_options.video.keyFrameInterval != 0) &#123; video_spec-&gt;get_map()["keyFrameInterval"] = sio::int_message::create(subscribe_options.video.keyFrameInterval); &#125; if (subscribe_options.video.frameRate != 0) &#123; video_spec-&gt;get_map()["framerate"] = sio::int_message::create(subscribe_options.video.frameRate); &#125; video_options-&gt;get_map()["parameters"] = video_spec; if (subscribe_options.video.rid != "") &#123; video_options-&gt;get_map()["simulcastRid"] = sio::string_message::create(subscribe_options.video.rid); &#125; tracks_options-&gt;get_vector().push_back(video_options); &#125; media_options-&gt;get_map()["tracks"] = tracks_options; sio_options-&gt;get_map()["media"] = media_options; sio::message::ptr transport_ptr = sio::object_message::create(); transport_ptr-&gt;get_map()["type"] = sio::string_message::create("webrtc"); sio_options-&gt;get_map()["transport"] = transport_ptr; signaling_channel_-&gt;SendInitializationMessage( sio_options, "", stream-&gt;Id(), [this](std::string session_id, std::string transport_id) &#123; // Pre-set the session's ID. SetSessionId(session_id); CreateOffer(); &#125;, on_failure); // TODO: on_failure subscribed_stream_ = stream;&#125; subscribe 的过程 跟 publish有很多类似的地方，AddTransceiver 创建 sender &amp; receiver 收发媒体流，构造subscribe message 通过 signaling_channel_ 发送 到 owt server发送成功之后，创建offer、发送sdp、setlocalsdp 、接受owt server 的answer、setremotesdp 等等跟publish的过程是一样，不再赘述。 signaling channel看下在 ConferenceSocketSignalingChannel::Connect 方法中的一段代码 1234567891011121314151617const std::string kEventNameStreamMessage = "stream";const std::string kEventNameTextMessage = "text";const std::string kEventNameOnUserPresence = "participant";const std::string kEventNameOnSignalingMessage = "progress";const std::string kEventNameOnDrop = "drop";for (const std::string&amp; notification_name : &#123;kEventNameStreamMessage, kEventNameTextMessage, kEventNameOnUserPresence, kEventNameOnSignalingMessage, kEventNameOnDrop&#125;) &#123; socket_client_-&gt;socket()-&gt;on( notification_name, sio::socket::event_listener_aux(std::bind( &amp;ConferenceSocketSignalingChannel::OnNotificationFromServer, this, std::placeholders::_1, std::placeholders::_2))); &#125; kEventNameOnSignalingMessage 这个之前介绍过了 kEventNameOnUserPresence : 有用户加入或者离开（action 区分），server 通过 participant 事件通知 client，kEventNameStreamMessage : 用户发布流后，server 通过 stream 事件通知 clientkEventNameTextMessage : 自定义消息kEventNameOnDrop : server 断开连接 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107.../src/talk/owt/sdk/conference/conferencesocketsignalingchannel.ccvoid ConferenceSocketSignalingChannel::OnNotificationFromServer( const std::string&amp; name, sio::message::ptr const&amp; data) &#123; if (name == kEventNameStreamMessage) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Received stream event."; if (data-&gt;get_map()["status"] != nullptr &amp;&amp; data-&gt;get_map()["status"]-&gt;get_flag() == sio::message::flag_string &amp;&amp; data-&gt;get_map()["id"] != nullptr &amp;&amp; data-&gt;get_map()["id"]-&gt;get_flag() == sio::message::flag_string) &#123; std::string stream_status = data-&gt;get_map()["status"]-&gt;get_string(); std::string stream_id = data-&gt;get_map()["id"]-&gt;get_string(); if (stream_status == "add") &#123; auto stream_info = data-&gt;get_map()["data"]; if (stream_info != nullptr &amp;&amp; stream_info-&gt;get_flag() == sio::message::flag_object) &#123; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnStreamAdded(stream_info); &#125; &#125; &#125; else if (stream_status == "update") &#123; sio::message::ptr update_message = sio::object_message::create(); update_message-&gt;get_map()["id"] = sio::string_message::create(stream_id); auto stream_update = data-&gt;get_map()["data"]; if (stream_update != nullptr &amp;&amp; stream_update-&gt;get_flag() == sio::message::flag_object) &#123; update_message-&gt;get_map()["event"] = stream_update; &#125; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnStreamUpdated(update_message); &#125; &#125; else if (stream_status == "remove") &#123; sio::message::ptr remove_message = sio::object_message::create(); remove_message-&gt;get_map()["id"] = sio::string_message::create(stream_id); std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnStreamRemoved(remove_message); &#125; &#125; &#125; &#125; else if (name == kEventNameTextMessage) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Received custom message."; std::string from = data-&gt;get_map()["from"]-&gt;get_string(); std::string message = data-&gt;get_map()["message"]-&gt;get_string(); std::string to = "me"; auto target = data-&gt;get_map()["to"]; if (target != nullptr &amp;&amp; target-&gt;get_flag() == sio::message::flag_string) &#123; to = target-&gt;get_string(); &#125; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnCustomMessage(from, message, to); &#125; &#125; else if (name == kEventNameOnUserPresence) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Received user join/leave message."; if (data == nullptr || data-&gt;get_flag() != sio::message::flag_object || data-&gt;get_map()["action"] == nullptr || data-&gt;get_map()["action"]-&gt;get_flag() != sio::message::flag_string) &#123; RTC_DCHECK(false); return; &#125; auto participant_action = data-&gt;get_map()["action"]-&gt;get_string(); if (participant_action == "join") &#123; // Get the pariticipant ID from data; auto participant_info = data-&gt;get_map()["data"]; if (participant_info != nullptr &amp;&amp; participant_info-&gt;get_flag() == sio::message::flag_object &amp;&amp; participant_info-&gt;get_map()["id"] != nullptr &amp;&amp; participant_info-&gt;get_map()["id"]-&gt;get_flag() == sio::message::flag_string) &#123; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnUserJoined(participant_info); &#125; &#125; &#125; else if (participant_action == "leave") &#123; auto participant_info = data-&gt;get_map()["data"]; if (participant_info != nullptr &amp;&amp; participant_info-&gt;get_flag() == sio::message::flag_string) &#123; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnUserLeft(participant_info); &#125; &#125; &#125; else &#123; RTC_DCHECK_NOTREACHED(); &#125; &#125; else if (name == kEventNameOnSignalingMessage) &#123; RTC_LOG(LS_VERBOSE) &lt;&lt; "Received signaling message from server."; std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnSignalingMessage(data); &#125; &#125; else if (name == kEventNameOnDrop) &#123; RTC_LOG(LS_INFO) &lt;&lt; "Received drop message."; socket_client_-&gt;set_reconnect_attempts(0); std::lock_guard&lt;std::mutex&gt; lock(observer_mutex_); for (auto it = observers_.begin(); it != observers_.end(); ++it) &#123; (*it)-&gt;OnServerDisconnected(); &#125; &#125;&#125; observer &amp; delegate各种事件的传递，通过 observer &amp; delegate 回调的方式 从 c++ 到 OC 以conferenceClient举例 1234567891011121314151617181920212223.../src/talk/owt/sdk/conference/objc/OWTConferenceClient.mm- (void)setDelegate:(id&lt;OWTConferenceClientDelegate&gt;)delegate &#123; if (delegate != nil) &#123; __weak OWTConferenceClient *weakSelf = self; _observer = std::unique_ptr&lt; owt::conference::ConferenceClientObserverObjcImpl, std::function&lt;void(owt::conference::ConferenceClientObserverObjcImpl*)&gt;&gt;( new owt::conference::ConferenceClientObserverObjcImpl(weakSelf, delegate), [=](owt::conference::ConferenceClientObserverObjcImpl* observer) &#123; __strong OWTConferenceClient *strongSelf = weakSelf; if (strongSelf != nil) &#123; strongSelf-&gt;_nativeConferenceClient-&gt;RemoveObserver(*observer); &#125; delete observer; &#125;); _nativeConferenceClient-&gt;AddObserver(*_observer.get()); &#125; else &#123; _observer.reset(); &#125; _delegate = delegate;&#125; 通过 ConferenceClientObserverObjcImpl 这个类包装下，打通 oc 跟 c++ 链路。类似的用法 还有 ConferencePublicationObserverObjcImpl 、ConferenceSubscriptionObserverObjcImpl、ParticipantObserverObjcImpl 不一一说了 再来看下 各种 observer 的定义 以及 AddObserver &amp; RemoveObserver 的方法声明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950.../src/talk/owt/sdk/include/cpp/owt/conference/conferenceclient.hclass OWT_EXPORT ConferenceClientObserver &#123; public: /** @brief Triggers when a stream is added. @param stream The stream which is added. */ virtual void OnStreamAdded( std::shared_ptr&lt;RemoteStream&gt; stream)&#123;&#125; /** @brief Triggers when a mixed stream is added. @param stream The stream which is added. */ virtual void OnStreamAdded( std::shared_ptr&lt;RemoteMixedStream&gt; stream)&#123;&#125; /** @brief Triggers when a message is received. @param message Message received. @param sender_id Sender's ID. @param to "all" if it is a broadcast message. "me" if it is sent only to current conference client. */ virtual void OnMessageReceived(std::string&amp; message, std::string&amp; sender_id, std::string&amp; to)&#123;&#125; /** @brief Triggers when a participant joined conference. @param user The user joined. */ virtual void OnParticipantJoined(std::shared_ptr&lt;Participant&gt;)&#123;&#125; /** @brief Triggers when server is disconnected. */ virtual void OnServerDisconnected()&#123;&#125;&#125;;/// An asynchronous class for app to communicate with a conference in MCU.class OWT_EXPORT ConferenceClient final : ConferenceSocketSignalingChannelObserver, ConferencePeerConnectionChannelObserver ... /// Add an observer for conferenc client. void AddObserver(ConferenceClientObserver&amp; observer); /// Remove an object from conference client. void RemoveObserver(ConferenceClientObserver&amp; observer); ... 12345678910111213141516.../src/talk/owt/sdk/conference/conferencepeerconnectionchannel.hclass ConferencePeerConnectionChannel : public PeerConnectionChannel, public std::enable_shared_from_this&lt;ConferencePeerConnectionChannel&gt; &#123; public: // Add a ConferencePeerConnectionChannel observer so it will be notified when // this object have some events. void AddObserver(ConferencePeerConnectionChannelObserver&amp; observer); // Remove a ConferencePeerConnectionChannel observer. If the observer doesn't // exist, it will do nothing. void RemoveObserver(ConferencePeerConnectionChannelObserver&amp; observer); ...&#125; 1234567891011121314151617181920212223242526272829.../src/talk/owt/sdk/include/cpp/owt/conference/conferenceclient.hclass OWT_EXPORT ConferenceSocketSignalingChannelObserver &#123; public: virtual ~ConferenceSocketSignalingChannelObserver()&#123;&#125; virtual void OnUserJoined(std::shared_ptr&lt;sio::message&gt; user) = 0; virtual void OnUserLeft(std::shared_ptr&lt;sio::message&gt; user) = 0; virtual void OnStreamAdded(std::shared_ptr&lt;sio::message&gt; stream) = 0; virtual void OnStreamRemoved(std::shared_ptr&lt;sio::message&gt; stream) = 0; virtual void OnStreamUpdated(std::shared_ptr&lt;sio::message&gt; stream) = 0; virtual void OnServerDisconnected() = 0; virtual void OnCustomMessage(std::string&amp; from, std::string&amp; message, std::string&amp; to) = 0; virtual void OnSignalingMessage(std::shared_ptr&lt;sio::message&gt; message) = 0; virtual void OnStreamError(std::shared_ptr&lt;sio::message&gt; stream) = 0; // Notify the ID for a published/subscribed stream. virtual void OnStreamId(const std::string&amp; id, const std::string&amp; label) = 0; virtual void OnSubscriptionId(const std::string&amp; subscription_id, const std::string&amp; stream_id) = 0;&#125;;class OWT_EXPORT ConferencePeerConnectionChannelObserver &#123; public: virtual ~ConferencePeerConnectionChannelObserver()&#123;&#125; // Triggered when an unrecoverable error happened. Error may reported by MCU // or detected by client. Currently, only errors from MCU are handled. virtual void OnStreamError( std::shared_ptr&lt;Stream&gt; stream, std::shared_ptr&lt;const Exception&gt; exception) = 0;&#125;;]]></content>
      <categories>
        <category>RTC</category>
      </categories>
      <tags>
        <tag>ios</tag>
        <tag>webrtc</tag>
        <tag>OWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ffmpeg源码之xx学习笔记]]></title>
    <url>%2F2023%2F01%2F16%2Fffmpeg%E6%BA%90%E7%A0%81%E4%B9%8Bxx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[AVClass12345678910111213AVFormatContext *s;s-&gt;av_class = &amp;av_format_context_class;static const AVClass av_format_context_class = &#123; .class_name = "AVFormatContext", .item_name = format_to_name, .option = avformat_options, .version = LIBAVUTIL_VERSION_INT, .child_next = format_child_next, .child_class_iterate = format_child_class_iterate, .category = AV_CLASS_CATEGORY_MUXER, .get_category = get_category,&#125;; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#define OFFSET(x) offsetof(AVFormatContext,x)static const AVOption avformat_options[] = &#123;&#123;"avioflags", NULL, OFFSET(avio_flags), AV_OPT_TYPE_FLAGS, &#123;.i64 = DEFAULT &#125;, INT_MIN, INT_MAX, D|E, "avioflags"&#125;,&#123;"direct", "reduce buffering", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVIO_FLAG_DIRECT &#125;, INT_MIN, INT_MAX, D|E, "avioflags"&#125;,&#123;"probesize", "set probing size", OFFSET(probesize), AV_OPT_TYPE_INT64, &#123;.i64 = 5000000 &#125;, 32, INT64_MAX, D&#125;,&#123;"formatprobesize", "number of bytes to probe file format", OFFSET(format_probesize), AV_OPT_TYPE_INT, &#123;.i64 = PROBE_BUF_MAX&#125;, 0, INT_MAX-1, D&#125;,&#123;"packetsize", "set packet size", OFFSET(packet_size), AV_OPT_TYPE_INT, &#123;.i64 = DEFAULT &#125;, 0, INT_MAX, E&#125;,&#123;"fflags", NULL, OFFSET(flags), AV_OPT_TYPE_FLAGS, &#123;.i64 = AVFMT_FLAG_AUTO_BSF &#125;, INT_MIN, INT_MAX, D|E, "fflags"&#125;,&#123;"flush_packets", "reduce the latency by flushing out packets immediately", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_FLUSH_PACKETS &#125;, INT_MIN, INT_MAX, E, "fflags"&#125;,&#123;"ignidx", "ignore index", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_IGNIDX &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"genpts", "generate pts", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_GENPTS &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"nofillin", "do not fill in missing values that can be exactly calculated", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_NOFILLIN &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"noparse", "disable AVParsers, this needs nofillin too", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_NOPARSE &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"igndts", "ignore dts", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_IGNDTS &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"discardcorrupt", "discard corrupted frames", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_DISCARD_CORRUPT &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"sortdts", "try to interleave outputted packets by dts", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_SORT_DTS &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"fastseek", "fast but inaccurate seeks", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_FAST_SEEK &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"nobuffer", "reduce the latency introduced by optional buffering", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_NOBUFFER &#125;, 0, INT_MAX, D, "fflags"&#125;,&#123;"bitexact", "do not write random/volatile data", 0, AV_OPT_TYPE_CONST, &#123; .i64 = AVFMT_FLAG_BITEXACT &#125;, 0, 0, E, "fflags" &#125;,&#123;"shortest", "stop muxing with the shortest stream", 0, AV_OPT_TYPE_CONST, &#123; .i64 = AVFMT_FLAG_SHORTEST &#125;, 0, 0, E, "fflags" &#125;,&#123;"autobsf", "add needed bsfs automatically", 0, AV_OPT_TYPE_CONST, &#123; .i64 = AVFMT_FLAG_AUTO_BSF &#125;, 0, 0, E, "fflags" &#125;,&#123;"seek2any", "allow seeking to non-keyframes on demuxer level when supported", OFFSET(seek2any), AV_OPT_TYPE_BOOL, &#123;.i64 = 0 &#125;, 0, 1, D&#125;,&#123;"analyzeduration", "specify how many microseconds are analyzed to probe the input", OFFSET(max_analyze_duration), AV_OPT_TYPE_INT64, &#123;.i64 = 0 &#125;, 0, INT64_MAX, D&#125;,&#123;"cryptokey", "decryption key", OFFSET(key), AV_OPT_TYPE_BINARY, &#123;.dbl = 0&#125;, 0, 0, D&#125;,&#123;"indexmem", "max memory used for timestamp index (per stream)", OFFSET(max_index_size), AV_OPT_TYPE_INT, &#123;.i64 = 1&lt;&lt;20 &#125;, 0, INT_MAX, D&#125;,&#123;"rtbufsize", "max memory used for buffering real-time frames", OFFSET(max_picture_buffer), AV_OPT_TYPE_INT, &#123;.i64 = 3041280 &#125;, 0, INT_MAX, D&#125;, /* defaults to 1s of 15fps 352x288 YUYV422 video */&#123;"fdebug", "print specific debug info", OFFSET(debug), AV_OPT_TYPE_FLAGS, &#123;.i64 = DEFAULT &#125;, 0, INT_MAX, E|D, "fdebug"&#125;,&#123;"ts", NULL, 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_FDEBUG_TS &#125;, INT_MIN, INT_MAX, E|D, "fdebug"&#125;,&#123;"max_delay", "maximum muxing or demuxing delay in microseconds", OFFSET(max_delay), AV_OPT_TYPE_INT, &#123;.i64 = -1 &#125;, -1, INT_MAX, E|D&#125;,&#123;"start_time_realtime", "wall-clock time when stream begins (PTS==0)", OFFSET(start_time_realtime), AV_OPT_TYPE_INT64, &#123;.i64 = AV_NOPTS_VALUE&#125;, INT64_MIN, INT64_MAX, E&#125;,&#123;"fpsprobesize", "number of frames used to probe fps", OFFSET(fps_probe_size), AV_OPT_TYPE_INT, &#123;.i64 = -1&#125;, -1, INT_MAX-1, D&#125;,&#123;"audio_preload", "microseconds by which audio packets should be interleaved earlier", OFFSET(audio_preload), AV_OPT_TYPE_INT, &#123;.i64 = 0&#125;, 0, INT_MAX-1, E&#125;,&#123;"chunk_duration", "microseconds for each chunk", OFFSET(max_chunk_duration), AV_OPT_TYPE_INT, &#123;.i64 = 0&#125;, 0, INT_MAX-1, E&#125;,&#123;"chunk_size", "size in bytes for each chunk", OFFSET(max_chunk_size), AV_OPT_TYPE_INT, &#123;.i64 = 0&#125;, 0, INT_MAX-1, E&#125;,/* this is a crutch for avconv, since it cannot deal with identically named options in different contexts. * to be removed when avconv is fixed */&#123;"f_err_detect", "set error detection flags (deprecated; use err_detect, save via avconv)", OFFSET(error_recognition), AV_OPT_TYPE_FLAGS, &#123;.i64 = AV_EF_CRCCHECK &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"err_detect", "set error detection flags", OFFSET(error_recognition), AV_OPT_TYPE_FLAGS, &#123;.i64 = AV_EF_CRCCHECK &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"crccheck", "verify embedded CRCs", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_CRCCHECK &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"bitstream", "detect bitstream specification deviations", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_BITSTREAM &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"buffer", "detect improper bitstream length", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_BUFFER &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"explode", "abort decoding on minor error detection", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_EXPLODE &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"ignore_err", "ignore errors", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_IGNORE_ERR &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"careful", "consider things that violate the spec, are fast to check and have not been seen in the wild as errors", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_CAREFUL &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"compliant", "consider all spec non compliancies as errors", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_COMPLIANT | AV_EF_CAREFUL &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"aggressive", "consider things that a sane encoder shouldn't do as an error", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_AGGRESSIVE | AV_EF_COMPLIANT | AV_EF_CAREFUL&#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"use_wallclock_as_timestamps", "use wallclock as timestamps", OFFSET(use_wallclock_as_timestamps), AV_OPT_TYPE_BOOL, &#123;.i64 = 0&#125;, 0, 1, D&#125;,&#123;"skip_initial_bytes", "set number of bytes to skip before reading header and frames", OFFSET(skip_initial_bytes), AV_OPT_TYPE_INT64, &#123;.i64 = 0&#125;, 0, INT64_MAX-1, D&#125;,&#123;"correct_ts_overflow", "correct single timestamp overflows", OFFSET(correct_ts_overflow), AV_OPT_TYPE_BOOL, &#123;.i64 = 1&#125;, 0, 1, D&#125;,&#123;"flush_packets", "enable flushing of the I/O context after each packet", OFFSET(flush_packets), AV_OPT_TYPE_INT, &#123;.i64 = -1&#125;, -1, 1, E&#125;,&#123;"metadata_header_padding", "set number of bytes to be written as padding in a metadata header", OFFSET(metadata_header_padding), AV_OPT_TYPE_INT, &#123;.i64 = -1&#125;, -1, INT_MAX, E&#125;,&#123;"output_ts_offset", "set output timestamp offset", OFFSET(output_ts_offset), AV_OPT_TYPE_DURATION, &#123;.i64 = 0&#125;, -INT64_MAX, INT64_MAX, E&#125;,&#123;"max_interleave_delta", "maximum buffering duration for interleaving", OFFSET(max_interleave_delta), AV_OPT_TYPE_INT64, &#123; .i64 = 10000000 &#125;, 0, INT64_MAX, E &#125;,&#123;"f_strict", "how strictly to follow the standards (deprecated; use strict, save via avconv)", OFFSET(strict_std_compliance), AV_OPT_TYPE_INT, &#123;.i64 = DEFAULT &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"strict", "how strictly to follow the standards", OFFSET(strict_std_compliance), AV_OPT_TYPE_INT, &#123;.i64 = DEFAULT &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"very", "strictly conform to a older more strict version of the spec or reference software", 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_VERY_STRICT &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"strict", "strictly conform to all the things in the spec no matter what the consequences", 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_STRICT &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"normal", NULL, 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_NORMAL &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"unofficial", "allow unofficial extensions", 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_UNOFFICIAL &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"experimental", "allow non-standardized experimental variants", 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_EXPERIMENTAL &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"max_ts_probe", "maximum number of packets to read while waiting for the first timestamp", OFFSET(max_ts_probe), AV_OPT_TYPE_INT, &#123; .i64 = 50 &#125;, 0, INT_MAX, D &#125;,&#123;"avoid_negative_ts", "shift timestamps so they start at 0", OFFSET(avoid_negative_ts), AV_OPT_TYPE_INT, &#123;.i64 = -1&#125;, -1, 2, E, "avoid_negative_ts"&#125;,&#123;"auto", "enabled when required by target format", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_AVOID_NEG_TS_AUTO &#125;, INT_MIN, INT_MAX, E, "avoid_negative_ts"&#125;,&#123;"disabled", "do not change timestamps", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_AVOID_NEG_TS_DISABLED &#125;, INT_MIN, INT_MAX, E, "avoid_negative_ts"&#125;,&#123;"make_non_negative", "shift timestamps so they are non negative", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_AVOID_NEG_TS_MAKE_NON_NEGATIVE &#125;, INT_MIN, INT_MAX, E, "avoid_negative_ts"&#125;,&#123;"make_zero", "shift timestamps so they start at 0", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_AVOID_NEG_TS_MAKE_ZERO &#125;, INT_MIN, INT_MAX, E, "avoid_negative_ts"&#125;,&#123;"dump_separator", "set information dump field separator", OFFSET(dump_separator), AV_OPT_TYPE_STRING, &#123;.str = ", "&#125;, 0, 0, D|E&#125;,&#123;"codec_whitelist", "List of decoders that are allowed to be used", OFFSET(codec_whitelist), AV_OPT_TYPE_STRING, &#123; .str = NULL &#125;, 0, 0, D &#125;,&#123;"format_whitelist", "List of demuxers that are allowed to be used", OFFSET(format_whitelist), AV_OPT_TYPE_STRING, &#123; .str = NULL &#125;, 0, 0, D &#125;,&#123;"protocol_whitelist", "List of protocols that are allowed to be used", OFFSET(protocol_whitelist), AV_OPT_TYPE_STRING, &#123; .str = NULL &#125;, 0, 0, D &#125;,&#123;"protocol_blacklist", "List of protocols that are not allowed to be used", OFFSET(protocol_blacklist), AV_OPT_TYPE_STRING, &#123; .str = NULL &#125;, 0, 0, D &#125;,&#123;"max_streams", "maximum number of streams", OFFSET(max_streams), AV_OPT_TYPE_INT, &#123; .i64 = 1000 &#125;, 0, INT_MAX, D &#125;,&#123;"skip_estimate_duration_from_pts", "skip duration calculation in estimate_timings_from_pts", OFFSET(skip_estimate_duration_from_pts), AV_OPT_TYPE_BOOL, &#123;.i64 = 0&#125;, 0, 1, D&#125;,&#123;"max_probe_packets", "Maximum number of packets to probe a codec", OFFSET(max_probe_packets), AV_OPT_TYPE_INT, &#123; .i64 = 2500 &#125;, 0, INT_MAX, D &#125;,&#123;NULL&#125;,&#125;; 12int avformat_open_input(AVFormatContext **ps, const char *filename, const AVInputFormat *fmt, AVDictionary **options) 以AVFormatContext来分析，av_format_context_class.option 是一个列表，基本对应着 AVFormatContext 内部成员。option 通过offset宏 来找到对应的成员的偏移位置。在avformat_open_input方法中，会遍历外部传入的options参数，对AVFormatContext实例进行属性成员赋值。 类似的这种用法 在ffmpeg中很常见，比如 AVIOContext、URLContext、AVCodecContext … priv_data123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139typedef struct URLContext &#123; const AVClass *av_class; /**&lt; information for av_log(). Set by url_open(). */ const struct URLProtocol *prot; void *priv_data; char *filename; /**&lt; specified URL */ int flags; int max_packet_size; /**&lt; if non zero, the stream is packetized with this max packet size */ int is_streamed; /**&lt; true if streamed (no seek possible), default = false */ int is_connected; AVIOInterruptCB interrupt_callback; int64_t rw_timeout; /**&lt; maximum time to wait for (network) read/write operation completion, in mcs */ const char *protocol_whitelist; const char *protocol_blacklist; int min_packet_size; /**&lt; if non zero, the stream is packetized with this min packet size */&#125; URLContext;const URLProtocol ff_https_protocol = &#123; .name = "https", .url_open2 = http_open, .url_read = http_read, .url_write = http_write, .url_seek = http_seek, .url_close = http_close, .url_get_file_handle = http_get_file_handle, .url_get_short_seek = http_get_short_seek, .url_shutdown = http_shutdown, .priv_data_size = sizeof(HTTPContext), .priv_data_class = &amp;https_context_class, .flags = URL_PROTOCOL_FLAG_NETWORK, .default_whitelist = "http,https,tls,rtp,tcp,udp,crypto,httpproxy"&#125;;#define HTTP_CLASS(flavor) \static const AVClass flavor ## _context_class = &#123; \ .class_name = # flavor, \ .item_name = av_default_item_name, \ .option = options, \ .version = LIBAVUTIL_VERSION_INT, \&#125;#if CONFIG_HTTP_PROTOCOLHTTP_CLASS(http);typedef struct HTTPContext &#123; const AVClass *class; URLContext *hd; unsigned char buffer[BUFFER_SIZE], *buf_ptr, *buf_end; int line_count; int http_code; /* Used if "Transfer-Encoding: chunked" otherwise -1. */ uint64_t chunksize; int chunkend; uint64_t off, end_off, filesize; char *uri; char *location; HTTPAuthState auth_state; HTTPAuthState proxy_auth_state; char *http_proxy; char *headers; char *mime_type; char *http_version; char *user_agent; char *referer; char *content_type; /* Set if the server correctly handles Connection: close and will close * the connection after feeding us the content. */ int willclose; int seekable; /**&lt; Control seekability, 0 = disable, 1 = enable, -1 = probe. */ int chunked_post; /* A flag which indicates if the end of chunked encoding has been sent. */ int end_chunked_post; /* A flag which indicates we have finished to read POST reply. */ int end_header; /* A flag which indicates if we use persistent connections. */ int multiple_requests; uint8_t *post_data; int post_datalen; int is_akamai; int is_mediagateway; char *cookies; ///&lt; holds newline (\n) delimited Set-Cookie header field values (without the "Set-Cookie: " field name) /* A dictionary containing cookies keyed by cookie name */ AVDictionary *cookie_dict; int icy; /* how much data was read since the last ICY metadata packet */ uint64_t icy_data_read; /* after how many bytes of read data a new metadata packet will be found */ uint64_t icy_metaint; char *icy_metadata_headers; char *icy_metadata_packet; AVDictionary *metadata;#if CONFIG_ZLIB int compressed; z_stream inflate_stream; uint8_t *inflate_buffer;#endif /* CONFIG_ZLIB */ AVDictionary *chained_options; /* -1 = try to send if applicable, 0 = always disabled, 1 = always enabled */ int send_expect_100; char *method; int reconnect; int reconnect_at_eof; int reconnect_on_network_error; int reconnect_streamed; int reconnect_delay_max; char *reconnect_on_http_error; int listen; char *resource; int reply_code; int is_multi_client; HandshakeState handshake_step; int is_connected_server; int short_seek_size; int64_t expires; char *new_location; AVDictionary *redirect_cache; uint64_t filesize_from_content_range;&#125; HTTPContext;static int url_alloc_for_protocol(URLContext **puc, const URLProtocol *up, const char *filename, int flags, const AVIOInterruptCB *int_cb) &#123;...if (up-&gt;priv_data_size) &#123; uc-&gt;priv_data = av_mallocz(up-&gt;priv_data_size); if (!uc-&gt;priv_data) &#123; err = AVERROR(ENOMEM); goto fail; &#125; if (up-&gt;priv_data_class) &#123; char *start; *(const AVClass **)uc-&gt;priv_data = up-&gt;priv_data_class; av_opt_set_defaults(uc-&gt;priv_data);...&#125; 以 URLContext 来分析，在 url_alloc_for_protocol 方法中，URLContext.priv_data 赋值的其实就是 对用的 URLProtocol的 priv_data_class实例，以http-protocol来举例，URLContext.priv_data 就是 httpcontext， 而 httpcontext 也符合上面avclass那一套，通过option对 httpcontext实例的成员变量赋值。 这里有个问题？URLContext 持有对应的 prot，为什么还要 弄一个 priv_data呢？ protocol里面有priv_data_class 、priv_data_size，也可以搞一个 priv_data，URLContext用过 prot来访问priv_data就好了。 类似的用法在ffmpeg中 也有很多，比如AVFormatContext，AVFormatContext.priv_data是处理 封装解封装的上下文，mp4格式对应的就是MOVContext. 比如 mp4格式的AVStream,AVStream.priv_data是 MOVStreamContext 123456789101112typedef struct AVFormatContext &#123; ... /** * Format private data. This is an AVOptions-enabled struct * if and only if iformat/oformat.priv_class is not NULL. * * - muxing: set by avformat_write_header() * - demuxing: set by avformat_open_input() */ void *priv_data; ...&#125; internal在 libavformat/internal.h 文件中， FFFormatContext &amp; FFStream 定义如下： 12345678910111213141516171819typedef struct FFFormatContext &#123; /** * The public context. */ AVFormatContext pub; ...&#125;typedef struct FFStream &#123; /** * The public context. */ AVStream pub; ...&#125; AVFormatContext &amp; AVStream 都是作为第一个成员存在于FFFormatContext &amp; FFStream 结构体中。 123456789101112131415161718192021222324252627AVFormatContext *avformat_alloc_context(void)&#123; FFFormatContext *const si = av_mallocz(sizeof(*si)); AVFormatContext *s; if (!si) return NULL; s = &amp;si-&gt;pub; ...&#125;AVStream *avformat_new_stream(AVFormatContext *s, const AVCodec *c)&#123; FFStream *sti; sti = av_mallocz(sizeof(*sti)); AVStream *st; st = &amp;sti-&gt;pub; ...&#125; 初始化逻辑都是通过创建 FFFormatContext &amp; FFStream 实例，然后取成员pub 1234567891011121314static av_always_inline FFFormatContext *ffformatcontext(AVFormatContext *s)&#123; return (FFFormatContext*)s;&#125;static av_always_inline FFStream *ffstream(AVStream *st)&#123; return (FFStream*)st;&#125;static av_always_inline const FFStream *cffstream(const AVStream *st)&#123; return (FFStream*)st;&#125; av_always_inline 是编译器优化，强制内敛 由于 AVFormatContext &amp; AVStream 作为第一个成员，所以二者可以跟FFFormatContext &amp; FFStream 做强制类型转换从定义上看， 好像AVFormatContext &amp; AVStream 是应该作为internal，但是实际用法上，FFFormatContext &amp; FFStream才是作为internal 来辅助 AVFormatContext &amp; AVStream存储相关信息 FFIOContext -&gt; AVIOContext , FFCodec -&gt; AVCodec 也是这种形式 ffmepg中也不是 所有的internal 都是这种用法， AVCodecInternal、AVFilterInternal 就是 常规的 作为 AVCodecContext &amp; AVFilterContext 的 internal 成员存在的 个人目前使用的ffmepg 是 5.x的version，早期4.x的时候，avformat &amp; avstream 也是类似 codec &amp; filter 这样常规的用法]]></content>
      <categories>
        <category>ffmepg</category>
      </categories>
      <tags>
        <tag>ffmepg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ffmepg源码之http学习笔记]]></title>
    <url>%2F2022%2F12%2F22%2Fffmepg%E6%BA%90%E7%A0%81%E4%B9%8Bhttp%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[respect致敬雷神 start从 AVFormatContext （解封装结构体） 入口开始吧 … 123456789101112131415161718192021222324252627282930libavformat/options.cAVFormatContext *avformat_alloc_context(void)&#123; FFFormatContext *const si = av_mallocz(sizeof(*si)); AVFormatContext *s; if (!si) return NULL; s = &amp;si-&gt;pub; s-&gt;av_class = &amp;av_format_context_class; /// io入口 s-&gt;io_open = io_open_default; s-&gt;io_close = ff_format_io_close_default; s-&gt;io_close2= io_close2_default; av_opt_set_defaults(s); si-&gt;pkt = av_packet_alloc(); si-&gt;parse_pkt = av_packet_alloc(); if (!si-&gt;pkt || !si-&gt;parse_pkt) &#123; avformat_free_context(s); return NULL; &#125; si-&gt;shortest_end = AV_NOPTS_VALUE; return s;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150libavformat/demux.cint avformat_open_input(AVFormatContext **ps, const char *filename, const AVInputFormat *fmt, AVDictionary **options)&#123; AVFormatContext *s = *ps; FFFormatContext *si; AVDictionary *tmp = NULL; ID3v2ExtraMeta *id3v2_extra_meta = NULL; int ret = 0; if (!s &amp;&amp; !(s = avformat_alloc_context())) return AVERROR(ENOMEM); si = ffformatcontext(s); if (!s-&gt;av_class) &#123; av_log(NULL, AV_LOG_ERROR, "Input context has not been properly allocated by avformat_alloc_context() and is not NULL either\n"); return AVERROR(EINVAL); &#125; if (fmt) s-&gt;iformat = fmt; if (options) av_dict_copy(&amp;tmp, *options, 0); if (s-&gt;pb) // must be before any goto fail s-&gt;flags |= AVFMT_FLAG_CUSTOM_IO; if ((ret = av_opt_set_dict(s, &amp;tmp)) &lt; 0) goto fail; if (!(s-&gt;url = av_strdup(filename ? filename : ""))) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; /// 初始化输入流的信息。这里会初始化AVInputFormat if ((ret = init_input(s, filename, &amp;tmp)) &lt; 0) goto fail; s-&gt;probe_score = ret; if (!s-&gt;protocol_whitelist &amp;&amp; s-&gt;pb &amp;&amp; s-&gt;pb-&gt;protocol_whitelist) &#123; s-&gt;protocol_whitelist = av_strdup(s-&gt;pb-&gt;protocol_whitelist); if (!s-&gt;protocol_whitelist) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; &#125; if (!s-&gt;protocol_blacklist &amp;&amp; s-&gt;pb &amp;&amp; s-&gt;pb-&gt;protocol_blacklist) &#123; s-&gt;protocol_blacklist = av_strdup(s-&gt;pb-&gt;protocol_blacklist); if (!s-&gt;protocol_blacklist) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; &#125; if (s-&gt;format_whitelist &amp;&amp; av_match_list(s-&gt;iformat-&gt;name, s-&gt;format_whitelist, ',') &lt;= 0) &#123; av_log(s, AV_LOG_ERROR, "Format not on whitelist \'%s\'\n", s-&gt;format_whitelist); ret = AVERROR(EINVAL); goto fail; &#125; avio_skip(s-&gt;pb, s-&gt;skip_initial_bytes); /* Check filename in case an image number is expected. */ if (s-&gt;iformat-&gt;flags &amp; AVFMT_NEEDNUMBER) &#123; if (!av_filename_number_test(filename)) &#123; ret = AVERROR(EINVAL); goto fail; &#125; &#125; s-&gt;duration = s-&gt;start_time = AV_NOPTS_VALUE; /* Allocate private data. */ if (s-&gt;iformat-&gt;priv_data_size &gt; 0) &#123; if (!(s-&gt;priv_data = av_mallocz(s-&gt;iformat-&gt;priv_data_size))) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; if (s-&gt;iformat-&gt;priv_class) &#123; *(const AVClass **) s-&gt;priv_data = s-&gt;iformat-&gt;priv_class; av_opt_set_defaults(s-&gt;priv_data); if ((ret = av_opt_set_dict(s-&gt;priv_data, &amp;tmp)) &lt; 0) goto fail; &#125; &#125; /* e.g. AVFMT_NOFILE formats will not have an AVIOContext */ if (s-&gt;pb) ff_id3v2_read_dict(s-&gt;pb, &amp;si-&gt;id3v2_meta, ID3v2_DEFAULT_MAGIC, &amp;id3v2_extra_meta); if (s-&gt;iformat-&gt;read_header) if ((ret = s-&gt;iformat-&gt;read_header(s)) &lt; 0) &#123; if (s-&gt;iformat-&gt;flags_internal &amp; FF_FMT_INIT_CLEANUP) goto close; goto fail; &#125; if (!s-&gt;metadata) &#123; s-&gt;metadata = si-&gt;id3v2_meta; si-&gt;id3v2_meta = NULL; &#125; else if (si-&gt;id3v2_meta) &#123; av_log(s, AV_LOG_WARNING, "Discarding ID3 tags because more suitable tags were found.\n"); av_dict_free(&amp;si-&gt;id3v2_meta); &#125; if (id3v2_extra_meta) &#123; if (!strcmp(s-&gt;iformat-&gt;name, "mp3") || !strcmp(s-&gt;iformat-&gt;name, "aac") || !strcmp(s-&gt;iformat-&gt;name, "tta") || !strcmp(s-&gt;iformat-&gt;name, "wav")) &#123; if ((ret = ff_id3v2_parse_apic(s, id3v2_extra_meta)) &lt; 0) goto close; if ((ret = ff_id3v2_parse_chapters(s, id3v2_extra_meta)) &lt; 0) goto close; if ((ret = ff_id3v2_parse_priv(s, id3v2_extra_meta)) &lt; 0) goto close; &#125; else av_log(s, AV_LOG_DEBUG, "demuxer does not support additional id3 data, skipping\n"); ff_id3v2_free_extra_meta(&amp;id3v2_extra_meta); &#125; if ((ret = avformat_queue_attached_pictures(s)) &lt; 0) goto close; if (s-&gt;pb &amp;&amp; !si-&gt;data_offset) si-&gt;data_offset = avio_tell(s-&gt;pb); si-&gt;raw_packet_buffer_size = 0; update_stream_avctx(s); if (options) &#123; av_dict_free(options); *options = tmp; &#125; *ps = s; return 0;close: if (s-&gt;iformat-&gt;read_close) s-&gt;iformat-&gt;read_close(s);fail: ff_id3v2_free_extra_meta(&amp;id3v2_extra_meta); av_dict_free(&amp;tmp); if (s-&gt;pb &amp;&amp; !(s-&gt;flags &amp; AVFMT_FLAG_CUSTOM_IO)) avio_closep(&amp;s-&gt;pb); avformat_free_context(s); *ps = NULL; return ret;&#125; 1234567891011121314151617181920212223242526272829303132333435363738libavformat/demux.cstatic int init_input(AVFormatContext *s, const char *filename, AVDictionary **options)&#123; int ret; AVProbeData pd = &#123; filename, NULL, 0 &#125;; int score = AVPROBE_SCORE_RETRY; ///初始化的过程 s-&gt;pb 还没创建好 if (s-&gt;pb) &#123; s-&gt;flags |= AVFMT_FLAG_CUSTOM_IO; if (!s-&gt;iformat) return av_probe_input_buffer2(s-&gt;pb, &amp;s-&gt;iformat, filename, s, 0, s-&gt;format_probesize); else if (s-&gt;iformat-&gt;flags &amp; AVFMT_NOFILE) av_log(s, AV_LOG_WARNING, "Custom AVIOContext makes no sense and " "will be ignored with AVFMT_NOFILE format.\n"); return 0; &#125; /* Guess file format. */ // av_probe_input_format2 首次判断inputformat格式 iformat，这一步通常判断不出来 if ((s-&gt;iformat &amp;&amp; s-&gt;iformat-&gt;flags &amp; AVFMT_NOFILE) || (!s-&gt;iformat &amp;&amp; (s-&gt;iformat = av_probe_input_format2(&amp;pd, 0, &amp;score)))) return score; /// io_open 就是前面赋值的入口函数 io_open_default if ((ret = s-&gt;io_open(s, &amp;s-&gt;pb, filename, AVIO_FLAG_READ | s-&gt;avio_flags, options)) &lt; 0) return ret; if (s-&gt;iformat) return 0; // av_probe_input_buffer2再次判断inputformat格式 iformat，通过buf媒体流数据判断。 return av_probe_input_buffer2(s-&gt;pb, &amp;s-&gt;iformat, filename, s, 0, s-&gt;format_probesize);&#125; 1234567891011121314151617181920libavformat/options.cstatic int io_open_default(AVFormatContext *s, AVIOContext **pb, const char *url, int flags, AVDictionary **options)&#123; int loglevel; if (!strcmp(url, s-&gt;url) || s-&gt;iformat &amp;&amp; !strcmp(s-&gt;iformat-&gt;name, "image2") || s-&gt;oformat &amp;&amp; !strcmp(s-&gt;oformat-&gt;name, "image2") ) &#123; loglevel = AV_LOG_DEBUG; &#125; else loglevel = AV_LOG_INFO; av_log(s, loglevel, "Opening \'%s\' for %s\n", url, flags &amp; AVIO_FLAG_WRITE ? "writing" : "reading"); /// io-open return ffio_open_whitelist(pb, url, flags, &amp;s-&gt;interrupt_callback, options, s-&gt;protocol_whitelist, s-&gt;protocol_blacklist);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188libavformat/aviobuf.cint ffio_open_whitelist(AVIOContext **s, const char *filename, int flags, const AVIOInterruptCB *int_cb, AVDictionary **options, const char *whitelist, const char *blacklist )&#123; URLContext *h; int err; *s = NULL; /// 创建 URLContext err = ffurl_open_whitelist(&amp;h, filename, flags, int_cb, options, whitelist, blacklist, NULL); if (err &lt; 0) return err; /// 创建AVIOContext 赋值 urlcontext err = ffio_fdopen(s, h); if (err &lt; 0) &#123; ffurl_close(h); return err; &#125; return 0;&#125;```clibavformat/avio.cint ffurl_alloc(URLContext **puc, const char *filename, int flags, const AVIOInterruptCB *int_cb)&#123; const URLProtocol *p = NULL; /// 根据filename 找到 http p = url_find_protocol(filename); if (p) /// 创建URLContext 并赋值 URLProtocol return url_alloc_for_protocol(puc, p, filename, flags, int_cb); *puc = NULL; return AVERROR_PROTOCOL_NOT_FOUND;&#125;static int url_alloc_for_protocol(URLContext **puc, const URLProtocol *up, const char *filename, int flags, const AVIOInterruptCB *int_cb)&#123; URLContext *uc; int err;#if CONFIG_NETWORK if (up-&gt;flags &amp; URL_PROTOCOL_FLAG_NETWORK &amp;&amp; !ff_network_init()) return AVERROR(EIO);#endif if ((flags &amp; AVIO_FLAG_READ) &amp;&amp; !up-&gt;url_read) &#123; av_log(NULL, AV_LOG_ERROR, "Impossible to open the '%s' protocol for reading\n", up-&gt;name); return AVERROR(EIO); &#125; if ((flags &amp; AVIO_FLAG_WRITE) &amp;&amp; !up-&gt;url_write) &#123; av_log(NULL, AV_LOG_ERROR, "Impossible to open the '%s' protocol for writing\n", up-&gt;name); return AVERROR(EIO); &#125; /// 创建 URLContext uc = av_mallocz(sizeof(URLContext) + strlen(filename) + 1); if (!uc) &#123; err = AVERROR(ENOMEM); goto fail; &#125; uc-&gt;av_class = &amp;ffurl_context_class; uc-&gt;filename = (char *)&amp;uc[1]; strcpy(uc-&gt;filename, filename); /// 赋值 urlprotocal uc-&gt;prot = up; uc-&gt;flags = flags; uc-&gt;is_streamed = 0; /* default = not streamed */ uc-&gt;max_packet_size = 0; /* default: stream file */ /// httpprotocal 设置过了 priv_data 为 HTTPContext if (up-&gt;priv_data_size) &#123; uc-&gt;priv_data = av_mallocz(up-&gt;priv_data_size); if (!uc-&gt;priv_data) &#123; err = AVERROR(ENOMEM); goto fail; &#125; if (up-&gt;priv_data_class) &#123; char *start; *(const AVClass **)uc-&gt;priv_data = up-&gt;priv_data_class; av_opt_set_defaults(uc-&gt;priv_data); if (av_strstart(uc-&gt;filename, up-&gt;name, (const char**)&amp;start) &amp;&amp; *start == ',') &#123; int ret= 0; char *p= start; char sep= *++p; char *key, *val; p++; if (strcmp(up-&gt;name, "subfile")) ret = AVERROR(EINVAL); while(ret &gt;= 0 &amp;&amp; (key= strchr(p, sep)) &amp;&amp; p&lt;key &amp;&amp; (val = strchr(key+1, sep)))&#123; *val= *key= 0; if (strcmp(p, "start") &amp;&amp; strcmp(p, "end")) &#123; ret = AVERROR_OPTION_NOT_FOUND; &#125; else ret= av_opt_set(uc-&gt;priv_data, p, key+1, 0); if (ret == AVERROR_OPTION_NOT_FOUND) av_log(uc, AV_LOG_ERROR, "Key '%s' not found.\n", p); *val= *key= sep; p= val+1; &#125; if(ret&lt;0 || p!=key)&#123; av_log(uc, AV_LOG_ERROR, "Error parsing options string %s\n", start); av_freep(&amp;uc-&gt;priv_data); av_freep(&amp;uc); err = AVERROR(EINVAL); goto fail; &#125; memmove(start, key+1, strlen(key)); &#125; &#125; &#125; if (int_cb) uc-&gt;interrupt_callback = *int_cb; *puc = uc; return 0;fail: *puc = NULL; if (uc) av_freep(&amp;uc-&gt;priv_data); av_freep(&amp;uc);#if CONFIG_NETWORK if (up-&gt;flags &amp; URL_PROTOCOL_FLAG_NETWORK) ff_network_close();#endif return err;&#125;int ffurl_open_whitelist(URLContext **puc, const char *filename, int flags, const AVIOInterruptCB *int_cb, AVDictionary **options, const char *whitelist, const char* blacklist, URLContext *parent)&#123; AVDictionary *tmp_opts = NULL; AVDictionaryEntry *e; /// 创建 URLContext int ret = ffurl_alloc(puc, filename, flags, int_cb); if (ret &lt; 0) return ret; if (parent) &#123; ret = av_opt_copy(*puc, parent); if (ret &lt; 0) goto fail; &#125; if (options &amp;&amp; (ret = av_opt_set_dict(*puc, options)) &lt; 0) goto fail; if (options &amp;&amp; (*puc)-&gt;prot-&gt;priv_data_class &amp;&amp; (ret = av_opt_set_dict((*puc)-&gt;priv_data, options)) &lt; 0) goto fail; if (!options) options = &amp;tmp_opts; av_assert0(!whitelist || !(e=av_dict_get(*options, "protocol_whitelist", NULL, 0)) || !strcmp(whitelist, e-&gt;value)); av_assert0(!blacklist || !(e=av_dict_get(*options, "protocol_blacklist", NULL, 0)) || !strcmp(blacklist, e-&gt;value)); if ((ret = av_dict_set(options, "protocol_whitelist", whitelist, 0)) &lt; 0) goto fail; if ((ret = av_dict_set(options, "protocol_blacklist", blacklist, 0)) &lt; 0) goto fail; if ((ret = av_opt_set_dict(*puc, options)) &lt; 0) goto fail; /// 发起链接 ret = ffurl_connect(*puc, options); if (!ret) return 0;fail: ffurl_closep(puc); return ret;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162libavformat/avio.cint ffurl_connect(URLContext *uc, AVDictionary **options)&#123; int err; AVDictionary *tmp_opts = NULL; AVDictionaryEntry *e; if (!options) options = &amp;tmp_opts; // Check that URLContext was initialized correctly and lists are matching if set av_assert0(!(e=av_dict_get(*options, "protocol_whitelist", NULL, 0)) || (uc-&gt;protocol_whitelist &amp;&amp; !strcmp(uc-&gt;protocol_whitelist, e-&gt;value))); av_assert0(!(e=av_dict_get(*options, "protocol_blacklist", NULL, 0)) || (uc-&gt;protocol_blacklist &amp;&amp; !strcmp(uc-&gt;protocol_blacklist, e-&gt;value))); if (uc-&gt;protocol_whitelist &amp;&amp; av_match_list(uc-&gt;prot-&gt;name, uc-&gt;protocol_whitelist, ',') &lt;= 0) &#123; av_log(uc, AV_LOG_ERROR, "Protocol '%s' not on whitelist '%s'!\n", uc-&gt;prot-&gt;name, uc-&gt;protocol_whitelist); return AVERROR(EINVAL); &#125; if (uc-&gt;protocol_blacklist &amp;&amp; av_match_list(uc-&gt;prot-&gt;name, uc-&gt;protocol_blacklist, ',') &gt; 0) &#123; av_log(uc, AV_LOG_ERROR, "Protocol '%s' on blacklist '%s'!\n", uc-&gt;prot-&gt;name, uc-&gt;protocol_blacklist); return AVERROR(EINVAL); &#125; if (!uc-&gt;protocol_whitelist &amp;&amp; uc-&gt;prot-&gt;default_whitelist) &#123; av_log(uc, AV_LOG_DEBUG, "Setting default whitelist '%s'\n", uc-&gt;prot-&gt;default_whitelist); uc-&gt;protocol_whitelist = av_strdup(uc-&gt;prot-&gt;default_whitelist); if (!uc-&gt;protocol_whitelist) &#123; return AVERROR(ENOMEM); &#125; &#125; else if (!uc-&gt;protocol_whitelist) av_log(uc, AV_LOG_DEBUG, "No default whitelist set\n"); // This should be an error once all declare a default whitelist if ((err = av_dict_set(options, "protocol_whitelist", uc-&gt;protocol_whitelist, 0)) &lt; 0) return err; if ((err = av_dict_set(options, "protocol_blacklist", uc-&gt;protocol_blacklist, 0)) &lt; 0) return err; err = /// uc.prot 是 http url_open2 对应的是 http_open uc-&gt;prot-&gt;url_open2 ? uc-&gt;prot-&gt;url_open2(uc, uc-&gt;filename, uc-&gt;flags, options) : uc-&gt;prot-&gt;url_open(uc, uc-&gt;filename, uc-&gt;flags); av_dict_set(options, "protocol_whitelist", NULL, 0); av_dict_set(options, "protocol_blacklist", NULL, 0); if (err) return err; uc-&gt;is_connected = 1; /* We must be careful here as ffurl_seek() could be slow, * for example for http */ if ((uc-&gt;flags &amp; AVIO_FLAG_WRITE) || !strcmp(uc-&gt;prot-&gt;name, "file")) if (!uc-&gt;is_streamed &amp;&amp; ffurl_seek(uc, 0, SEEK_SET) &lt; 0) uc-&gt;is_streamed = 1; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859libavformat/aviobuf.cint ffio_fdopen(AVIOContext **s, URLContext *h)&#123; uint8_t *buffer = NULL; int buffer_size, max_packet_size; max_packet_size = h-&gt;max_packet_size; if (max_packet_size) &#123; buffer_size = max_packet_size; /* no need to bufferize more than one packet */ &#125; else &#123; buffer_size = IO_BUFFER_SIZE; &#125; if (!(h-&gt;flags &amp; AVIO_FLAG_WRITE) &amp;&amp; h-&gt;is_streamed) &#123; if (buffer_size &gt; INT_MAX/2) return AVERROR(EINVAL); buffer_size *= 2; &#125; buffer = av_malloc(buffer_size); if (!buffer) return AVERROR(ENOMEM); /// 创建 AVIOContext /// s-&gt;opaque = h; *s = avio_alloc_context(buffer, buffer_size, h-&gt;flags &amp; AVIO_FLAG_WRITE, h, (int (*)(void *, uint8_t *, int)) ffurl_read, (int (*)(void *, uint8_t *, int)) ffurl_write, (int64_t (*)(void *, int64_t, int))ffurl_seek); if (!*s) &#123; av_freep(&amp;buffer); return AVERROR(ENOMEM); &#125; (*s)-&gt;protocol_whitelist = av_strdup(h-&gt;protocol_whitelist); if (!(*s)-&gt;protocol_whitelist &amp;&amp; h-&gt;protocol_whitelist) &#123; avio_closep(s); return AVERROR(ENOMEM); &#125; (*s)-&gt;protocol_blacklist = av_strdup(h-&gt;protocol_blacklist); if (!(*s)-&gt;protocol_blacklist &amp;&amp; h-&gt;protocol_blacklist) &#123; avio_closep(s); return AVERROR(ENOMEM); &#125; (*s)-&gt;direct = h-&gt;flags &amp; AVIO_FLAG_DIRECT; (*s)-&gt;seekable = h-&gt;is_streamed ? 0 : AVIO_SEEKABLE_NORMAL; (*s)-&gt;max_packet_size = max_packet_size; (*s)-&gt;min_packet_size = h-&gt;min_packet_size; if(h-&gt;prot) &#123; (*s)-&gt;read_pause = (int (*)(void *, int))h-&gt;prot-&gt;url_read_pause; (*s)-&gt;read_seek = (int64_t (*)(void *, int, int64_t, int))h-&gt;prot-&gt;url_read_seek; if (h-&gt;prot-&gt;url_read_seek) (*s)-&gt;seekable |= AVIO_SEEKABLE_TIME; &#125; ((FFIOContext*)(*s))-&gt;short_seek_get = (int (*)(void *))ffurl_get_short_seek; (*s)-&gt;av_class = &amp;ff_avio_class; return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427libavformat/http.c/// 这个就是对应前面的 protocolconst URLProtocol ff_http_protocol = &#123; .name = "http", /// * .url_open2 = http_open, .url_accept = http_accept, .url_handshake = http_handshake, .url_read = http_read, .url_write = http_write, .url_seek = http_seek, .url_close = http_close, .url_get_file_handle = http_get_file_handle, .url_get_short_seek = http_get_short_seek, .url_shutdown = http_shutdown, /// * .priv_data_size = sizeof(HTTPContext), .priv_data_class = &amp;http_context_class, .flags = URL_PROTOCOL_FLAG_NETWORK, .default_whitelist = "http,https,tls,rtp,tcp,udp,crypto,httpproxy,data"&#125;;static int http_open(URLContext *h, const char *uri, int flags, AVDictionary **options)&#123; HTTPContext *s = h-&gt;priv_data; int ret; if( s-&gt;seekable == 1 ) h-&gt;is_streamed = 0; else h-&gt;is_streamed = 1; s-&gt;filesize = UINT64_MAX; s-&gt;location = av_strdup(uri); if (!s-&gt;location) return AVERROR(ENOMEM); s-&gt;uri = av_strdup(uri); if (!s-&gt;uri) return AVERROR(ENOMEM); if (options) av_dict_copy(&amp;s-&gt;chained_options, *options, 0); if (s-&gt;headers) &#123; int len = strlen(s-&gt;headers); if (len &lt; 2 || strcmp("\r\n", s-&gt;headers + len - 2)) &#123; av_log(h, AV_LOG_WARNING, "No trailing CRLF found in HTTP header. Adding it.\n"); ret = av_reallocp(&amp;s-&gt;headers, len + 3); if (ret &lt; 0) goto bail_out; s-&gt;headers[len] = '\r'; s-&gt;headers[len + 1] = '\n'; s-&gt;headers[len + 2] = '\0'; &#125; &#125; if (s-&gt;listen) &#123; return http_listen(h, uri, flags, options); &#125; /// 进这里 ret = http_open_cnx(h, options);bail_out: if (ret &lt; 0) &#123; av_dict_free(&amp;s-&gt;chained_options); av_dict_free(&amp;s-&gt;cookie_dict); av_dict_free(&amp;s-&gt;redirect_cache); av_freep(&amp;s-&gt;new_location); av_freep(&amp;s-&gt;uri); &#125; return ret;&#125;static int http_open_cnx(URLContext *h, AVDictionary **options)&#123; HTTPAuthType cur_auth_type, cur_proxy_auth_type; HTTPContext *s = h-&gt;priv_data; int ret, attempts = 0, redirects = 0; int reconnect_delay = 0; uint64_t off; char *cached;redo: cached = redirect_cache_get(s); if (cached) &#123; av_free(s-&gt;location); s-&gt;location = av_strdup(cached); if (!s-&gt;location) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; goto redo; &#125; av_dict_copy(options, s-&gt;chained_options, 0); cur_auth_type = s-&gt;auth_state.auth_type; cur_proxy_auth_type = s-&gt;auth_state.auth_type; off = s-&gt;off; /// 进这里 关键方法 ret = http_open_cnx_internal(h, options); if (ret &lt; 0) &#123; if (!http_should_reconnect(s, ret) || reconnect_delay &gt; s-&gt;reconnect_delay_max) goto fail; av_log(h, AV_LOG_WARNING, "Will reconnect at %"PRIu64" in %d second(s).\n", off, reconnect_delay); ret = ff_network_sleep_interruptible(1000U * 1000 * reconnect_delay, &amp;h-&gt;interrupt_callback); if (ret != AVERROR(ETIMEDOUT)) goto fail; reconnect_delay = 1 + 2 * reconnect_delay; /* restore the offset (http_connect resets it) */ s-&gt;off = off; ffurl_closep(&amp;s-&gt;hd); goto redo; &#125; attempts++; if (s-&gt;http_code == 401) &#123; if ((cur_auth_type == HTTP_AUTH_NONE || s-&gt;auth_state.stale) &amp;&amp; s-&gt;auth_state.auth_type != HTTP_AUTH_NONE &amp;&amp; attempts &lt; 4) &#123; ffurl_closep(&amp;s-&gt;hd); goto redo; &#125; else goto fail; &#125; if (s-&gt;http_code == 407) &#123; if ((cur_proxy_auth_type == HTTP_AUTH_NONE || s-&gt;proxy_auth_state.stale) &amp;&amp; s-&gt;proxy_auth_state.auth_type != HTTP_AUTH_NONE &amp;&amp; attempts &lt; 4) &#123; ffurl_closep(&amp;s-&gt;hd); goto redo; &#125; else goto fail; &#125; if ((s-&gt;http_code == 301 || s-&gt;http_code == 302 || s-&gt;http_code == 303 || s-&gt;http_code == 307 || s-&gt;http_code == 308) &amp;&amp; s-&gt;new_location) &#123; /* url moved, get next */ ffurl_closep(&amp;s-&gt;hd); if (redirects++ &gt;= MAX_REDIRECTS) return AVERROR(EIO); if (!s-&gt;expires) &#123; s-&gt;expires = (s-&gt;http_code == 301 || s-&gt;http_code == 308) ? INT64_MAX : -1; &#125; if (s-&gt;expires &gt; time(NULL) &amp;&amp; av_dict_count(s-&gt;redirect_cache) &lt; MAX_CACHED_REDIRECTS) &#123; redirect_cache_set(s, s-&gt;location, s-&gt;new_location, s-&gt;expires); &#125; av_free(s-&gt;location); s-&gt;location = s-&gt;new_location; s-&gt;new_location = NULL; /* Restart the authentication process with the new target, which * might use a different auth mechanism. */ memset(&amp;s-&gt;auth_state, 0, sizeof(s-&gt;auth_state)); attempts = 0; goto redo; &#125; return 0;fail: if (s-&gt;hd) ffurl_closep(&amp;s-&gt;hd); if (ret &lt; 0) return ret; return ff_http_averror(s-&gt;http_code, AVERROR(EIO));&#125;static int http_open_cnx_internal(URLContext *h, AVDictionary **options)&#123; /// lower_proto 关键 const char *path, *proxy_path, *lower_proto = "tcp", *local_path; char *env_http_proxy, *env_no_proxy; char *hashmark; char hostname[1024], hoststr[1024], proto[10]; char auth[1024], proxyauth[1024] = ""; char path1[MAX_URL_SIZE], sanitized_path[MAX_URL_SIZE + 1]; char buf[1024], urlbuf[MAX_URL_SIZE]; int port, use_proxy, err = 0; /// 前面定义httpprotocal的时候设置过了 HTTPContext *s = h-&gt;priv_data; av_url_split(proto, sizeof(proto), auth, sizeof(auth), hostname, sizeof(hostname), &amp;port, path1, sizeof(path1), s-&gt;location); ff_url_join(hoststr, sizeof(hoststr), NULL, NULL, hostname, port, NULL); env_http_proxy = getenv_utf8("http_proxy"); proxy_path = s-&gt;http_proxy ? s-&gt;http_proxy : env_http_proxy; env_no_proxy = getenv_utf8("no_proxy"); use_proxy = !ff_http_match_no_proxy(env_no_proxy, hostname) &amp;&amp; proxy_path &amp;&amp; av_strstart(proxy_path, "http://", NULL); freeenv_utf8(env_no_proxy); /// 如果是https 会走到 tls (tls_securetransport.c &amp; tls.c) /// tls 内部 最后也会走到tcp的 先略过 先直接看tcp if (!strcmp(proto, "https")) &#123; lower_proto = "tls"; use_proxy = 0; if (port &lt; 0) port = 443; /* pass http_proxy to underlying protocol */ if (s-&gt;http_proxy) &#123; err = av_dict_set(options, "http_proxy", s-&gt;http_proxy, 0); if (err &lt; 0) goto end; &#125; &#125; if (port &lt; 0) port = 80; hashmark = strchr(path1, '#'); if (hashmark) *hashmark = '\0'; if (path1[0] == '\0') &#123; path = "/"; &#125; else if (path1[0] == '?') &#123; snprintf(sanitized_path, sizeof(sanitized_path), "/%s", path1); path = sanitized_path; &#125; else &#123; path = path1; &#125; local_path = path; if (use_proxy) &#123; /* Reassemble the request URL without auth string - we don't * want to leak the auth to the proxy. */ ff_url_join(urlbuf, sizeof(urlbuf), proto, NULL, hostname, port, "%s", path1); path = urlbuf; av_url_split(NULL, 0, proxyauth, sizeof(proxyauth), hostname, sizeof(hostname), &amp;port, NULL, 0, proxy_path); &#125; /// 这里先以tcp来分析 ff_url_join(buf, sizeof(buf), lower_proto, NULL, hostname, port, NULL); if (!s-&gt;hd) &#123; /// s-&gt;hd 也是 URLContext，发现这里又进入了ffurl_open_whitelist 循环了 /// buf 是 tcp了 不是http 了， 否则就跳不出来了 /// 所以 s-&gt;hd 的 protocal 是 TCPProtocol err = ffurl_open_whitelist(&amp;s-&gt;hd, buf, AVIO_FLAG_READ_WRITE, &amp;h-&gt;interrupt_callback, options, h-&gt;protocol_whitelist, h-&gt;protocol_blacklist, h); &#125;end: freeenv_utf8(env_http_proxy); return err &lt; 0 ? err : http_connect( h, path, local_path, hoststr, auth, proxyauth);&#125;static int http_connect(URLContext *h, const char *path, const char *local_path, const char *hoststr, const char *auth, const char *proxyauth)&#123; HTTPContext *s = h-&gt;priv_data; int post, err; AVBPrint request; char *authstr = NULL, *proxyauthstr = NULL; uint64_t off = s-&gt;off; const char *method; int send_expect_100 = 0; av_bprint_init_for_buffer(&amp;request, s-&gt;buffer, sizeof(s-&gt;buffer)); /* send http header */ post = h-&gt;flags &amp; AVIO_FLAG_WRITE; if (s-&gt;post_data) &#123; /* force POST method and disable chunked encoding when * custom HTTP post data is set */ post = 1; s-&gt;chunked_post = 0; &#125; if (s-&gt;method) method = s-&gt;method; else method = post ? "POST" : "GET"; authstr = ff_http_auth_create_response(&amp;s-&gt;auth_state, auth, local_path, method); proxyauthstr = ff_http_auth_create_response(&amp;s-&gt;proxy_auth_state, proxyauth, local_path, method); if (post &amp;&amp; !s-&gt;post_data) &#123; if (s-&gt;send_expect_100 != -1) &#123; send_expect_100 = s-&gt;send_expect_100; &#125; else &#123; send_expect_100 = 0; /* The user has supplied authentication but we don't know the auth type, * send Expect: 100-continue to get the 401 response including the * WWW-Authenticate header, or an 100 continue if no auth actually * is needed. */ if (auth &amp;&amp; *auth &amp;&amp; s-&gt;auth_state.auth_type == HTTP_AUTH_NONE &amp;&amp; s-&gt;http_code != 401) send_expect_100 = 1; &#125; &#125; av_bprintf(&amp;request, "%s ", method); bprint_escaped_path(&amp;request, path); av_bprintf(&amp;request, " HTTP/1.1\r\n"); if (post &amp;&amp; s-&gt;chunked_post) av_bprintf(&amp;request, "Transfer-Encoding: chunked\r\n"); /* set default headers if needed */ if (!has_header(s-&gt;headers, "\r\nUser-Agent: ")) av_bprintf(&amp;request, "User-Agent: %s\r\n", s-&gt;user_agent); if (s-&gt;referer) &#123; /* set default headers if needed */ if (!has_header(s-&gt;headers, "\r\nReferer: ")) av_bprintf(&amp;request, "Referer: %s\r\n", s-&gt;referer); &#125; if (!has_header(s-&gt;headers, "\r\nAccept: ")) av_bprintf(&amp;request, "Accept: */*\r\n"); // Note: we send the Range header on purpose, even when we're probing, // since it allows us to detect more reliably if a (non-conforming) // server supports seeking by analysing the reply headers. if (!has_header(s-&gt;headers, "\r\nRange: ") &amp;&amp; !post &amp;&amp; (s-&gt;off &gt; 0 || s-&gt;end_off || s-&gt;seekable != 0)) &#123; av_bprintf(&amp;request, "Range: bytes=%"PRIu64"-", s-&gt;off); if (s-&gt;end_off) av_bprintf(&amp;request, "%"PRId64, s-&gt;end_off - 1); av_bprintf(&amp;request, "\r\n"); &#125; if (send_expect_100 &amp;&amp; !has_header(s-&gt;headers, "\r\nExpect: ")) av_bprintf(&amp;request, "Expect: 100-continue\r\n"); if (!has_header(s-&gt;headers, "\r\nConnection: ")) av_bprintf(&amp;request, "Connection: %s\r\n", s-&gt;multiple_requests ? "keep-alive" : "close"); if (!has_header(s-&gt;headers, "\r\nHost: ")) av_bprintf(&amp;request, "Host: %s\r\n", hoststr); if (!has_header(s-&gt;headers, "\r\nContent-Length: ") &amp;&amp; s-&gt;post_data) av_bprintf(&amp;request, "Content-Length: %d\r\n", s-&gt;post_datalen); if (!has_header(s-&gt;headers, "\r\nContent-Type: ") &amp;&amp; s-&gt;content_type) av_bprintf(&amp;request, "Content-Type: %s\r\n", s-&gt;content_type); if (!has_header(s-&gt;headers, "\r\nCookie: ") &amp;&amp; s-&gt;cookies) &#123; char *cookies = NULL; if (!get_cookies(s, &amp;cookies, path, hoststr) &amp;&amp; cookies) &#123; av_bprintf(&amp;request, "Cookie: %s\r\n", cookies); av_free(cookies); &#125; &#125; if (!has_header(s-&gt;headers, "\r\nIcy-MetaData: ") &amp;&amp; s-&gt;icy) av_bprintf(&amp;request, "Icy-MetaData: 1\r\n"); /* now add in custom headers */ if (s-&gt;headers) av_bprintf(&amp;request, "%s", s-&gt;headers); if (authstr) av_bprintf(&amp;request, "%s", authstr); if (proxyauthstr) av_bprintf(&amp;request, "Proxy-%s", proxyauthstr); av_bprintf(&amp;request, "\r\n"); av_log(h, AV_LOG_DEBUG, "request: %s\n", request.str); if (!av_bprint_is_complete(&amp;request)) &#123; av_log(h, AV_LOG_ERROR, "overlong headers\n"); err = AVERROR(EINVAL); goto done; &#125; /// 写入数据 发起请求 if ((err = ffurl_write(s-&gt;hd, request.str, request.len)) &lt; 0) goto done; if (s-&gt;post_data) if ((err = ffurl_write(s-&gt;hd, s-&gt;post_data, s-&gt;post_datalen)) &lt; 0) goto done; /* init input buffer */ s-&gt;buf_ptr = s-&gt;buffer; s-&gt;buf_end = s-&gt;buffer; s-&gt;line_count = 0; s-&gt;off = 0; s-&gt;icy_data_read = 0; s-&gt;filesize = UINT64_MAX; s-&gt;willclose = 0; s-&gt;end_chunked_post = 0; s-&gt;end_header = 0;#if CONFIG_ZLIB s-&gt;compressed = 0;#endif if (post &amp;&amp; !s-&gt;post_data &amp;&amp; !send_expect_100) &#123; /* Pretend that it did work. We didn't read any header yet, since * we've still to send the POST data, but the code calling this * function will check http_code after we return. */ s-&gt;http_code = 200; err = 0; goto done; &#125; /* wait for header */ err = http_read_header(h); if (err &lt; 0) goto done; if (s-&gt;new_location) s-&gt;off = off; err = (off == s-&gt;off) ? 0 : -1;done: av_freep(&amp;authstr); av_freep(&amp;proxyauthstr); return err;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162libavformat/avio.cint ffurl_write(URLContext *h, const unsigned char *buf, int size)&#123; if (!(h-&gt;flags &amp; AVIO_FLAG_WRITE)) return AVERROR(EIO); /* avoid sending too big packets */ if (h-&gt;max_packet_size &amp;&amp; size &gt; h-&gt;max_packet_size) return AVERROR(EIO); /// h-&gt;prot 通过前面的分析 就是TCPProtocol return retry_transfer_wrapper(h, (unsigned char *)buf, size, size, (int (*)(struct URLContext *, uint8_t *, int)) h-&gt;prot-&gt;url_write);&#125;static inline int retry_transfer_wrapper(URLContext *h, uint8_t *buf, int size, int size_min, int (*transfer_func)(URLContext *h, uint8_t *buf, int size))&#123; int ret, len; int fast_retries = 5; int64_t wait_since = 0; len = 0; while (len &lt; size_min) &#123; if (ff_check_interrupt(&amp;h-&gt;interrupt_callback)) return AVERROR_EXIT; /// 回调 ret = transfer_func(h, buf + len, size - len); if (ret == AVERROR(EINTR)) continue; if (h-&gt;flags &amp; AVIO_FLAG_NONBLOCK) return ret; if (ret == AVERROR(EAGAIN)) &#123; ret = 0; if (fast_retries) &#123; fast_retries--; &#125; else &#123; if (h-&gt;rw_timeout) &#123; if (!wait_since) wait_since = av_gettime_relative(); else if (av_gettime_relative() &gt; wait_since + h-&gt;rw_timeout) return AVERROR(EIO); &#125; av_usleep(1000); &#125; &#125; else if (ret == AVERROR_EOF) return (len &gt; 0) ? len : AVERROR_EOF; else if (ret &lt; 0) return ret; if (ret) &#123; fast_retries = FFMAX(fast_retries, 2); wait_since = 0; &#125; len += ret; &#125; return len;&#125; 12345678910111213141516171819202122232425262728293031libavformat/tcp.cconst URLProtocol ff_tcp_protocol = &#123; .name = "tcp", .url_open = tcp_open, .url_accept = tcp_accept, .url_read = tcp_read, .url_write = tcp_write, .url_close = tcp_close, .url_get_file_handle = tcp_get_file_handle, .url_get_short_seek = tcp_get_window_size, .url_shutdown = tcp_shutdown, .priv_data_size = sizeof(TCPContext), .flags = URL_PROTOCOL_FLAG_NETWORK, .priv_data_class = &amp;tcp_class,&#125;;static int tcp_write(URLContext *h, const uint8_t *buf, int size)&#123; TCPContext *s = h-&gt;priv_data; int ret; if (!(h-&gt;flags &amp; AVIO_FLAG_NONBLOCK)) &#123; ret = ff_network_wait_fd_timeout(s-&gt;fd, 1, h-&gt;rw_timeout, &amp;h-&gt;interrupt_callback); if (ret) return ret; &#125; /// socket send ret = send(s-&gt;fd, buf, size, MSG_NOSIGNAL); return ret &lt; 0 ? ff_neterrno() : ret;&#125; 整体的代码流程如上，关键入口都在对应的地方加了注释 class relationavformat 初始化 根据 url协议找到 urlprotocol， 创建urlcontext &amp; aviocontext 三者的关系如下 123avformatcontext.pb = aviocontextaviocontext.opaque = urlcontexturlcontext.prot = urlprotocol urlprotocol privdata 指定为 httpcontexthttpcontext 内部持有的urlcontext 根据 tcp 找到tcpprotocolhttpconext.hd = urlcontext 后续进入到 tcp 发送数据的流程 SSL/TLS上面为了快速弄清楚调用过程，当做http处理，简化了tls这一块，https早已普及，所以https 跟 tcp 中间有一层 ssl/tls ffmpeg 内部处理tls协议的有好几个 gnutls openssl schannel securetransport libtls mbedtls根据不用的平台保留对应的哪一个，我的设备是mac，对应的是 securetransport 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208libavformat/tls_securetransport.cconst URLProtocol ff_tls_protocol = &#123; .name = "tls", .url_open2 = tls_open, .url_read = tls_read, .url_write = tls_write, .url_close = tls_close, .url_get_file_handle = tls_get_file_handle, .url_get_short_seek = tls_get_short_seek, .priv_data_size = sizeof(TLSContext), .flags = URL_PROTOCOL_FLAG_NETWORK, .priv_data_class = &amp;tls_class,&#125;;typedef struct TLSContext &#123; const AVClass *class; TLSShared tls_shared; SSLContextRef ssl_context; CFArrayRef ca_array; int lastErr;&#125; TLSContext;libavformat/tls.htypedef struct TLSShared &#123; char *ca_file; int verify; char *cert_file; char *key_file; int listen; char *host; char *http_proxy; char underlying_host[200]; int numerichost; /// tls内部就是tcp URLContext *tcp;&#125; TLSShared;static int tls_open(URLContext *h, const char *uri, int flags, AVDictionary **options)&#123; TLSContext *c = h-&gt;priv_data; TLSShared *s = &amp;c-&gt;tls_shared; int ret; /// tcp open if ((ret = ff_tls_open_underlying(s, h, uri, options)) &lt; 0) goto fail; /// sslcontext 创建过程 见底部引用的苹果文档 c-&gt;ssl_context = SSLCreateContext(NULL, s-&gt;listen ? kSSLServerSide : kSSLClientSide, kSSLStreamType); if (!c-&gt;ssl_context) &#123; av_log(h, AV_LOG_ERROR, "Unable to create SSL context\n"); ret = AVERROR(ENOMEM); goto fail; &#125; if (s-&gt;ca_file) &#123; if ((ret = load_ca(h)) &lt; 0) goto fail; &#125; if (s-&gt;ca_file || !s-&gt;verify) CHECK_ERROR(SSLSetSessionOption, c-&gt;ssl_context, kSSLSessionOptionBreakOnServerAuth, true); if (s-&gt;cert_file) if ((ret = load_cert(h)) &lt; 0) goto fail; CHECK_ERROR(SSLSetPeerDomainName, c-&gt;ssl_context, s-&gt;host, strlen(s-&gt;host)); /// io 回调 CHECK_ERROR(SSLSetIOFuncs, c-&gt;ssl_context, tls_read_cb, tls_write_cb); CHECK_ERROR(SSLSetConnection, c-&gt;ssl_context, h); while (1) &#123; OSStatus status = SSLHandshake(c-&gt;ssl_context); if (status == errSSLServerAuthCompleted) &#123; SecTrustRef peerTrust; SecTrustResultType trustResult; if (!s-&gt;verify) continue; if (SSLCopyPeerTrust(c-&gt;ssl_context, &amp;peerTrust) != noErr) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; if (SecTrustSetAnchorCertificates(peerTrust, c-&gt;ca_array) != noErr) &#123; ret = AVERROR_UNKNOWN; goto fail; &#125; if (SecTrustEvaluate(peerTrust, &amp;trustResult) != noErr) &#123; ret = AVERROR_UNKNOWN; goto fail; &#125; if (trustResult == kSecTrustResultProceed || trustResult == kSecTrustResultUnspecified) &#123; // certificate is trusted status = errSSLWouldBlock; // so we call SSLHandshake again &#125; else if (trustResult == kSecTrustResultRecoverableTrustFailure) &#123; // not trusted, for some reason other than being expired status = errSSLXCertChainInvalid; &#125; else &#123; // cannot use this certificate (fatal) status = errSSLBadCert; &#125; if (peerTrust) CFRelease(peerTrust); &#125; if (status == noErr) &#123; break; &#125; else if (status != errSSLWouldBlock) &#123; av_log(h, AV_LOG_ERROR, "Unable to negotiate TLS/SSL session: %i\n", (int)status); ret = AVERROR(EIO); goto fail; &#125; &#125; return 0;fail: tls_close(h); return ret;&#125;static OSStatus tls_read_cb(SSLConnectionRef connection, void *data, size_t *dataLength)&#123; URLContext *h = (URLContext*)connection; TLSContext *c = h-&gt;priv_data; size_t requested = *dataLength; /// 交给下一层的tcp去处理 read /// 这里读到的数据 还没有经过tls解码，不可读 int read = ffurl_read(c-&gt;tls_shared.tcp, data, requested); if (read &lt;= 0) &#123; *dataLength = 0; switch(AVUNERROR(read)) &#123; case ENOENT: case 0: return errSSLClosedGraceful; case ECONNRESET: return errSSLClosedAbort; case EAGAIN: return errSSLWouldBlock; default: c-&gt;lastErr = read; return ioErr; &#125; &#125; else &#123; *dataLength = read; if (read &lt; requested) return errSSLWouldBlock; else return noErr; &#125;&#125;static OSStatus tls_write_cb(SSLConnectionRef connection, const void *data, size_t *dataLength)&#123; URLContext *h = (URLContext*)connection; TLSContext *c = h-&gt;priv_data; /// 交给下一层的tcp去处理 write /// data 已经经过的tls的加密处理了。不可读 int written = ffurl_write(c-&gt;tls_shared.tcp, data, *dataLength); if (written &lt;= 0) &#123; *dataLength = 0; switch(AVUNERROR(written)) &#123; case EAGAIN: return errSSLWouldBlock; default: c-&gt;lastErr = written; return ioErr; &#125; &#125; else &#123; *dataLength = written; return noErr; &#125;&#125;static int tls_read(URLContext *h, uint8_t *buf, int size)&#123; TLSContext *c = h-&gt;priv_data; size_t available = 0, processed = 0; int ret; SSLGetBufferedReadSize(c-&gt;ssl_context, &amp;available); if (available) size = FFMIN(available, size); /// 读数据，这里读到的数据已经经过的tls的解密，是明文了 断点可以查看数据 ret = SSLRead(c-&gt;ssl_context, buf, size, &amp;processed); ret = map_ssl_error(ret, processed); if (ret &gt; 0) return ret; if (ret == 0) return AVERROR_EOF; return print_tls_error(h, ret);&#125;static int tls_write(URLContext *h, const uint8_t *buf, int size)&#123; TLSContext *c = h-&gt;priv_data; size_t processed = 0; /// 写数据，应用层的数据还没有经过tls的加密，断点可读 int ret = SSLWrite(c-&gt;ssl_context, buf, size, &amp;processed); ret = map_ssl_error(ret, processed); if (ret &gt; 0) return ret; if (ret == 0) return AVERROR_EOF; return print_tls_error(h, ret);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465libavformat/tls.cint ff_tls_open_underlying(TLSShared *c, URLContext *parent, const char *uri, AVDictionary **options)&#123; int port; const char *p; char buf[200], opts[50] = ""; struct addrinfo hints = &#123; 0 &#125;, *ai = NULL; const char *proxy_path; char *env_http_proxy, *env_no_proxy; int use_proxy; set_options(c, uri); if (c-&gt;listen) snprintf(opts, sizeof(opts), "?listen=1"); av_url_split(NULL, 0, NULL, 0, c-&gt;underlying_host, sizeof(c-&gt;underlying_host), &amp;port, NULL, 0, uri); p = strchr(uri, '?'); if (!p) &#123; p = opts; &#125; else &#123; if (av_find_info_tag(opts, sizeof(opts), "listen", p)) c-&gt;listen = 1; &#125; ff_url_join(buf, sizeof(buf), "tcp", NULL, c-&gt;underlying_host, port, "%s", p); hints.ai_flags = AI_NUMERICHOST; if (!getaddrinfo(c-&gt;underlying_host, NULL, &amp;hints, &amp;ai)) &#123; c-&gt;numerichost = 1; freeaddrinfo(ai); &#125; if (!c-&gt;host &amp;&amp; !(c-&gt;host = av_strdup(c-&gt;underlying_host))) return AVERROR(ENOMEM); env_http_proxy = getenv_utf8("http_proxy"); proxy_path = c-&gt;http_proxy ? c-&gt;http_proxy : env_http_proxy; env_no_proxy = getenv_utf8("no_proxy"); use_proxy = !ff_http_match_no_proxy(env_no_proxy, c-&gt;underlying_host) &amp;&amp; proxy_path &amp;&amp; av_strstart(proxy_path, "http://", NULL); freeenv_utf8(env_no_proxy); if (use_proxy) &#123; char proxy_host[200], proxy_auth[200], dest[200]; int proxy_port; av_url_split(NULL, 0, proxy_auth, sizeof(proxy_auth), proxy_host, sizeof(proxy_host), &amp;proxy_port, NULL, 0, proxy_path); ff_url_join(dest, sizeof(dest), NULL, NULL, c-&gt;underlying_host, port, NULL); ff_url_join(buf, sizeof(buf), "httpproxy", proxy_auth, proxy_host, proxy_port, "/%s", dest); &#125; freeenv_utf8(env_http_proxy); ///熟悉的方法，根据上面拼接的tcp:// 打开tcp return ffurl_open_whitelist(&amp;c-&gt;tcp, buf, AVIO_FLAG_READ_WRITE, &amp;parent-&gt;interrupt_callback, options, parent-&gt;protocol_whitelist, parent-&gt;protocol_blacklist, parent);&#125; tls这块的处理逻辑 作为http 跟 tcp的中间层，都是遵循的URLProtocol协议，所以流程基本跟http、tcp一样。关键点也同样加了对应的注释辅助理解，关于sslcontext 见下面的苹果文档， 如果熟悉tls的四次握手，基本还是可以理解。 summary整个代码过程顺下来，基本就是网络协议的调用过程，http/https -&gt; ssl/tls -&gt; tcp(socket) 最后来一张wireshark的抓包，很明显看到从tcp的三次握手开始， 紧接着tls的四次捂手，然后就是真正的请求数据的传输了… reference#macos security]]></content>
      <categories>
        <category>ffmepg</category>
      </categories>
      <tags>
        <tag>ffmepg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ios获取udid]]></title>
    <url>%2F2022%2F11%2F04%2Fios%E8%8E%B7%E5%8F%96udid%2F</url>
    <content type="text"><![CDATA[背景苹果企业账号打包分发app，方便内部安装测试App。最近公司企业账号续费失败，申请新的企业账号没成功。（国内近2年基本没有企业申请成功）这就蛋疼了，了解到其他app是通过多开发者账号，adhoc证书白名单的方式来做分发，adhoc证书无法解决设备数量的问题，最多100台设备，而且还要提前注册好，一年更新一次，注册设备就是要拿到设备的udid。 获取udid了解到苹果可以通过safari以OTA(over the air)的方式拿到udid。 整体流程分三步 下发udid.mobileconfig文件 授权安装配置文件 回调解析xml拿到udid udid.mobileconfig一个xml文件，文件内容如下 1234567891011121314151617181920212223242526272829303132333435&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;&lt;plist version="1.0"&gt; &lt;dict&gt; &lt;key&gt;PayloadContent&lt;/key&gt; &lt;dict&gt; &lt;key&gt;URL&lt;/key&gt; &lt;string&gt;https://xxx.net/udid/action&lt;/string&gt; &lt;!--接收数据的接口地址--&gt; &lt;key&gt;DeviceAttributes&lt;/key&gt; &lt;array&gt; &lt;string&gt;SERIAL&lt;/string&gt; &lt;string&gt;MAC_ADDRESS_EN0&lt;/string&gt; &lt;string&gt;UDID&lt;/string&gt; &lt;string&gt;IMEI&lt;/string&gt; &lt;string&gt;ICCID&lt;/string&gt; &lt;string&gt;VERSION&lt;/string&gt; &lt;string&gt;PRODUCT&lt;/string&gt; &lt;/array&gt; &lt;/dict&gt; &lt;key&gt;PayloadOrganization&lt;/key&gt; &lt;string&gt;dev.xxx.com&lt;/string&gt; &lt;!--组织名称--&gt; &lt;key&gt;PayloadDisplayName&lt;/key&gt; &lt;string&gt;查询设备UDID&lt;/string&gt; &lt;!--安装时显示的标题--&gt; &lt;key&gt;PayloadVersion&lt;/key&gt; &lt;integer&gt;1&lt;/integer&gt; &lt;key&gt;PayloadUUID&lt;/key&gt; &lt;string&gt;CD0BD3C5-7164-B9DA-FCBB-D81AA343C7A0&lt;/string&gt; &lt;!--自己随机填写的唯一字符串--&gt; &lt;key&gt;PayloadIdentifier&lt;/key&gt; &lt;string&gt;dev.xxx.profile-service&lt;/string&gt; &lt;key&gt;PayloadDescription&lt;/key&gt; &lt;string&gt;本文件仅用来获取设备ID&lt;/string&gt; &lt;!--描述--&gt; &lt;key&gt;PayloadType&lt;/key&gt; &lt;string&gt;Profile Service&lt;/string&gt; &lt;/dict&gt;&lt;/plist&gt; 需要注意就是回调接口 需要支持https，否者会报ATS错误 the resource could not be loaded because the app transport security policy requires the use of a secure connection 起service下发 xml 文件，我这里以java spring mvc来实现的 123456789101112131415161718@RequestMapping("/udid.mobileconfig") @ResponseBody public ResponseEntity&lt;byte[]&gt; downloadsEntity(HttpServletRequest request) throws Exception &#123; String path = Objects.requireNonNull(this.getClass().getClassLoader().getResource("udid.mobileconfig")).toURI().getPath(); File file = new File(path); if (!file.isFile()) &#123; return null; &#125; byte[] bytes = FileUtils.readFileToByteArray(file); HttpHeaders headers = new HttpHeaders(); headers.add("Content-type","application/x-apple-aspen-config; charset=utf-8"); // 按苹果的规范 headers.add("Content-Disposition", "attachment;filename=" + "udid.mobileconfig"); HttpStatus status = HttpStatus.OK; return new ResponseEntity&lt;&gt;(bytes, headers, status); &#125; 授权安装下载udid.mobileconfig之后，系统会自动在设置里面登记该配置文件，需要自行去设置里面 授权安装。安装成功之后 会回调 mobileconfig里面提供的回调地址，udid等信息会以参数的形式一并带上，格式也是xml格式。 如果下载的过程提示无效文件，可能就是mobileconfig文件内容有问题，可以通过pc浏览器检查下文件内容，我这里就遇到了文件乱码的情况 上面提到文件内容是以字节流的方式下发的，配置好ByteArrayHttpMessageConverter的顺序就ok了 123456789101112131415161718192021222324252627&lt;!--自动注册基于注解风格的处理器 (数据转换 格式化 数据)--&gt; &lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;ref bean="stringHttpMessageConverter"/&gt; &lt;!--配置ByteArrayHttpMessageConverter--&gt; &lt;bean class="org.springframework.http.converter.ByteArrayHttpMessageConverter"/&gt; &lt;bean class="com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter"&gt; &lt;property name="features"&gt; &lt;array&gt; &lt;!-- List字段如果为null,输出为[],而非nul --&gt; &lt;value&gt;WriteNullListAsEmpty&lt;/value&gt; &lt;!-- 字符类型字段如果为null,输出为”“,而非null --&gt; &lt;value&gt;WriteNullStringAsEmpty&lt;/value&gt; &lt;!-- 进制循环引用 --&gt; &lt;value&gt;DisableCircularReferenceDetect&lt;/value&gt; &lt;!--Boolean字段如果为null,输出为false,而非null --&gt; &lt;value&gt;WriteNullBooleanAsFalse&lt;/value&gt; &lt;!--时间 yyyy.MM.dd hh:mm:ss --&gt; &lt;value type="com.alibaba.fastjson.serializer.SerializerFeature"&gt;WriteDateUseDateFormat&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt; &lt;/mvc:annotation-driven&gt; 解析udid还是以java实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@RequestMapping(value = "/action", method = RequestMethod.POST) @ResponseBody public ResponseEntity&lt;String&gt; action(HttpServletRequest request) throws IOException &#123; //获取HTTP请求的输入流 InputStream is = request.getInputStream(); //已HTTP请求输入流建立一个BufferedReader对象 BufferedReader br = new BufferedReader(new InputStreamReader(is,"UTF-8")); StringBuilder sb = new StringBuilder(); //读取HTTP请求内容 String buffer = null; while ((buffer = br.readLine()) != null) &#123; sb.append(buffer); &#125; String content = sb.toString().substring(sb.toString().indexOf("&lt;?xml"), sb.toString().indexOf("&lt;/plist&gt;")+8); System.out.println(content); // 创建xml解析对象 SAXReader reader = new SAXReader(); // 定义一个文档 Document document = null; //将字符串转换为 try &#123; document = reader.read(new ByteArrayInputStream(content.getBytes("GBK"))); &#125; catch (DocumentException e) &#123; e.printStackTrace(); &#125; String udid = "ERROR"; if(document != null) &#123; Element element = (Element) document.selectNodes("/plist/dict").get(0); for(Iterator iter = element.content().iterator(); iter.hasNext();) &#123; DefaultElement next = (DefaultElement) iter.next(); String name = next.getName(); String text = next.getText(); if(StringUtils.isEquals(name,"key") &amp;&amp; StringUtils.isEquals(text,"UDID") &amp;&amp; iter.hasNext()) &#123; next = (DefaultElement) iter.next(); name = next.getName(); text = next.getText(); udid = text; &#125; &#125; &#125; HttpHeaders headers = new HttpHeaders(); headers.add("Content-type","text/html; charset=utf-8"); headers.setLocation(URI.create("https://xxx.com/udid/udid?udid="+udid)); /// 跳转到指定的页面 展示udid信息 HttpStatus status = HttpStatus.MOVED_PERMANENTLY; return new ResponseEntity&lt;&gt;("", headers, status); &#125; 解析xml就可以拿到udid字段，注意的是 重定向一定要使用301重定向（MOVED_PERMANENTLY）,有些重定向默认是302重定向,这样就会导致安装失败,设备安装会提示”无效的描述文件]]></content>
      <categories>
        <category>ios</category>
      </categories>
      <tags>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MP4笔记]]></title>
    <url>%2F2022%2F10%2F28%2FMP4%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MP4 BOX第一层Ftyp、Moov、Mdat、Free等: ftyp Boxftyp是MP4文件的第一个Box，包含了视频文件使用的编码格式、标准等，这个Box作用基本就是MP4这种封装格式的标识，同时在一份MP4文件中只有一个这样的Box。ftyp box通常放在文件的开始，通过对该box解析可以让我们的软件（播放器、demux、解析器）知道应该使用哪种协议对这该文件解析，是后续解读文件基础 Moov BoxMoov Box这个Box也是MP4文件中必须有但是只存在一个的Box,这个Box里面一般存的是媒体文件的元数据，这个Box本身是很简单的，是一种Container Box，里面的数据是子Box,自己更像是一个分界标识。 所谓的媒体元数据主要包含类似SPS PPS的编解码参数信息，还有音视频的时间戳等信息。对于MP4还有一个重要的采样表stbl信息，这里面定义了采样Sample、Chunk、Track的映射关系，是MP4能够进行随机拖动和播放的关键，也是需要好好理解的部分，对于实现一些音视频特殊操作很有帮助。 Mdat BoxMdat Box这个Box是存储音视频数据的Box，要从这个Box解封装出真实的媒体数据。当然这个Box一般都会存在，但是不是必须的。 在前面的文章《音视频压缩：H264码流层次结构和NALU详解》中已经讲解了H264的基本结构是由一系列的NALU组成。原始的NALU单元组成： Start code + NALU header + NALU payload 但是在MP4格式文件中，H264 slice并不是以00 00 00 01 Start Code来进行分割，而是存储在Mdat Box的Data中。 Mdat Box的格式： Box header + Box Data Box length + Box Type + NALU length + NALU header + Nalu Data……. NALU length + NALU header + Nalu Data Free BoxFree Box中的内容是无关紧要的，可以被忽略即该box被删除后，不会对播放产生任何影响。这种类型的Box也不是必须的，可有可无，类似的Box还有Sikp Box.虽然在解析是可以忽略，但是需要注意该Box的删除对其它Box的偏移量影响，特别是当Moov Box放到Mdat Box后面的情况。 MOOVMvhd Box这个Box也是全文件唯一的一个Box,一般处于Moov Box的第一个子Box,这个Box对整个媒体文件所包含的媒体数据（包含Video track和Audio Track等）进行全面的描述。其中包含了媒体的创建和修改时间，默认音量、色域、时长等信息。 Trak BoxTrak Box定义了媒体文件中媒体中一个Track的信息，视频有Video Track,音频有Audio Track，媒体文件中可以有多个Track，每个Track具有自己独立的时间和空间的信息，可以进行独立操作。 每个Track Box都需要有一个Tkhd Box和Mdia Box，其它的Box都是可选择的 TrakTkhd Box该Box描述了该Track的媒体整体信息包括时长、图像的宽度和高度等，实际比较重要 Mdia Box这个Box也是Container Box，里面包含子Box，一般必须有Mdhd Box、Hdlr Box、Minf Box。基本就是当前Track媒体头信息和媒体句柄以及媒体信息。 Box自身非常简单，就是一个标识而已，最复杂的还是里面包含的子Box. MdiaMdhd Box这个Box是Full Box，意味着Box Header有Version和Flag字段，该Box里面主要定义了该Track的媒体头信息，其中我们最关心的两个字段是Time scale和Duration，分别表示了该Track的时间戳和时长信息，这个时间戳信息也是PTS和DTS的单位。 Hdlr Box这个Box是Full Box，意味着Box Header有Version和Flag字段，该Box解释了媒体的播放过程信息，用来设置不同Track的处理方式，标识了该Track的类型。 Minf Box这个Box是我认为Moov Box里面最重要最复杂的Box，内部还有子Box，我们还是从上到下从外到内的分析各个Box。该Box建立了时间到真实音视频Sample的映射关系，对于音视频数据操作时很有帮助的。 同时该Box是Container Box，下面一般含有三大必须的子Box: 媒体信息头Box: Vmhd Box或者Smhd Box; 数据信息Box：Dinf Box 采样表Box：Stbl Box MinfVmhd BoxDinf BoxStbl Box怎样把媒体数据的Sample和时间进行映射的，看下Sample-Trunk-Track的三者关系。 我们知道Sample是媒体数据的存储单元，其中存储在Media的chunk中，在分析Mdat Box的H264 NALU打包时已经体现出来了 其中每个Sample的时间和位置、编解码信息、和chunk关系等都是由Stbl Box来描述的，该Box又称为采样参数列表即Sample Table。 Stbl Box包含了Track中media媒体的所有时间和索引，利用这个容器的Sample信息，就可以定位Sample的媒体时间、类型、大小以及和其相邻的Sample。同时该Box是必须在Trak Box中存在的。 其一般要包含下列子Box： 采样描述容器： Sample Description即Stsd Box 采样时间容器： Time To Sample 即Stts Box 采样同步容器： Sync Sample即Stss Chunk采样容器: Sample To Chunk即Stsc 采样大小容器： Sample Size即Stsz Chunk偏移容器：Chunk Offest即Stco Stsd Box 该Box存储了编码类型和初始化解码器需要的信息，与特定的Track Type有关，根于不同的Track使用不一样的编码标准。 box header和version字段后会有一个entry count字段，根据entry的个数，每个entry会有type信息，如“vide”、“sund”等，根据type不同sample description会提供不同的信息，例如对于video track，会有“VisualSampleEntry”类型信息，对于audio track会有“AudioSampleEntry”类型信息。视频的编码类型、宽高、长度，音频的声道、采样等信息都会出现在这个box中。 对于Video Track里面而言，H264的SPS PPS就存在该Box里面，对于解码非常重要。 Stsd/Avc1 Box: 当为Video Track，所以VisualSampleEntry为Avc1 Box，内部含有SPS PPS 等音视频解码信息。 Stsd/Avc1/AVCC Box 该Box则包含了真实的SPS PPS等信息，包含着音视频编解码参数： 音频的STSD的Box Data主要由mp4a和esds Box嵌套组成，里面包含了通道个数，采样率、音频AAC编码级别等信息 Stsd/mp4a Box: Stsd/mp4a/Esds Box 里面主要是将音频的编码信息和音频码率信息放到该Box里面，所以解码音频时非常关键。 Stts Box 这个Box是sample number和解码时间DTS之间的映射表，通过这个表格，我们可以找到任何时间的sample。Stts Box这个表格有两项值，其中一项是连续的样点数目即sample count和样点相对时间差值即sample delta即表格中每个条目提供了在同一个时间偏移量里面连续的sample序号以及sample偏移量。当然这里的相对时间差单位是由该Track的Mdhd Box描述的时间单位 Ctts Box 该Box保存了每个sample的composition time和decode time之间的差值，这里通过Composition Time就可以计算出Sample的PTS。 不存在B帧情况下PTS是等于DTS的，则不含B帧的视频文件是没有Ctts Box的，同样音频也没有Ctts Box。 Stss Box I帧是播放的起始位置，只有编码器拿到第一个I帧才能渲染出第一幅画面。所以后续的一些特殊随机操作，高标清切换时都需要找I帧，只有随机找到I帧才能完成这些特殊操作。 其中该Box就是存储了那些Sample是I关键帧，很显然音频Track也是不存在这个Box的。 Stsc Box MP4中存在Track-Trunk-Sample概念，讲到现在Track和Sample已经都讲到了，但是还没有讲解Trunk和Sample的映射关系，这个Box就是说明那些Sample可以划分为一个Trunk。 媒体数据被分为若干个chunk, chunk可以有不同的大小，同一个chunk中的样点sample也允许有不同的大小；通过本表可以定位一个样点的chunk位置,同时该Box里面的Box Data里面有三个字段，分别是first chunk、sample per chunk、sample description index。 fist chunk: 具有相同采样点sample和sample_description_index的chunk中，第一个chunk的索引值,也就是说该chunk索引值一直到下一个索引值之间的所有chunk都具有相同的sample个数，同时这些sample的描述description也一样； samples per chunk: 上面所有chunk的sample个数 sample description index: 描述采样点的采样描述项的索引值，范围为1到样本描述表中的表项数目； 这 3 个字段实际上决定了一个 MP4 中有多少个 chunks，每个 chunks 有多少个 samples。 Stsz Box 前面分析了sample的PTS、DTS等，也分析了chunk里面sample的信息，但是没有分析sample的大小，这是我们在文件读取和解析Sample的关键。这里给出每个Sample的Size即包含的字节数。 包含了媒体中全部sample的数目和一张给出每个sample大小的表。这个box相对来说体积是比较大的。 Stco Box该Box存储了Chunk Offset，表示了每个Chunk在文件中的位置，这样我们就能找到了chunk在文件的偏移量，然后根据其它表的关联关系就可以读取每个Sample的大小。 H.264数据打包MP4文件步骤 构造Ftyp Box，这是MP4文件的基本标识，基本按照规范构造即可； 构造Mdat Box，这里面都是音视频媒体数据，将NALU数据封装成一个个Sample，从上到下排列起来即可； 构造Moov Box,这个Box，先构造除了stbl的其它Box,这样从子Box，一直按照层次构造上来，这些Box的基本信息是明确的，个别信息通过解析SPS、PPS是可以明确知道的，所以上文已经明确了这些Box的字段，所以构造起来并不难； 构造Moov Box的Stbl部分，这部分要封装： SPS、PPS的NALU数据 也要构造时间戳PTS和DTS信息 Sample的大小和数量 Chunk的数量和在文件偏移量 Sample和Chunk的映射关系等， 需要仔细处理，一旦构造错误都会播放失败； 其中Moov到底是放到Mdat Box前面还是后面，需要在封装打包前确定后，因为会牵一发动全身，里面的信息都依赖在具体文件的位置，至于前面后面区别见下篇文章； 再填充一些你感兴趣的非必要信息Box，至此组装成整个MP4文件即可； reference# MP4核心Box详解]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[H264笔记]]></title>
    <url>%2F2022%2F10%2F26%2FH264%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[H264H264是视频编码规范，在这个规范下H264码流有两种方式：AnnexB格式 &amp;&amp; AVCC格式 AnnexB AnnexB 的原理是在每个 NALU 前面写上一个特殊的起始码，通过这个起始码来当做 NALU 的分隔符，从而分割每个 NALU [start code] NALU [start code] NALU [start code] NALU AVCC avcC 则采用了另外一种方式。那就是在 NALU 前面写上几个字节，这几个字节组成一个整数（大端字节序）这个整数表示了整个 NALU 的长度 [extra data] [length] NALU [length] NALU [length] NALU NALUNALU (Network Abstraction Layer Unit) 翻译过来就是网络抽象层单元。在 H.264/AVC 视频编码标准中，最外面一层叫做 NAL(Network Abstract Layer) 网络抽象层。所有的码流数据，最终都被封装成了一个一个的 NALU（Network Abstract Layer Unit）就是网络抽象层单元。 对应的NAL 还有一个视频编码层VCL(video code layer)，视频编码后的原始数据 SODB （String Of Data Bits） 视频编码层（VCL），是对视频编码核心算法过程、子宏块、宏块、片等概念的定义。这层主要是为了尽可能的独立于网络来高效的对视频内容进行编码。编码完成后，输出的数据是 SODB（String Of Data Bits）。 网络适配层（NAL），是对图像序列、图像等片级别以上的概念的定义。这层负责将 VCL 产生的比特字符串适配到各种各样的网络和多元环境中。该层将 VCL 层输出的 SODB 数据打包成 RBSP（Raw Byte Sequence Payload）。SODB 是编码后的原始数据，RBSP 是在原始编码数据后面添加了结尾比特，一个比特 1 和若干个比特 0，用于字节对齐。然后再在 RBSP 头部加上 NAL Header 来组成一个一个的 NAL 单元。 sps &amp; ppsSPS（Sequence Paramater Set） 序列参数集 PPS（Picture Paramater Set） 图像参数集 SPS 和 PPS 存放的的信息是码流中的参数信息，后续的解码流程是要依赖着里面的参数的，所以解码器在解码一路码流的时候，总是要首先读入 SPS 和 PPS AnnexB 格式的码流，sps 跟 pps 是作为nalu存在的，一般都最前面AVCC格式的码流，sps 跟 pps 是放在extra data中。 NALU header nalu 的第一个字节，其实就是nalu header forbidden_zero_bit nal_ref_idc nal_unit_type forbidden_zero_bit 占用 1 位，nal_ref_idc 占用 2 位，nal_unit_type 占用 5 位。三个元素一共占用 8 位，也就是一个字节 forbidden_zero_bit 禁止位，正常情况下为 0。在某些情况下，如果 NALU 发生丢失数据的情况，可以将这一位置为 1，以便接收方纠错或丢掉该单元 nal_ref_idc 该元素表示这个 NALU 的重要性。可能的值有 4 个，越重要的 NALU 越不能丢弃 nal_ref_idc 重要性 3 HIGHEST 2 HIGH 1 LOW 0 DISPOSABLE nal_unit_type nal_unit_type NALU 类型 0 未定义 1 非 IDR SLICE slice_layer_without_partitioning_rbsp( ) 2 非 IDR SLICE，采用 A 类数据划分片段 slice_data_partition_a_layer_rbsp( ) 3 非 IDR SLICE，采用 B 类数据划分片段 slice_data_partition_b_layer_rbsp( ) 4 非 IDR SLICE，采用 C 类数据划分片段 slice_data_partition_c_layer_rbsp( ) 5 IDR SLICE slice_layer_without_partitioning_rbsp( ) 6 补充增强信息 SEI sei_rbsp( ) 7 序列参数集 SPS seq_parameter_set_rbsp( ) 8 图像参数集 PPS pic_parameter_set_rbsp( ) 9 分隔符 access_unit_delimiter_rbsp( ) 10 序列结束符 end_of_seq_rbsp( ) 11 码流结束符 end_of_stream_rbsp( ) 12 填充数据 filler_data_rbsp( ) 13 序列参数扩展集 seq_parameter_set_extension_rbsp( ) 14~18 保留 19 未分割的辅助编码图像的编码条带 slice_layer_without_partitioning_rbsp( ) 20~23 保留 24~31 未指定 slice我们之前介绍过 SPS 和 PPS，SPS 和 PPS 中储存的信息是一些参数项，例如，图像的长宽，图像的 profile 信息等。那么在 Slice 中，存放的信息就是编码后的图像信息了，也就是说，解码 Slice，我们就能还原出来图像了。 一个 Slice 通常被分为两个部分，Slice Header 和 Slice Body Slice Header Slice Body slice_type 值 含义 0 P(P Slice) 1 B(B Slice) 2 I(I Slice) 3 SP(SP Slice) 4 SI(SI Slice) 5 P(P Slice) 6 B(B Slice) 7 I(I Slice) 8 SP(SP Slice) 9 SI(SI Slice) 可以看到 slice_type 规定了 slice 的类型，这也解释了之前的一些误区。有人说根据 NALU 的类型就能判读是 I Slice 还是 P Slice 还是 B Slice。其实是错误的，要判断这些必须要读到 slice_type 才行 pic_parameter_set_id slice_type 之后，是 pic_parameter_set_id，这个属性表明该 Slice 要依赖的 PPS 的 id。我们在解析 PPS 的时候，PPS 里面有一个 pic_parameter_set_id 的属性。当你解析出 Slice 中的 pic_parameter_set_id 之后，拿着这个 id 找到与之对应的 PPS 就可以了。然后 PPS 里还有个 seq_parameter_set_id，用这个 id 就可以找到依赖的 SPS 的 id。这样，你就可以为这个 Slice 查找到合适的 SPS 和 PPS 了。 MP4h264编码之后的码流，最终还是会封装到各种媒体文件中，这里就只了解下最常用的mp4. Mdat Box这个Box是存储音视频数据的Box，要从这个Box解封装出真实的媒体数据。当然这个Box一般都会存在，但是不是必须的。但是在MP4格式文件中，H264 slice并不是以00 00 00 01 Start Code来进行分割，而是存储在Mdat Box的Data中。 Mdat Box的格式： Box header + Box Data Box length + Box Type + NALU length + NALU header + Nalu Data……. NALU length + NALU header + Nalu Data Mdat里的NALU一般不再包含SPS PPS等数据，这些数据已经放到Moov Box里面了 mp4里的NALU其实就是AVCC格式的 references深入浅出理解视频编码 H264 结构最详尽的 H.264 编码相关概念介绍自己动手写 H.264 解码器H.264 媒体流 AnnexB 和 AVCC 格式分析 及 FFmpeg 解析mp4的H.264码流方法]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[形状特效]]></title>
    <url>%2F2022%2F10%2F14%2F%E5%BD%A2%E7%8A%B6%E7%89%B9%E6%95%88%2F</url>
    <content type="text"><![CDATA[形状特效初次接触到形状特效是在剪映，剪映中叫蒙版，就是列举了一些常见形状的蒙层效果，除了常规的旋转、缩放、位移等操作外，还有个虚化的效果。可以去剪映感受下。 接触到这东西，觉得效果挺不错，打算研究下，正好手头的编辑工具项目也可以用上。线性、圆形、矩形，这几个规范的，通过平方根公式 跟 三角形余弦定理基本都可以推算出来(可以参考github shader)，重点来了，爱心 &amp; 五角星 怎么搞？ 特殊形状爱心 、五角星，一开始觉得可能是通过 三角函数、关键点、贝塞尔曲线 等数学计算实现的？ 想想觉得这样是不是过于复杂， 一翻google之后，还是没啥思路。放大招抓包，剪映的素材都是下发的，必然会有网络请求的，一顿操作之后，如愿拿到了资源包。 爱心 五角星 看到了这两个，很明显不是通过复杂数学计算得到的。 但是这一层一层的渐变红绿图 是什么鬼？资源包里还有shader文件的😄 float alpha = (col.r * 4.0 * 256.0 + col.g * 255.0) / 781.0; 重点的就是这行计算逻辑，还是懵的。 col.r col.g 对应图上的红绿分量可以理解*4 /781 是什么鬼？ 4 对应 图上的四层? 用颜色取色器取色看看，这一看就明白了。 1234567第1层c00d00 第2层800000~80ff00 第3层400000~40ff00 第4层000000~00ff00R * 4 + G 算下来 从 0 到 781 781 = 12(c0) * 16 * 4 + 13(0d) 每层红色分量固定(00/40/80/c0)，绿色分量渐变0到255(00-ff)，所以整个图层解析下来就是 从0 到 781. alpha 计算下来就有 256 *3 + 1 = 769 个值，最里面的一层最小的，固定为1，保障无虚化的时候也有最基本的形状效果。 搞明白之后，顿时觉得秒啊， 回头一想，为什么要搞的那么多复杂呢？ 反正是为了计算alpha，直接搞个灰度图 从黑到白，不就好了？仔细一想，灰度图一个分量最多256个，最终的效果割裂感会很明显，所以用到了r、g 两个分量，让效果更丝滑，实际上如果你想 也可以用rbg三个分量，重新设计下计算公式 让范围更大更丝滑。 举一反三学以致用，那必须举一反三下了。 花型 用五角星图去找设计师，参考一下做一个花型的。设计师也一下没掌握精髓，照着搞了一个但不是我想要的。所以有了人生第一次指导设计师画图了，一翻操作之后 搞定了 最后的shader 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#iUniform float iReverse = 0.#iUniform float _X = 0.5#iUniform float _Y = 0.5#iUniform float _S = 0.25#iUniform float _A = 0.#iUniform float _F = 0.00#iUniform float iPlatform_iOS = 1.0const float PI = 3.1415926;#iChannel0 &quot;file://followw813654.jpeg&quot;#iChannel1 &quot;file://mask1.png&quot;vec2 rotation(vec2 uv, float angle, float ratio)&#123; vec2 center = vec2(0.5, 0.5); mat2 zRotation = mat2(cos(angle), sin(angle), -sin(angle) * ratio, cos(angle) * ratio); vec2 centeredPoint = uv - center; vec2 newUv = zRotation * centeredPoint; return vec2(newUv.x, newUv.y / ratio) + center;&#125;vec2 scale(vec2 uv, vec2 scale)&#123; vec2 newPos = vec2(0.5) + (uv - vec2(0.5)) / scale; return newPos;&#125;vec2 offset(vec2 uv, vec2 offset)&#123; return uv + offset;&#125;void main()&#123; highp vec2 textureCoordinate = gl_FragCoord.xy/iResolution.xy; float radio = iResolution.x / iResolution.y; bool ls = (radio &gt; 1.0); vec4 base = texture(iChannel0, textureCoordinate); float _Radio = 1.0; float _Scale = _S * 5.; _Scale = ls ? _Scale / radio : _Scale; float _Angle = 360. * PI / 180. * _A; vec2 _Offset = vec2(_X * 2.0 - 1.0, (_Y * 2.0 - 1.0)); vec2 newUV = offset(textureCoordinate, vec2(-_Offset.x, _Offset.y)); newUV = scale(newUV, vec2(1. * _Scale, radio * _Scale)); newUV = rotation(newUV, _Angle, _Radio); newUV.y = (iPlatform_iOS == 1.) ? newUV.y : (1. - newUV.y); vec4 mask = texture(iChannel1, newUV) * step(newUV.x, 1.) * step(newUV.y, 1.) * step(0., newUV.x) * step(0., newUV.y); vec2 col = mask.rg; // 怎么理解 ？ 根据 mask图的 作图颜色搭配 动态调整 // 第1层c00d00 第2层800000~80ff00 第3层400000~40ff00 第4层000000~00ff00 // R * 4 + G 算下来 从 0 到 781 // 781 = 12 * 16 * 4 + 13 float alpha = (col.r * 4.0 * 256.0 + col.g * 255.0) / 781.0; alpha = smoothstep(0.49 - abs(sin(iTime/2.)), 0.51 + abs(sin(iTime/2.)), alpha); if (iReverse &gt; .5) &#123; alpha = 1.0 - alpha; &#125; gl_FragColor = mix(vec4(0, 0, 0, alpha), base, alpha);&#125; 完整效果]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态滤镜]]></title>
    <url>%2F2022%2F10%2F14%2F%E5%8A%A8%E6%80%81%E6%BB%A4%E9%95%9C%2F</url>
    <content type="text"><![CDATA[滤镜音视频编辑工具中，对图像或者视频加滤镜是一种常见的做法，普通滤镜很好理解，一张LUT图就搞定了，LUT图一般设计师会提供好，开发拿过来转成纹理，opengl做下颜色转换就好了，这里就不多说了。 但是今天想讲的是动态滤镜，也有叫特效的，动效等等。比如小红书/剪映 等等,感受一下就大概知道了。 动态滤镜先上一个效果感受下： 视频： 很明显动态滤镜跟普通滤镜根本上就不是一回事了，所以更像是动效 特效了，这里不纠结叫什么，重点是关注下怎么做的呢？直观感受就是静态图片上覆盖了一帧帧的透明图片，确实就是这么一回事。 用一帧帧的带alpha通道的图片集，通过opengl blend一帧帧融合也可以实现， 弊端很明显，虽然这种动态效果一般也只有几秒，按30fps，也需要百来张图，图片资源很大，gif格式也会存在同样的问题。如果用视频呢？ 但是视频没有alpha通道。。 方案参考腾讯动画方案 vap 我们用一种特殊的视频，视频的一半表示rgb 另一半表示alpha通道。 表示alpha通道的 rgb 三个分量值一样，所以是灰度图效果，知道这种特殊的视频构成 接下来就好处理了 123456789101112( precision mediump float; varying highp vec2 textureCoordinate; uniform sampler2D inputImageTexture; void main() &#123; vec4 textureColor = texture2D(inputImageTexture,textureCoordinate); vec4 leftColor = texture2D(inputImageTexture,vec2(textureCoordinate.x / 2.0,textureCoordinate.y)); vec4 rightColor = texture2D(inputImageTexture,vec2(textureCoordinate.x / 2.0 + 0.5,textureCoordinate.y)); gl_FragColor = vec4(leftColor.rgb * rightColor.b ,rightColor.b); &#125;); 实际应用中，借用GPUImageMoive来解码一半一半的视频，然后自定义filter，fragmentshader 就是上面的这段，最后渲染到GPUImageView上，就达到了在静态图片上播放透明视频的效果。 12345678910111213141516171819@interface GPUImageCustomFilter : GPUImageFilter@end@implementation GPUImageCustomFilter- (id)init &#123; if (self = [super initWithFragmentShaderFromString:kGPUImageCustomFragmentShaderString]) &#123; &#125; return self;&#125;/// size width / 2- (void)setInputSize:(CGSize)newSize atIndex:(NSInteger)textureIndex &#123; [super setInputSize:CGSizeMake(newSize.width / 2.0, newSize.height) atIndex:textureIndex];&#125;@end 使用customFilter 123456789101112131415161718NSURL *url = [NSURL fileURLWithPath:videoPath];CGRect frame = previewView.bounds;GPUImageView *gpuImageView = [[GPUImageView alloc] init];gpuImageView.backgroundColor = [UIColor clearColor];gpuImageView.fillMode = kGPUImageFillModePreserveAspectRatioAndFill;gpuImageView.frame = frame;GPUImageMovie *movie = [[GPUImageMovie alloc] initWithURL:url];movie.shouldRepeat = YES;movie.playAtActualSpeed = YES;movie.runBenchmark = YES;GPUImageCustomFilter *customFilter = [[GPUImageCustomFilter alloc] init];[movie addTarget:customFilter];[customFilter addTarget:gpuImageView];[movie startProcessing];]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像描边]]></title>
    <url>%2F2022%2F09%2F10%2F%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[image border最近项目中搞了个图像描边的需求，常见的美图工具App都有类似的功能，典型的如美图秀秀，一开始觉得应该不太复杂，正常评估时间，实际做的时候，发现问题比想象中的复杂多了，结果项目不得不延期 😭，所以有必要搞篇文章来总结下教训。 先说整体的流程： 1、原图 -&gt; 2、抠图 -&gt; 3、边缘检测 -&gt; 4、绘制边缘 -&gt; 5、结果导出 这个流程还是很容易想到，但是除了最后一步相对来说容易点，2、3、4都是一路坑 😞 image matting首先是抠图，就是这样的 跟我们这边的算法同学对接，爬虫收集图像、标注、模型训练 一套组合下来，效果不理想，生产环境不可用，第一步就卡住了 😞 为了赶项目周期，最后使用了阿里云的方案，这里就不多说了，算法同学持续优化模型，待成熟之后替换阿里云。 Edge detection这一步相对来说是最复杂的，这里遇到的问题也是最大，耗时最久 最初的方案大致是这样的：抠图结果-&gt;采样缩放图像-&gt;遍历图像bitmap取满足条件的点，条件简单的理解就是点周围3x3范围的点像素值取平均值。为什么要检测边缘，是因为要做虚线描边，获取连续的边缘点之后然后在画布上连接点画出来。 代码大概是这样的… 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647+ (NSArray *)imageFindContours:(UIImage *)image &#123; NSMutableArray *points = [[NSMutableArray array] init]; UIImage *newImage = [image mediumResolution:CGSizeMake(30, 30)]; CFDataRef imageData = CGDataProviderCopyData(CGImageGetDataProvider(newImage.CGImage)); const uint8_t *data = CFDataGetBytePtr(imageData); int w = newImage.size.width; int h = newImage.size.height; unsigned char *bitmap = malloc(w * h * 4); memcpy(bitmap, data, w * h * 4); CGFloat leftmost = 1; CGFloat rightmost = 0; for (int i = 1; i &lt; h - 1; i += 2) &#123; for (int j = 1; j &lt; w - 1; j += 2) &#123; unsigned int left = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, 0)]; unsigned int right = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, 0)]; unsigned int up = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, -1)]; unsigned int down = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, 1)]; unsigned int leftUp = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, -1)]; unsigned int rightUp = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, -1)]; unsigned int leftDown = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, 1)]; unsigned int rightDown = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, 1)]; unsigned int center = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, 0)]; unsigned int avg = (left + right + up + down + leftUp + rightUp + leftDown + rightDown + center) / 9; int offset = i * w + j; if ((avg &gt;= (255. * 0.4) &amp;&amp; avg &lt;= (255. * 0.9)) &amp;&amp; center &gt; 65) &#123; bitmap[offset * 4] = 255; bitmap[offset * 4 + 1] = 0; bitmap[offset * 4 + 2] = 0; bitmap[offset * 4 + 3] = 255; CGFloat scale = 1.15; CGFloat x = (CGFloat)((((float) j / w) - 0.5) * scale + 0.5); CGFloat y = (CGFloat)((((float) i / h) - 0.5) * scale + 0.5); CGPoint point = CGPointMake(x, y); [points addObject:[NSValue valueWithCGPoint:point]]; if (x &lt;= leftmost) leftmost = x; if (x &gt;= rightmost) rightmost = x; &#125; &#125; &#125; ...&#125; 这个有个致命的问题，就是找到点之后，但是没办法有序的连起来，也试过一些方案，但是图像的边缘情况太复杂了，总是有问题，这里就不多说了。 接下来就需求其他的方案，大名鼎鼎的OpenCV出场了，参考官方文档，编译产物，接入app 调试下来就能获取正确的结果了，这里就不多说了，直接看下代码，对应的节点有注释说明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859+ (NSArray *)findContours:(UIImage *)image &#123; Mat src; Mat src_gray; src = [self cvMatFromUIImage:image]; cvtColor(src, src_gray, COLOR_BGR2GRAY); //UIImage *grayImg = [self UIImageFromCVMat:src_gray]; blur(src_gray, src_gray, cv::Size(3, 3)); //UIImage *blurgrayImg = [self UIImageFromCVMat:src_gray]; /// 利用阈值二值化 threshold(src_gray,src_gray,128,255,cv::THRESH_BINARY); /// 用Canny算子检测边缘 //Canny(src_gray, src_gray, 128, 255 , 3); //UIImage *canny_outputImg = [self UIImageFromCVMat:src_gray]; vector&lt;vector&lt;cv::Point&gt; &gt; contours; vector&lt;Vec4i&gt; hierarchy; /// 寻找轮廓 findContours(src_gray, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, cv::Point(0, 0)); /// 绘出轮廓 Mat drawing = Mat::zeros(src_gray.size(), CV_8UC3); for (int i = 0; i &lt; contours.size(); i++) &#123; Scalar color = Scalar(255, 255, 255); drawContours(drawing, contours, i, color, 1, 8, hierarchy, 0, cv::Point()); &#125; //UIImage *contoursImg = [self UIImageFromCVMat:drawing]; NSMutableArray *array = [NSMutableArray arrayWithCapacity:contours.size()]; for (int i = 0; i &lt; contours.size(); i++) &#123; if (hierarchy[i][3] &gt;= 0 || hierarchy[i][2] &gt;= 0) &#123; continue; &#125; vector&lt;cv::Point&gt; vect = contours[i]; std::vector&lt;cv::Point&gt;::const_iterator it; // declare a read-only iterator it = vect.cbegin(); // assign it to the start of the vector while (it != vect.cend()) &#123; // while it hasn't reach the end //std::cout &lt;&lt; it-&gt;x &lt;&lt;' '&lt;&lt; it-&gt;y &lt;&lt;' '; // print the value of the element it points to [array addObject:@(CGPointMake(it-&gt;x / image.size.width, it-&gt;y / image.size.height))]; ++it; // and iterate to the next element &#125; &#125; return @[array];&#125; 这个方案唯一的缺陷就是需要引入OpenCV静态库，增加包大小，也想过咱只用到了边缘检测，其他的牛逼功能暂时也用不到，裁剪下只保留需要的类是不是就可以，但是大致翻了下，牵扯的太多，最终放弃了, 最后也并没有使用OpenCV的方案。 因为发现了更轻量级的方案，Suzuki边缘检测算法，后面也了解该算法其实就是OpenCV内部的一种边缘检测方案。 恰好Android同学找到了一个开源库，java版本的Suzuki边缘检测算法。代码拉下来结合算法文档来回撸几遍，大致能理解了，android直接java拖进去用上，ios翻译成OC，也不复杂，因为算法核心方法也不过几十行代码，就算不理解，硬翻也能翻译过来。 贴一下翻译成OC的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212+ (NSArray *)findContours:(UIImage *)img threshold:(CGFloat)threshold &#123; img = [self blurImage:img blur:0.2]; /// 记录原图尺寸 int ow = (int) img.size.width; int w = ow; int h = (int) img.size.height; /// 考虑字节对齐 w 要重新计算 CFDataRef imageData = CGDataProviderCopyData(CGImageGetDataProvider(img.CGImage)); const uint8_t *data = CFDataGetBytePtr(imageData); w = (int) CFDataGetLength(imageData) / (h*4); char *F = malloc((size_t) CFDataGetLength(imageData)/4); /// 二值化处理 threshold *= 255.f; for (int i = 0; i &lt; h; i++) &#123; for (int j = 0; j &lt; w; j++) &#123; if (data[(i * w + j) * 4] &gt; threshold) &#123; F[i * w + j] = 1; &#125; else &#123; F[i * w + j] = 0; &#125; &#125; &#125; NSMutableArray&lt;Contour *&gt; *contours = [NSMutableArray array]; for (int i = 1; i &lt; h - 1; i++) &#123; F[i * w] = 0; F[i * w + w - 1] = 0; &#125; for (int i = 0; i &lt; w; i++) &#123; F[i] = 0; F[w * h - 1 - i] = 0; &#125; int nbd = 1; int lnbd = 1; for (int i = 1; i &lt; h - 1; i++) &#123; lnbd = 1; for (int j = 1; j &lt; w - 1; j++) &#123; int i2 = 0, j2 = 0; if (F[i * w + j] == 0) &#123; continue; &#125; //(a) If fij = 1 and fi, j-1 = 0, then decide that the pixel //(i, j) is the border following starting point of an outer //border, increment NBD, and (i2, j2) &lt;- (i, j - 1). if (F[i * w + j] == 1 &amp;&amp; F[i * w + (j - 1)] == 0) &#123; nbd++; i2 = i; j2 = j - 1; //(b) Else if fij &gt;= 1 and fi,j+1 = 0, then decide that the //pixel (i, j) is the border following starting point of a //hole border, increment NBD, (i2, j2) &lt;- (i, j + 1), and //LNBD + fij in case fij &gt; 1. &#125; else if (F[i * w + j] &gt;= 1 &amp;&amp; F[i * w + j + 1] == 0) &#123; nbd++; i2 = i; j2 = j + 1; if (F[i * w + j] &gt; 1) &#123; lnbd = F[i * w + j]; &#125; &#125; else &#123; //(c) Otherwise, go to (4). //(4) If fij != 1, then LNBD &lt;- |fij| and resume the raster //scan from pixel (i,j+1). The algorithm terminates when the //scan reaches the lower right corner of the picture if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; continue; &#125; //(2) Depending on the types of the newly found border //and the border with the sequential number LNBD //(i.e., the last border met on the current row), //decide the parent of the current border as shown in Table 1. // TABLE 1 // Decision Rule for the Parent Border of the Newly Found Border B // ---------------------------------------------------------------- // Type of border B' // \ with the sequential // \ number LNBD // Type of B \ Outer border Hole border // --------------------------------------------------------------- // Outer border The parent border The border B' // of the border B' // // Hole border The border B' The parent border // of the border B' // ---------------------------------------------------------------- Contour *B = [Contour new]; B.points = [NSMutableArray array]; [B.points addObject:[NSValue valueWithCGPoint:CGPointMake(j * 1.f / ow, i * 1.f / h)]]; B.isHole = (j2 == (j + 1)); B.idx = nbd; [contours addObject:B]; Contour *B0 = [Contour new]; for (int c = 0; c &lt; contours.count; c++) &#123; if (contours[c].idx == lnbd) &#123; B0 = contours[c]; break; &#125; &#125; if (B0.isHole) &#123; if (B.isHole) &#123; B.parentIdx = B0.parentIdx; &#125; else &#123; B.parentIdx = lnbd; &#125; &#125; else &#123; if (B.isHole) &#123; B.parentIdx = lnbd; &#125; else &#123; B.parentIdx = B0.parentIdx; &#125; &#125; //(3) From the starting point (i, j), follow the detected border: //this is done by the following substeps (3.1) through (3.5). //(3.1) Starting from (i2, j2), look around clockwise the pixels //in the neigh- borhood of (i, j) and tind a nonzero pixel. //Let (i1, j1) be the first found nonzero pixel. If no nonzero //pixel is found, assign -NBD to fij and go to (4). int i1j1[2] = &#123;-1, -1&#125;; cwNon0(F, w, h, i, j, i2, j2, 0, i1j1); if (i1j1[0] == -1 &amp;&amp; i1j1[1] == -1) &#123; F[i * w + j] = -nbd; //go to (4) if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; continue; &#125; int i1 = i1j1[0]; int j1 = i1j1[1]; // (3.2) (i2, j2) &lt;- (i1, j1) ad (i3,j3) &lt;- (i, j). i2 = i1; j2 = j1; int i3 = i; int j3 = j; while (true) &#123; //(3.3) Starting from the next elementof the pixel (i2, j2) //in the counterclock- wise order, examine counterclockwise //the pixels in the neighborhood of the current pixel (i3, j3) //to find a nonzero pixel and let the first one be (i4, j4). int i4j4[2] = &#123;-1, -1&#125;; ccwNon0(F, w, h, i3, j3, i2, j2, 1, i4j4); int i4 = i4j4[0]; int j4 = i4j4[1]; [contours[contours.count - 1].points addObject:[NSValue valueWithCGPoint:CGPointMake(j4 * 1.f / ow, i4 * 1.f / h)]]; //(a) If the pixel (i3, j3 + 1) is a O-pixel examined in the //substep (3.3) then fi3, j3 &lt;- -NBD. if (F[i3 * w + j3 + 1] == 0) &#123; F[i3 * w + j3] = (char) -nbd; //(b) If the pixel (i3, j3 + 1) is not a O-pixel examined //in the substep (3.3) and fi3,j3 = 1, then fi3,j3 &lt;- NBD. &#125; else if (F[i3 * w + j3] == 1) &#123; F[i3 * w + j3] = (char) nbd; &#125; else &#123; //(c) Otherwise, do not change fi3, j3. &#125; //(3.5) If (i4, j4) = (i, j) and (i3, j3) = (i1, j1) //(coming back to the starting point), then go to (4); if (i4 == i &amp;&amp; j4 == j &amp;&amp; i3 == i1 &amp;&amp; j3 == j1) &#123; if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; break; //otherwise, (i2, j2) + (i3, j3),(i3, j3) + (i4, j4), //and go back to (3.3). &#125; else &#123; i2 = i3; j2 = j3; i3 = i4; j3 = j4; &#125; &#125; &#125; &#125; free(F); ...&#125; SDF上面提到边缘检测找连续的边缘点只是为了解决虚线描边，其他的描边情况其实是用不到这些点的，但是这里也遇到问题了。 一开始的想法跟上面通过3x3范围取平均值，通过条件过滤来做的，kernel code大概是这样 12345678910111213141516171819static NSString *KernelString = @&quot;\kernel vec4 borderDraw(sampler image, sampler mask, sampler source, vec4 rgba, float midpoint, float width) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 sumColor = vec4(0.0);\ float radiu = 2 * width;\ for (float m = -radiu; m &lt;= radiu; m += 2) &#123;\ for (float n = -radiu; n &lt;= radiu; n += 2) &#123;\ vec4 rgba = sample(image, samplerTransform(image, vec2(uv.x + m, uv.y + n)));\ sumColor += rgba;\ &#125;\ &#125;\ float avg = sumColor.a / float(radiu * radiu / 2.0);\ if (color.a &lt; 1.0 &amp;&amp; (avg &gt; .05 &amp;&amp; avg &lt; 1.)) &#123;\ return rgba;\ &#125;\ return color;\&#125;&quot;; 这套方案做demo的时候，感觉效果还行，一点点毛疵，以为是条件判断不严谨，以为后续调整下可以解决，还有个严重的问题，就是这个方案计算量太大，图片分辨率1080左右，表现就有点卡，尤其是拖动滑竿调整，描边粗细 、间距 ，实时渲染有明显的卡顿，但是我们又不能降低图片质量，所以这个方案最终也就是停留在demo阶段了。 因为计算量太大，所以想办法降低像素计算量，SDF出场了，通过距离场，可以生成一张图，这张图可以告知像素边界信息，直接通过边界信息，省去了极大的计算量。 SDF ：signed distance filed 有向距离场sdf有两种方式，一种是循环（横向x纵向） 一种是双线性（横向+纵向），很明显前一种计算量远远大于后一种，联调下来第二种方案实际效果也是相当不错了。 这里还要考虑一个问题，因为描边是有粗细跟间距的，所以可以通过调整距离参数生成图，很好的解决了描边粗细跟间距问题的，SDF方案在虚线描边的case也是有用的，通过把SDF生成图拿去做边缘检测找连续点。 来看下SDF生成图的效果 抠图横向SDF结果接纵向SDF结果 贴一下Metal版本的SDF计算逻辑 横向SDF 12345678910111213141516171819202122232425262728293031323334353637383940extern "C" &#123; namespace coreimage &#123; constant float threshold = 0.5; float source(sampler image,float2 uv) &#123; return image.sample(image.transform(uv)).a - threshold; &#125; float4 sdfhor(sampler image,float width,destination dest) &#123; float D2 = width * 2.0 + 1.0; // 获取当前点坐标 float2 uv = dest.coord(); float s = sign(source(image,uv)); float d = 0.; for(int i= 0; i &lt; width; i++) &#123; d ++; float sp = sign(source(image,float2(uv.x + d, uv.y))); if(s * sp &lt; 0.) &#123; break; &#125; sp = sign(source(image,float2(uv.x - d, uv.y))); if(s * sp &lt; 0.) &#123; break; &#125; &#125; float sd = -s * d / D2 ; return float4(float3(sd),1.0); &#125; &#125;&#125; 纵向SDF 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758extern "C" &#123; namespace coreimage &#123; float sd(sampler image,float2 uv,float width) &#123; float D2 = float(width * 2 + 1); float x = image.sample(image.transform(uv)).x; return x * D2; &#125; float4 sdf(sampler image,float width,destination dest) &#123; // 获取当前点坐标 float2 uv = dest.coord(); float dx = sd(image,uv,width); float dMin = abs(dx); float dy = 0.0; for(int i= 0; i &lt; width; i++)&#123; dy += 1.0; float2 offset = float2(0.0, dy); float dx1 = sd(image,uv+offset,width); //sign switch if(dx1 * dx &lt; 0.)&#123; dMin = dy; break; &#125; dMin = min(dMin, length (float2(dx1, dy))); float dx2 = sd(image,uv-offset,width); //sign switch if(dx2 * dx &lt; 0.)&#123; dMin = dy; break; &#125; dMin = min(dMin, length (float2(dx2, dy))); if(dy &gt; dMin)break; &#125; float D2 = float(width * 2 + 1); dMin *= sign(dx); float d = dMin/D2; d = 1.0 - d; d = smoothstep(0.5 ,1.0, d); return float4(float3(d),1.0); &#125; &#125;&#125; border有了距离场，描边的工作就一下子简单多了。 目前实现的五种描边效果就是这样式的 项目中使用coreimage自定义kernel做的，当然也可以metal搞定 sdfsourceKernelString 对应上面第三个效果 12345678910111213141516171819202122232425262728293031static NSString *sdfsourceKernelString = @"\kernel vec4 borderDraw(sampler image, sampler sdf, sampler source, float d1, float d2) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 sdfcolor = sample(sdf, samplerTransform(sdf, uv));\ vec4 sourcecolor = sample(source, samplerTransform(source, uv));\ if (sdfcolor.x &gt;= d1 &amp;&amp; sdfcolor.x &lt;= d2) &#123;\ return sourcecolor;\ &#125;\ return color;\&#125;";static NSString *sdfKernelString = @"\kernel vec4 borderDraw(sampler image, sampler sdf, sampler source, vec4 rgba, vec2 offset, float d1, float d2) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 offsetColor = sample(image, samplerTransform(image, uv-offset));\ vec4 sdfcolor = sample(sdf, samplerTransform(sdf, uv));\ vec4 sourcecolor = sample(source, samplerTransform(source, uv));\ if(offset.x != 0.0 || offset.y != 0.0) &#123;\ if(color.a &lt; 0.5 &amp;&amp; offsetColor.a &gt; 0.5 ) &#123;\ return mix(rgba,color,color.a);\ &#125;\ &#125; else if (sdfcolor.x &gt;= d1 &amp;&amp; sdfcolor.x &lt;= d2) &#123;\ return mix(rgba,sourcecolor,offsetColor.a);\ &#125;\ return color;\&#125;"; 最后一个虚线描边就是常规的连接边缘点安排画布绘制，然后跟抠图做一个合并导出，就不展开说了。 Othter最后还有一些注意点，比如抠图图像是在边缘，则需要考虑下预留描边空间，判断是否有落在边缘，如果有则补充点空间。还有一个就是最小包围盒，抠图很可能只占据原图的一部分预期，为了展示效果，需要把抠图的最小包围盒找到，找这个最小包围盒，不需要那么精确，找出一个差不多的最小矩形框就行，项目上用的就是粗暴的像素遍历，找四个角的位置就可以了，通过最小包围盒，也能判断抠图是否靠近边缘。 The Last综上，关键的几个步骤基本都尝试了多种方式，分析比较得出最合适项目需求的技术方案， 最终从性能、体验等维度拿到相对不错的结果，单纯从描边功能上来说，对比修复工具也不输 O(∩_∩)O哈哈~ referencePContourSDF双线性SDF]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[camera orientation]]></title>
    <url>%2F2022%2F09%2F09%2Fcamera-orientation%2F</url>
    <content type="text"><![CDATA[camera手机相机录视频或者拍照，需要考虑设备的方向，主要在两方面 采集过程中，无论手机什么方向采集，采集画面都是正着看，不用侧头 录制/或者拍照后，视频/图片在正常手持设备的情况下正常显示，不用横着手机 正常手持一般都是竖着拿手机，所以就算横着拍摄，竖着拿手机也需要正常显示，不理解可以参考下系统相机。 所以这里就存在一个方向修正的问题。 device orientation苹果手机，如果设置里锁定了方向，是没办法通过 [UIDevice currentDevice].orientation这个方法拿到的，同样UIDeviceOrientationDidChangeNotification这个通知也获取不到。同理 [[UIApplication sharedApplication] statusBarOrientation]; 跟 **UIApplicationDidChangeStatusBarOrientationNotification** 也是不可用的。 可以通过 CMMotionManager 监听设备的方向 12345678910111213141516171819202122232425262728293031323334_motionManager = [[CMMotionManager alloc] init];_motionManager.deviceMotionUpdateInterval = 1/15.0;if (!_motionManager.deviceMotionAvailable) &#123; _motionManager = nil; return self;&#125;OBJC_WEAK(self)[_motionManager startDeviceMotionUpdatesToQueue:[NSOperationQueue currentQueue] withHandler: ^(CMDeviceMotion*motion, NSError *error)&#123; [weak_self performSelectorOnMainThread:@selector(handleDeviceMotion:) withObject:motion waitUntilDone:YES];&#125;];- (void)handleDeviceMotion:(CMDeviceMotion *)deviceMotion &#123; double x = deviceMotion.gravity.x; double y = deviceMotion.gravity.y; if (fabs(y) &gt;= fabs(x)) &#123; if (y &gt;= 0) &#123; _deviceOrientation = UIDeviceOrientationPortraitUpsideDown; _videoOrientation = AVCaptureVideoOrientationPortraitUpsideDown; &#125; else &#123; _deviceOrientation = UIDeviceOrientationPortrait; _videoOrientation = AVCaptureVideoOrientationPortrait; &#125; &#125; else &#123; if (x &gt;= 0) &#123; _deviceOrientation = UIDeviceOrientationLandscapeRight; _videoOrientation = AVCaptureVideoOrientationLandscapeRight; &#125; else &#123; _deviceOrientation = UIDeviceOrientationLandscapeLeft; _videoOrientation = AVCaptureVideoOrientationLandscapeLeft; &#125; &#125;&#125; capture项目中需要支持设置滤镜、特效等等，所以显示采集的图像需要自己渲染，通过官方的AVCaptureSession配置好采集流程，不管是竖着还是横着采集，获取到的pixelBuffer都是横向的，因为默认就是home键在右边采集图像，但是显示的视图一般都是竖向，所以这里要做一个90°的旋转处理,OPENGL 就是一个旋转矩阵。 123456789101112void main()&#123; // z mat4 rotationMatrix = mat4(cos(angle) , sin(angle) , 0.0, 0.0, -sin(angle) , cos(angle) , 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0); vec4 outPosition = rotationMatrix * position; gl_Position = outPosition; textureCoordinate = inputTextureCoordinate.xy;&#125; 如果只是解决采集画面显示，也可以通过设置connection.videoOrientation = [self currentVideoOrientation]; 来解决项目中没有用这个，因为后面还需要录制视频导出视频文件。 12345678910111213141516171819202122// 当前设备取向- (AVCaptureVideoOrientation)currentVideoOrientation&#123; AVCaptureVideoOrientation orientation; switch (self.motionManager.deviceOrientation) &#123; case UIDeviceOrientationPortrait: orientation = AVCaptureVideoOrientationPortrait; break; case UIDeviceOrientationLandscapeLeft: orientation = AVCaptureVideoOrientationLandscapeRight; break; case UIDeviceOrientationLandscapeRight: orientation = AVCaptureVideoOrientationLandscapeLeft; break; case UIDeviceOrientationPortraitUpsideDown: orientation = AVCaptureVideoOrientationPortraitUpsideDown; break; default: orientation = AVCaptureVideoOrientationPortrait; break; &#125; return orientation;&#125; Record录制过程通过AVAssetWriterInput *writerInput = [AVAssetWriterInput assetWriterInputWithMediaType:obj outputSettings:options]; 和[self.mPixelBufferAdaptor appendPixelBuffer:pixelBuffer withPresentationTime:ts] 来积累pixelBuffer， 为了保障导出的视频可以正常显示，需要记录视频的方向，通过设置writerInput.transform[ = self transformFromCurrentVideoOrientationToOrientation:AVCaptureVideoOrientationPortrait],系统自动帮我们做转换 1234567891011121314151617181920212223242526272829303132// 旋转视频方向函数实现- (CGAffineTransform)transformFromCurrentVideoOrientationToOrientation:(AVCaptureVideoOrientation)orientation &#123; CGFloat orientationAngleOffset = [self angleOffsetFromPortraitOrientationToOrientation:orientation]; CGFloat videoOrientationAngleOffset = [self angleOffsetFromPortraitOrientationToOrientation:self.currentOrientation]; CGFloat angleOffset; if (self.position == AVCaptureDevicePositionBack) &#123; angleOffset = videoOrientationAngleOffset - orientationAngleOffset + M_PI_2; &#125; else &#123; angleOffset = orientationAngleOffset - videoOrientationAngleOffset + M_PI_2; &#125; CGAffineTransform transform = CGAffineTransformMakeRotation(angleOffset); return transform;&#125;- (CGFloat)angleOffsetFromPortraitOrientationToOrientation:(AVCaptureVideoOrientation)orientation &#123; CGFloat angle = 0.0; switch (orientation) &#123; case AVCaptureVideoOrientationPortrait: angle = 0.0; break; case AVCaptureVideoOrientationPortraitUpsideDown: angle = M_PI; break; case AVCaptureVideoOrientationLandscapeRight: angle = -M_PI_2; break; case AVCaptureVideoOrientationLandscapeLeft: angle = M_PI_2; break; &#125; return angle;&#125; 主要就是通过录制视频时候的设备方向跟显示方向计算一下transform Pic拍照生成图片是同样的道理，直接给出方法，主要就是图片方向的旋转 123456789101112131415161718- (UIImageOrientation)getImageRotationOrientationFromCaptureVideoOrientation:(AVCaptureVideoOrientation)orientation &#123; UIImageOrientation imageOrientation; switch (orientation) &#123; case AVCaptureVideoOrientationPortrait: imageOrientation = UIImageOrientationRight; break; case AVCaptureVideoOrientationPortraitUpsideDown: imageOrientation = UIImageOrientationLeft; break; case AVCaptureVideoOrientationLandscapeRight: imageOrientation = UIImageOrientationUp; break; case AVCaptureVideoOrientationLandscapeLeft: imageOrientation = UIImageOrientationDown; break; &#125; return imageOrientation;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455+ (UIImage *)image:(UIImage *)image rotation:(UIImageOrientation)orientation &#123; long double rotate = 0.0; CGRect rect; float translateX = 0; float translateY = 0; float scaleX = 1.0; float scaleY = 1.0; switch (orientation) &#123; case UIImageOrientationLeft: rotate = M_PI_2; rect = CGRectMake(0, 0, image.size.height, image.size.width); translateX = 0; translateY = -rect.size.width; scaleY = rect.size.width/rect.size.height; scaleX = rect.size.height/rect.size.width; break; case UIImageOrientationRight: rotate = -M_PI_2; rect = CGRectMake(0, 0, image.size.height, image.size.width); translateX = -rect.size.height; translateY = 0; scaleY = rect.size.width/rect.size.height; scaleX = rect.size.height/rect.size.width; break; case UIImageOrientationDown: rotate = M_PI; rect = CGRectMake(0, 0, image.size.width, image.size.height); translateX = -rect.size.width; translateY = -rect.size.height; break; default: rotate = 0.0; rect = CGRectMake(0, 0, image.size.width, image.size.height); translateX = 0; translateY = 0; break; &#125; UIGraphicsBeginImageContext(rect.size); CGContextRef context = UIGraphicsGetCurrentContext(); //做CTM变换 CGContextTranslateCTM(context, 0.0, rect.size.height); CGContextScaleCTM(context, 1.0, -1.0); CGContextRotateCTM(context, rotate); CGContextTranslateCTM(context, translateX, translateY); CGContextScaleCTM(context, scaleX, scaleY); //绘制图片 CGContextDrawImage(context, CGRectMake(0, 0, rect.size.width, rect.size.height), image.CGImage); UIImage *newPic = UIGraphicsGetImageFromCurrentImageContext(); return newPic;&#125;]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>camera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[__has_include]]></title>
    <url>%2F2022%2F09%2F06%2Fhas-include%2F</url>
    <content type="text"><![CDATA[VS在搞音视频的过程中，早期为了快速实现功能，会用到一些开源库，比如FFMpeg、OpenCV、LibYUV 等，这些大名鼎鼎的开源库能够解决音视频领域的很多问题，编解码、边缘检测、RGB-YUV格式转换等等。 这几个开源库还都是C/C++的，完美解决跨平台的问题，所以早期可以快速上线，但也会带来增加包大小的问题，这是最明显的，另外开源方案也并非没有BUG。对开源库做裁剪减少大小/提issue等待官方处理、自己处理BUG等这些成本相对也不小。 另一个选择就是使用其他的替代方案：比如原生的技术实现、解决指定问题独立模块算法、DIY \ 编解码 边缘检测 格式转换 开源方案 FFmpeg OpenCV LibYUV 替代方案 VideoToolBox/MediaCodec 边界跟踪算法Suzuki85 DIY 说明 软解-&gt; 硬解 边界跟踪算法Suzuki85 就是 OpenCV 内部的一种边缘检测方案 自己处理像素数据 后期通过替代方案可以解决上面提到的问题，主要是包大小问题。另外也可以自己动手加深了解开源的技术方案 __has_include通过替代方案可以移除开源库，但是接入开源方案实现功能的相关代码就没必要移除了。所以可以通过条件编译做一个区分。正好__has_include这个宏可以满足要求 描述此宏传入一个你想引入文件的名称作为参数，如果该文件能够被引入则返回1，否则返回0。 用法 12345#if __has_include(&lt;XXX/XXX.h&gt;)#import &lt;XXX/XXX.h&gt;#else#import "YYY.h"#endif 项目中的实际用法： 123456789#if __cplusplus &amp;&amp; __has_include(&lt;Libyuv/libyuv.h&gt;)#include &lt;Libyuv/libyuv.h&gt;#endif#if __cplusplus &amp;&amp; __has_include(&lt;Libyuv/libyuv.h&gt;)...#else...#endif 12345#if __has_include(&lt;opencv2/imgcodecs/ios.h&gt;) contourArr = [OpenCVWrapper findContours:image];#else contourArr = [PContour findContours:image threshold:threshold];#endif 12345678910111213141516#if __cplusplus &amp;&amp; __has_include(&lt;ffmpeg/avformat.h&gt;)extern "C" &#123;#include "ffmpeg/timestamp.h"#include "ffmpeg/avformat.h"#include "ffmpeg/bsf.h"#include "ffmpeg/swscale.h"#include "ffmpeg/swresample.h"#include "ffmpeg/avformat.h"#include "ffmpeg/imgutils.h"#include "ffmpeg/samplefmt.h"&#125;#endif ...#endif]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RGB2YUV]]></title>
    <url>%2F2022%2F09%2F05%2FRGB2YUV%2F</url>
    <content type="text"><![CDATA[RGB2YUV处理音视频，对YUV RGB肯定不陌生，这里记录下经过OpenGL处理后的 RGB 格式的pixelBuffer 转成 YUV 格式导出视频。 DIY自己处理矩阵的计算，YUV的存储格式，可以加深对YUV的理解， 类似的反过来处理，或者处理其他格式的YUV 422 444等，也是一样的。所以还是有必要了解一下，DO IT YOURSELF. 重点是注意内存对齐，YUV420采样存储方式就好了，其他没什么复杂的。 转换后 如果出现这种像素错位的情况，一般是内存对齐 导致的补位没有考虑到，参考下代码里面的 stride字段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109CFDictionaryRef CreateCFDictionary(CFTypeRef* keys, CFTypeRef* values, size_t size) &#123; return CFDictionaryCreate(kCFAllocatorDefault, keys, values, size, &amp;kCFTypeDictionaryKeyCallBacks, &amp;kCFTypeDictionaryValueCallBacks); &#125;static void bt709_rgb2yuv8bit_TV(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.183 * R + 0.614 * G + 0.062 * B + 16; U = -0.101 * R - 0.339 * G + 0.439 * B + 128; V = 0.439 * R - 0.399 * G - 0.040 * B + 128; &#125;CVPixelBufferRef RGB2YCbCr8Bit(CVPixelBufferRef pixelBuffer) &#123; CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); int stride = (int) CVPixelBufferGetBytesPerRow(pixelBuffer) / 4; OSType pixelFormat = kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 1; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey, &#125;; CFDictionaryRef io_surface_value = CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;; CFDictionaryRef attributes = CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 0); size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 1); int plane_h1 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 0); int plane_h2 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 1); uint8_t *y = (uint8_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 0); memset(y, 0x80, plane_h1 * y_stride); uint8_t *uv = (uint8_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 1); memset(uv, 0x80, plane_h2 * uv_stride); int y_bufferSize = w * h; int uv_bufferSize = w * h / 4; uint8_t *y_planeData = (uint8_t *) malloc(y_bufferSize * sizeof(uint8_t)); uint8_t *u_planeData = (uint8_t *) malloc(uv_bufferSize * sizeof(uint8_t)); uint8_t *v_planeData = (uint8_t *) malloc(uv_bufferSize * sizeof(uint8_t)); int u_offset = 0; int v_offset = 0; uint8_t R, G, B; uint8_t Y, U, V; for (int i = 0; i &lt; h; i ++) &#123; for (int j = 0; j &lt; w; j ++) &#123; int offset = i * stride + j; B = baseAddress[offset * 4]; G = baseAddress[offset * 4 + 1]; R = baseAddress[offset * 4 + 2]; bt709_rgb2yuv8bit_TV(R, G, B, Y, U, V); y_planeData[i * w + j] = Y; //隔行扫描 偶数行的偶数列取U 奇数行的偶数列取V if (j % 2 == 0) &#123; (i % 2 == 0) ? u_planeData[u_offset++] = U : v_planeData[v_offset++] = V; &#125; &#125; &#125; for (int i = 0; i &lt; plane_h1; i ++) &#123; memcpy(y + i * y_stride, y_planeData + i * w, w); if (i &lt; plane_h2) &#123; for (int j = 0 ; j &lt; w ; j+=2) &#123; //NV12 和 NV21 格式都属于 YUV420SP 类型。它也是先存储了 Y 分量，但接下来并不是再存储所有的 U 或者 V 分量，而是把 UV 分量交替连续存储。 //NV12 是 IOS 中有的模式，它的存储顺序是先存 Y 分量，再 UV 进行交替存储。 memcpy(uv + i * y_stride + j, u_planeData + i * w/2 + j/2, 1); memcpy(uv + i * y_stride + j + 1, v_planeData + i * w/2 + j/2, 1); &#125; &#125; &#125; free(y_planeData); free(u_planeData); free(v_planeData); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); return pixelBufferCopy; &#125; LibYUV看过了上一种方式，LibYUV就更好理解了，这里主要通过pod 依赖下 LibYUV-ios， 就不自己编译了。 pod &#39;Libyuv&#39;,&#39;1703&#39; LibYUV 不能直接RGB转成NV12 ,需要通过I420过度下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); size_t bgraStride = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer,0); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); OSType pixelFormat = kCVPixelFormatType_420YpCbCr8Planar; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 1; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey &#125;; CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;; CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); unsigned char* y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,0); unsigned char* u = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,1); unsigned char* v = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,2); int32_t width = (int32_t)CVPixelBufferGetWidth(pixelBufferCopy); int32_t height = (int32_t)CVPixelBufferGetHeight(pixelBufferCopy); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,0); size_t u_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,1); size_t v_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,2); libyuv::ARGBToI420(baseAddress, (int)bgraStride, y, (int)y_stride, u, (int)u_stride, v, (int)v_stride, width, height); CVPixelBufferRef pixelBufferNV12 = NULL; const size_t size = 1; CFTypeRef _keys[size] = &#123; kCVPixelBufferIOSurfacePropertiesKey &#125;; CFDictionaryRef _io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef _values[size] = &#123;_io_surface_value&#125;; CFDictionaryRef _attributes = vtc::CreateCFDictionary(_keys, _values, size); CVReturn _status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, _attributes, &amp;pixelBufferNV12); if (_status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (_attributes) &#123; CFRelease(_attributes); _attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferNV12, 0); unsigned char* _y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,0); unsigned char* _uv = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,1); size_t _y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 0); size_t _uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 1); int32_t _width = (int32_t)CVPixelBufferGetWidth(pixelBufferNV12); int32_t _height = (int32_t)CVPixelBufferGetHeight(pixelBufferNV12); libyuv::I420ToNV12(y, (int)y_stride, u, (int)u_stride, v, (int)v_stride, _y, (int)_y_stride, _uv, (int)_uv_stride, _width, _height); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); CVPixelBufferUnlockBaseAddress(pixelBufferNV12, 0); CVPixelBufferRelease(pixelBufferCopy); RGB-&gt;NV21更新下:上次使用libyuv的时候 看漏了，其实有argb转nv21的 123456789101112131415161718192021222324252627282930313233343536373839404142CVPixelBufferLockBaseAddress(pixelBuffer, 0);uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer);size_t bgraStride = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer,0);int w = (int) CVPixelBufferGetWidth(pixelBuffer);int h = (int) CVPixelBufferGetHeight(pixelBuffer);CVPixelBufferRef pixelBufferNV12 = NULL;const size_t attributes_size = 1;CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey&#125;;CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0);CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;;CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size);CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, attributes, &amp;pixelBufferNV12);if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr;&#125;if (attributes) &#123; CFRelease(attributes); attributes = nullptr;&#125;CVPixelBufferLockBaseAddress(pixelBufferNV12, 0);unsigned char* y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,0);unsigned char* uv = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,1);size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 0);size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 1);int32_t width = (int32_t)CVPixelBufferGetWidth(pixelBufferNV12);int32_t height = (int32_t)CVPixelBufferGetHeight(pixelBufferNV12);// ARGB-&gt;NV21libyuv::ARGBToNV12(baseAddress, (int)bgraStride, y, (int)y_stride, uv, (int)uv_stride, width, height)CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);CVPixelBufferUnlockBaseAddress(pixelBufferNV12, 0);return pixelBufferNV12;]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>YUV</tag>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter Display P3]]></title>
    <url>%2F2022%2F08%2F31%2Fflutter-Display-P3%2F</url>
    <content type="text"><![CDATA[Display P3Display P3 是苹果手机相机使用的一种色域，查看图片信息可以看到， 然而在flutter中 渲染 Display P3格式的图片 bitmap颜色失真,不了解色域的参考下前面HDR相关的文章 失真效果： 修复效果： 原因就是Flutter 直接把Display P3社区的当做sRGB色域的图像处理了，而没有做色域转换 FlutterFlutter 跟 Native 图片打通，常见有两种方式：bitmap传递 &amp;&amp; 外接纹理，这边文章针对的是前者，外接纹理也会有这种问题的，无非就是pixelBuffer 转纹理，pixelBuffer一样也是有色域问题的，解决方案可以参考 Flutter HDR,原理是一样的。 针对图片的bitmap做色域转换，方案有很多，这里列出常见的两种： ImageIO123456789101112131415161718192021222324252627282930CGImageSourceRef src = CGImageSourceCreateWithData((__bridge CFDataRef) imageData, NULL); NSUInteger frameCount = CGImageSourceGetCount(src); if (frameCount &gt; 0) &#123; NSDictionary *options = @&#123;(__bridge NSString *)kCGImageSourceShouldCache : @YES, (__bridge NSString *)kCGImageSourceShouldCacheImmediately : @NO &#125;; NSDictionary *props = (NSDictionary *) CFBridgingRelease(CGImageSourceCopyPropertiesAtIndex(src, (size_t) 0, (__bridge CFDictionaryRef)options)); NSString *profileName = [props objectForKey:(NSString *) kCGImagePropertyProfileName]; if ([profileName isEqualToString:@"Display P3"]) &#123; NSMutableData *data = [NSMutableData data]; CGImageDestinationRef destRef = CGImageDestinationCreateWithData((__bridge CFMutableDataRef)data, kUTTypePNG, 1, NULL); NSMutableDictionary *properties = [NSMutableDictionary dictionary]; properties[(__bridge NSString *)kCGImageDestinationLossyCompressionQuality] = @(1); properties[(__bridge NSString *)kCGImageDestinationEmbedThumbnail] = @(0); properties[(__bridge NSString *)kCGImagePropertyNamedColorSpace] = (__bridge id _Nullable)(kCGColorSpaceSRGB); properties[(__bridge NSString *)kCGImageDestinationOptimizeColorForSharing] = @(YES); CGImageDestinationAddImageFromSource(destRef, src, 0, (__bridge CFDictionaryRef)properties); CGImageDestinationFinalize(destRef); CFRelease(destRef); return data; &#125; &#125; return imageData; 核心就是这个属性，很好理解吧 12345/* Create an image using a colorspace, that has is compatible with older devices * The value should be kCFBooleanTrue or kCFBooleanFalse * Defaults to kCFBooleanFalse = don&apos;t do any color conversion */IMAGEIO_EXTERN const CFStringRef kCGImageDestinationOptimizeColorForSharing IMAGEIO_AVAILABLE_STARTING(10.12, 9.3); 重新Render一张图1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980CGImageSourceRef src = CGImageSourceCreateWithData((__bridge CFDataRef) imageData, NULL); NSUInteger frameCount = CGImageSourceGetCount(src); if (frameCount &gt; 0) &#123; NSDictionary *frameProperties = (NSDictionary *) CFBridgingRelease(CGImageSourceCopyPropertiesAtIndex(src, (size_t) 0, NULL)); NSString *profileName = [frameProperties objectForKey:(NSString *) kCGImagePropertyProfileName]; if ([profileName isEqualToString:@"Display P3"]) &#123; CGImageRef imageRef = CGImageSourceCreateImageAtIndex(src, (size_t) 0, NULL); CIImage *image = [CIImage imageWithCGImage:imageRef]; CIContext *context = [[CIContext alloc] init]; float w = image.extent.size.width; float h = image.extent.size.height; unsigned char *bitmap = malloc(w * h * 4); CIRenderDestination *destination = [[CIRenderDestination alloc] initWithBitmapData:bitmap width:w height:h bytesPerRow:w * 4 format:kCIFormatBGRA8]; NSError *error = nil; [context startTaskToRender:image toDestination:destination error:&amp;error]; if (error) &#123; CFRelease(src); return imageData; &#125; CFRelease(src); UIImage *newImage = [FlutterImagePlugin imageFromBRGABytes:bitmap imageSize:image.extent.size]; free(bitmap); CGImageRelease(imageRef); if (newImage == nil) &#123; return imageData; &#125; return UIImagePNGRepresentation(newImage); &#125; &#125;+ (UIImage *)imageFromBRGABytes:(unsigned char *)imageBytes imageSize:(CGSize)imageSize &#123; CGImageRef imageRef = [self imageRefFromBGRABytes:imageBytes imageSize:imageSize]; if (imageRef == NULL) &#123; return nil; &#125; UIImage *image = [UIImage imageWithCGImage:imageRef]; CGImageRelease(imageRef); return image;&#125;+ (CGImageRef)imageRefFromBGRABytes:(unsigned char *)imageBytes imageSize:(CGSize)imageSize &#123; CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB(); CGContextRef context = CGBitmapContextCreate(imageBytes, imageSize.width, imageSize.height, 8, imageSize.width * 4, colorSpaceSDImage, kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst); if (context == NULL) &#123; return NULL; &#125; CGImageRef imageRef = CGBitmapContextCreateImage(context); if (imageRef == NULL) &#123; CGContextRelease(context); return NULL; &#125; CGContextRelease(context); return imageRef;&#125; 从代码量跟性能考量，无脑选前者 😝]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter-hdr]]></title>
    <url>%2F2022%2F08%2F24%2Fflutter-hdr%2F</url>
    <content type="text"><![CDATA[flutterflutter 播放 HDR 视频，色彩跟亮度都有问题 ，github 也有反馈这个issue Video Player HDR Problem 跟原生的对比可以很明显看到差距，官方看起来好像也不重视这个问题 😕 关于 HDR 格式，可以看下前面的文章 HDR笔记 这里记录下Flutter video player plugin 中处理视频色彩的方法，亮度提升需要通过硬件激活，Flutter中好像没法处理。 颜色的处理核心就是做了一个HDR-&gt;SDR的tonemap,恰好CIImage中提供了这样的Filter，处理就方便多了，看过HDR笔记 这篇文章的话，应该也可以自己通过 EOTF + 色域映射 来处理。暂时不知道 CIImage中是怎么处理的，猜测是差不多的。 12345678910111213141516171819202122232425262728293031323334353637383940- (CVPixelBufferRef)pixelBufferFormCIImage:(CIImage *)image &#123; NSDictionary *options = [NSDictionary dictionaryWithObjectsAndKeys: @&#123;&#125;, kCVPixelBufferIOSurfacePropertiesKey, @YES, kCVPixelBufferCGImageCompatibilityKey, @YES, kCVPixelBufferCGBitmapContextCompatibilityKey, nil]; CVPixelBufferRef pixelBufferCopy = NULL; CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, image.extent.size.width, image.extent.size.height, kCVPixelFormatType_32BGRA, (__bridge CFDictionaryRef) options, &amp;pixelBufferCopy); if (status == kCVReturnSuccess) &#123; CIRenderDestination *destination = [[CIRenderDestination alloc] initWithPixelBuffer:pixelBufferCopy]; [self.mContext startTaskToRender:image toDestination:destination error:nil]; &#125; return pixelBufferCopy;&#125;- (CVPixelBufferRef)copyPixelBuffer &#123; CMTime outputItemTime = [_videoOutput itemTimeForHostTime:CACurrentMediaTime()]; if ([_videoOutput hasNewPixelBufferForItemTime:outputItemTime]) &#123; CVPixelBufferRef p = [_videoOutput copyPixelBufferForItemTime:outputItemTime itemTimeForDisplay:NULL]; CFTypeRef colorPrimaries = CVBufferGetAttachment(p, kCVImageBufferTransferFunctionKey, NULL); if (colorPrimaries &amp;&amp; CFEqual(colorPrimaries, kCVImageBufferTransferFunction_ITU_R_2100_HLG)) &#123; if (@available(iOS 14.1, *)) &#123; CIImage *image = [CIImage imageWithCVPixelBuffer:p options:@&#123;kCIImageToneMapHDRtoSDR : @(YES)&#125;]; CVPixelBufferRef newP = [self pixelBufferFormCIImage:image]; CVPixelBufferRelease(p); return newP; &#125; &#125; return p; &#125; else &#123; return NULL; &#125;&#125;]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>HDR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDR笔记]]></title>
    <url>%2F2022%2F08%2F18%2FHDR%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Noun explanationITU：国际电信联盟 International Telecommunication UnionITU-R：国际电信联盟无线电通信部门 ITU Radiocommunication Sector CIE :国际照明协会 （英文：International Commission on Illumination ，法文：Commission internationale de l&#39;éclairage ，采用法文缩写：CIE ） SMPTE：电影电视工程师协会 Society of Motion Picture and Television Engineers Hue：色调 色彩 色相Chroma：色调饱和度 浓度Luminance：亮度 明度 HDR ConceptHDR:High Dynamic Range 字面上是动态范围，一般指亮度上可以表达更大的亮度范围，呈现更大的亮度对比度。但是实际实际上HDR的技术和标准涉及色彩相关的一组属性的改善，可以带来更多的颜色、更大的亮度对比度、更高精度的量化。 OETF/EOTF: Optical-Electro/Electro-Optical Transfer Function 光电/电光转换函数 人对亮度的感知是非线性的，对暗部细节敏感，对亮部细节不敏感，利用这个特点设计了非线性的光电转换和电光转换的函数。这样的处理不仅可以节省带宽，也可以基本满足用户体验需求。光电转换的时候做了特殊的非线性编码，为暗部细节分配更多的码率，亮部细节进行了压缩或者截断减少码率的分配。电光转换进行显示还原的时候，通过应用一个逆的非线性变化，还原出线性光。 Info通过mediainfo 查看一个HDR视频信息 Video ID 1 Format HEVC Format/Info High Efficiency Video Coding Format profile Main 10@L5@High Codec ID hvc1 Codec ID/Info High Efficiency Video Coding Duration 14 s 0 ms Bit rate 123 kb/s Width 3 840 pixels Height 2 160 pixels Display aspect ratio 16:9 Frame rate mode Constant Frame rate 30.000 FPS Color space YUV Chroma subsampling 4:2:0 Bit depth 10 bits Scan type Progressive Bits/(PixelFrame)* 0.000 Stream size 211 KiB (98%) Title Core Media Video Encoded date UTC 2020-06-14 21:45:40 Tagged date UTC 2020-06-14 21:47:33 Color range Limited Color primaries BT.2020 Transfer characteristics HLG Matrix coefficients BT.2020 non-constant Codec configuration box hvcC 下面几个参数属于元数据 ，可能没有 后文会讲到 Video Mastering display color primaries Display P3 / R: x=0.677980 y=0.321980, G: x=0.245000 y=0.703000, B: x=0.137980 y=0.052000, White point: x=0.312680 y=0.328980 Mastering display luminance min: 0.0001 cd/m2, max: 1000 cd/m2 Maximum Content Light Level 1000 cd/m2 Maximum Frame-Average Light Level 400 cd/m2 重点关注 ： Color range ：色彩范围 Color primaries ：色彩原色 Transfer characteristics ：传输特性 Matrix coefficients ：矩阵系数 元数据字段 Mastering display color primaries Mastering display luminance Maximum Content Light Level Maximum Frame-Average Light Level 部分参数可以通过ffprobe查看对应的选项 ffprobe -h &gt;&gt; ffprobe.txt Color range色彩范围主要是两个： Full range （PC range ） Video range（limited range，tv range） Full Range 就是我们所熟悉的 [0, 255]，而在 Limited Range 中，Y’ 的值被限制在 [16, 235]，Cb 和 Cr 的值被限制在 [16, 240] （针对8bit的）。 HDR一个重要属性就是量化精度。 SDR技术使用8bit进行颜色的表达，而HDR使用10bit/12bit进行颜色的表示，从而减少了8bit容易出现的人为条带效应。 ios 中的精度 &amp; 色彩范围 ： 123kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange = &apos;420v&apos;, /* Bi-Planar Component Y&apos;CbCr 8-bit 4:2:0, video-range (luma=[16,235] chroma=[16,240]). baseAddr points to a big-endian CVPlanarPixelBufferInfo_YCbCrBiPlanar struct */kCVPixelFormatType_420YpCbCr10BiPlanarVideoRange = &apos;x420&apos;, /* 2 plane YCbCr10 4:2:0, each 10 bits in the MSBs of 16bits, video-range (luma=[64,940] chroma=[64,960]) */ 123kCVPixelFormatType_420YpCbCr8BiPlanarFullRange = &apos;420f&apos;, /* Bi-Planar Component Y&apos;CbCr 8-bit 4:2:0, full-range (luma=[0,255] chroma=[1,255]). baseAddr points to a big-endian CVPlanarPixelBufferInfo_YCbCrBiPlanar struct */ kCVPixelFormatType_420YpCbCr10BiPlanarFullRange = &apos;xf20&apos;, /* 2 plane YCbCr10 4:2:0, each 10 bits in the MSBs of 16bits, full-range (Y range 0-1023) */ 关于为什么要将YUV量化为tv range 16-235 ？ 以下是维基百科摘抄的一段， 意思是tv range是为了解决滤波（模数转换）后的过冲现象， Y′ values are conventionally shifted and scaled to the range [16, 235] (referred to as studio swing or “TV levels”) rather than using the full range of [0, 255] (referred to as full swing or “PC levels”). This practice was standardized in SMPTE-125M in order to accommodate signal overshoots (“ringing”) due to filtering. The value 235 accommodates a maximal black-to-white overshoot of 255 − 235 = 20, or 20 / (235 − 16) = 9.1%, which is slightly larger than the theoretical maximal overshoot (Gibbs phenomenon) of about 8.9% of the maximal step. The toe-room is smaller, allowing only 16 / 219 = 7.3% overshoot, which is less than the theoretical maximal overshoot of 8.9%. This is why 16 is added to Y′ and why the Y′ coefficients in the basic transform sum to 220 instead of 255.^[9]^ U and V values, which may be positive or negative, are summed with 128 to make them always positive, giving a studio range of 16–240 for U and V. (These ranges are important in video editing and production, since using the wrong range will result either in an image with “clipped” blacks and whites, or a low-contrast image.) Color primaries一般理解为色域，色域指可以显示的所有颜色的范围，常见的有Rec.709（全高清广播标准）、Rec.2020（4K/8K广播标准BT.2020）、Adobe RGB、P3等。 bt709 unknown reserved bt470m bt470bg smpte170m smpte240m film bt2020 smpte428 smpte431 smpte432 下图显示了人眼能够感知的所有RGB值的范围。三角形表示色域：三角形越大，可以显示的颜色越多。 这张马蹄形的图上面可以看到HDR使用的色域是BT2020， SDR使用的是BT709，可以明显看到HDR的色域大于SDR。 理解颜色空间： 颜色空间 由 颜色模型 跟 色域 共同定义。颜色模型的概念为：一种抽象数学模型，通过一组数字来描述颜色（例如RGB使用三元组、CMYK使用四元组）例如Adobe RGB和sRGB都基于RGB颜色模型，但它们是两个不同的颜色空间，因为色域不一样 Color Transfer描述光电转换过程的视频属性也叫颜色传输函数Color Transfer 就是上面表格里面的 Transfer characteristics 常见的hdr转换曲线为HLG和PQ，其中，smpte2084为PQ曲线（感知量化），arib-std-b67为HLG曲线（混合对数伽玛） bt709 unknown reserved bt470m bt470bg smpte170m smpte240m linear log100 log316 iec61966-2-4 bt1361e iec61966-2-1 bt2020-10 bt2020-12 smpte2084 smpte428 arib-std-b67 传统的SDR视频使用的BT709的光电转换函数，对高亮部分进行了截断，可以表达的亮度动态范围有限，最大亮度只有100nit。而HDR视频，增加了高亮部分细节的表达，很大的扩展亮度的动态范围。 不同HDR的设计初衷不同，其中PQ的设计更接近人眼的特点，亮度表达更准确，可以表示高达10000nit的亮度。而HLG的设计考虑了老设备的兼容性，和传统bt709的传输函数有部分是重合的，天然的对老设备具有一定兼容性 MetadataHDR元数据分为两种，静态元数据 和动态元数据 ； 使用PQ曲线的HDR10是采用静态元数据的，但是杜比公司提出来的杜比视界和三星的HDR10+，尽管使用了PQ曲线，但是他们使用的是动态元数据，HLG没有元数据。 其中 DolbyVision 等价于SMPTE ST 2094-10, HDR10+ 等价于 SMPTE ST 2094-40 静态元数据规定了整个片子像素级别最大亮度上限，在ST 2086中有标准化的定义。静态元数据的缺点是必须做全局的色调映射，没有足够的调节空间，兼容性不好。 动态元数据可以很好地解决这个问题。动态元数据主要有两个方面的作用：与静态元数据相比，它可以在每一个场景或者每一帧画面，给调色师一个发挥的空间，以展现更丰富的细节；另一个方面，通过动态元数据，在目标显示亮度上做色调映射，可以最大程度在目标显示器上呈现作者的创作意图。 最大内容亮度（MaxCLL）：整个视频流中最亮像素的亮度。 最大帧平均亮度（MaxFALL）：整个视频流中最亮帧的平均亮度 另外解释一下多出现的几个参数:progressive,SAR,DAR. progressive,其实就是扫描方式,逐行扫描.另外的一种方式就是隔行扫描:interlaced.我们平时所谓的1080p,这个p就是progressive,表示的是1080尺寸的逐行扫描视频.DAR - display aspect ratio就是视频播放时，我们看到的图像宽高的比例，缩放视频也要按这个比例来，否则会使图像看起来被压扁或者拉长了似的。 SAR - storage aspect ratio就是对图像采集时，横向采集与纵向采集构成的点阵，横向点数与纵向点数的比值。比如VGA图像640/480 = 4:3，D-1 PAL图像720/576 = 5:4 PAR - pixel aspect ratio大多数情况为1:1,就是一个正方形像素，否则为长方形像素这三者的关系PAR x SAR = DAR或者PAR = DAR/SAR Tone Mapping色调映射的目的是使高动态范围HDR图像能够适应低动态范围LDR显示器。 色调映射算法的目的在于将HDR图像的亮度进行压缩，进而映射到LDR显示设备的显示范围之内，同时，在映射的过程中要尽量保持原HDR图像的细节与颜色等重要信息。 所以色调映射算法需要具有两方面的性质： 能够将图像亮度进行压缩。 能够保持图像细节与颜色。 EOTF/OETF HLG 1234567891011121314151617181920212223242526272829float ARIB_B67_A = 0.17883277;float ARIB_B67_B = 0.28466892;float ARIB_B67_C = 0.55991073;highp float arib_b67_inverse_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt;= (1.0/2.0)) x = (x * x) * (1.0 / 3.0); else x = (exp((x - ARIB_B67_C) / ARIB_B67_A) + ARIB_B67_B) / 12.0; return x;&#125;highp float arib_b67_ootf(highp float x)&#123; return x &lt; 0.0 ? x : pow(x, 1.2);&#125;highp float arib_b67_eotf(highp float x)&#123; return arib_b67_ootf(arib_b67_inverse_oetf(x));&#125;highp float arib_b67_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt;= (1.0 / 12.0)) x = sqrt(3.0 * x); else x = ARIB_B67_A * log(12.0 * x - ARIB_B67_B) + ARIB_B67_C; return x;&#125; PQ 12345678910111213highp float ST2084_M1 = 0.1593017578125;const float ST2084_M2 = 78.84375;const float ST2084_C1 = 0.8359375;const float ST2084_C2 = 18.8515625;const float ST2084_C3 = 18.6875;highp float FLT_MIN = 1.17549435082228750797e-38;highp float st_2084_eotf(highp float x)&#123; highp float xpow = pow(x, float(1.0 / ST2084_M2)); highp float num = max(xpow - ST2084_C1, 0.0); highp float den = max(ST2084_C2 - ST2084_C3 * xpow, FLT_MIN); return pow(num/den, 1.0 / ST2084_M1);&#125; BT.709 1234567891011const float REC709_ALPHA = 1.09929682680944;const float REC709_BETA = 0.018053968510807;highp float rec_709_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt; REC709_BETA ) x = x * 4.5; else x = REC709_ALPHA * pow(x, 0.45) - (REC709_ALPHA - 1.0); return x;&#125; bt2020 -&gt; bt709 1231.6605, -0.5876, -0.0728-0.1246, 1.1329, -0.0083-0.0182, -0.1006, 1.1187 bt709 -&gt; bt2020 1230.6274, 0.3293, 0.04330.0691, 0.9195, 0.01140.0164, 0.0880, 0.8956 Matrix coefficientsYCbCr-&gt;RGB Video Range 1234567891011121314151617181920// BT.601, which is the standard for SDTV.static const GLfloat kColorConversion601[] = &#123; 1.164, 1.164, 1.164, 0.0, -0.392, 2.017, 1.596, -0.813, 0.0,&#125;;// BT.709, which is the standard for HDTV.static const GLfloat kColorConversion709[] = &#123; 1.164, 1.164, 1.164, 0.0, -0.213, 2.112, 1.793, -0.533, 0.0,&#125;;// BT.2020 (which is the standard for UHDTV, ITU_R_2020 )static const GLfloat kColorConversion2020[] = &#123; 1.1644f, 1.1644f, 1.1644f, 0.0f, -0.1881, 2.1501, 1.6853, -0.6529, 0.0f,&#125;; Full Range 1234567891011121314151617181920// BT.601 full range static const GLfloat kColorConversion601FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.343, 1.765,1.4, -0.711, 0.0,&#125;;// BT.709 full range static const GLfloat kColorConversion709FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.187, 1.855,1.574, -0.468, 0.0,&#125;;// BT.2020 full range static const GLfloat kColorConversion2020FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.1645, 1.8814,1.4746, -0.5713, 0.0,&#125;; OpenGL Video Range 1234567891011121314151617precision mediump float; varying mediump vec2 textureCoordinate; uniform sampler2D luminanceTexture; uniform sampler2D chrominanceTexture; uniform highp mat3 colorConversionMatrix; void main() &#123; mediump vec3 yuv; highp vec3 rgb; yuv.x = texture2D(luminanceTexture, textureCoordinate).r - (16.0/255.0); yuv.yz = texture2D(chrominanceTexture, textureCoordinate).ra - vec2(0.5, 0.5); rgb = colorConversionMatrix * yuv; gl_FragColor = vec4(rgb, 1.); &#125; Full Range 12345678910111213141516precision mediump float; varying mediump vec2 textureCoordinate; uniform sampler2D luminanceTexture; uniform sampler2D chrominanceTexture; uniform highp mat3 colorConversionMatrix; void main() &#123; mediump vec3 yuv; highp vec3 rgb; yuv.x = texture2D(luminanceTexture, textureCoordinate).r; yuv.yz = texture2D(chrominanceTexture, textureCoordinate).ra - vec2(0.5, 0.5); rgb = colorConversionMatrix * yuv; gl_FragColor = vec4(rgb, 1.); &#125; RGB-YUV123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687// FULL RANGE static void bt2020_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2627 * R + 0.6780 * G + 0.0593 * B; U = -0.1396 * R - 0.3604 * G + 0.5000 * B + 128; V = 0.5000 * R - 0.4598 * G - 0.0402 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt2020_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2256 * R + 0.5823 * G + 0.0509 * B + 16; U = -0.1222 * R - 0.3154 * G + 0.4375 * B + 128; V = 0.4375 * R - 0.4023 * G - 0.0352 * B + 128; /// 8 分解为2 + 6 /// &lt;&lt;2 ：对应 把 yuv 从 (luma=[16,235] chroma=[16,240]) 拉到 (luma=[64,940] chroma=[64,960]) /// &lt;&lt;6 ：10bit 按字节为单位需要两个字节，因为按照大端模式存储（低地址到高地址的顺序存放数据的高位字节到低位字节）后面6个bit是padding 补0 Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt709_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2126 * R + 0.7152 * G + 0.0722 * B; U = -0.1146 * R - 0.3854 * G + 0.5000 * B + 128; V = 0.5000 * R - 0.4542 * G - 0.0458 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt709_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.183 * R + 0.614 * G + 0.062 * B + 16; U = -0.101 * R - 0.339 * G + 0.439 * B + 128; V = 0.439 * R - 0.399 * G - 0.040 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt601_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.169 * R - 0.331 * G + 0.500 * B + 128; V = 0.500 * R - 0.419 * G - 0.081 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt601_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.257 * R + 0.504 * G + 0.098 * B + 16; U = -0.148 * R - 0.291 * G + 0.439 * B + 128; V = 0.439 * R - 0.368 * G - 0.071 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt601_rgb2yuv_PC(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.169 * R - 0.331 * G + 0.500 * B + 128; V = 0.500 * R - 0.419 * G - 0.081 * B + 128; &#125; // VIDEO RANGE static void bt601_rgb2yuv_TV(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.257 * R + 0.504 * G + 0.098 * B + 16; U = -0.148 * R - 0.291 * G + 0.439 * B + 128; V = 0.439 * R - 0.368 * G - 0.071 * B + 128; &#125; 12345678910111213141516171819202122inline CFDictionaryRef HDRAttachmentsOfMedium(void) &#123; const size_t attributes_size = 6; CFTypeRef keys[attributes_size] = &#123; kCVImageBufferFieldCountKey, kCVImageBufferChromaLocationBottomFieldKey, kCVImageBufferChromaLocationTopFieldKey, kCVImageBufferTransferFunctionKey, kCVImageBufferColorPrimariesKey, kCVImageBufferYCbCrMatrixKey &#125;; CFTypeRef values[attributes_size] = &#123; kCFBooleanTrue, kCVImageBufferChromaLocation_Left, kCVImageBufferChromaLocation_Left, kCVImageBufferTransferFunction_ITU_R_2100_HLG, kCVImageBufferColorPrimaries_ITU_R_2020, kCVImageBufferYCbCrMatrix_ITU_R_2020 &#125;; return CreateCFDictionary(keys, values, attributes_size); &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495CVPixelBufferRef RGB2YCbCr10Bit(CVPixelBufferRef pixelBuffer, CFDictionaryRef dic) &#123; CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); //int stride = (int) CVPixelBufferGetBytesPerRow(pixelBuffer) / 4; OSType pixelFormat = kCVPixelFormatType_420YpCbCr10BiPlanarVideoRange; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 5; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey, kCVPixelBufferExtendedPixelsBottomKey, kCVPixelBufferExtendedPixelsTopKey, kCVPixelBufferExtendedPixelsRightKey, kCVPixelBufferExtendedPixelsLeftKey &#125;; CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value, kCFBooleanFalse, kCFBooleanFalse, kCFBooleanFalse, kCFBooleanFalse&#125;; CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; int plane_h1 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 0); int plane_h2 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 1); CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); if (dic == nullptr || CFDictionaryGetCount(dic) &lt;= 0) &#123; dic = HDRAttachmentsOfMedium(); &#125; CVBufferSetAttachments(pixelBufferCopy, dic, kCVAttachmentMode_ShouldPropagate); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 0); //size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 1); unsigned long y_bufferSize = w * h; unsigned long uv_bufferSize = w * h / 4; uint16_t *y_planeData = (uint16_t *) malloc(y_bufferSize * sizeof(uint16_t)); uint16_t *u_planeData = (uint16_t *) malloc(uv_bufferSize * sizeof(uint16_t)); uint16_t *v_planeData = (uint16_t *) malloc(uv_bufferSize * sizeof(uint16_t)); uint16_t *y = (uint16_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 0); uint16_t *uv = (uint16_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 1); int u_offset = 0; int v_offset = 0; uint8_t R, G, B; uint16_t Y, U, V; for (int i = 0; i &lt; h; i ++) &#123; for (int j = 0; j &lt; w; j ++) &#123; int offset = i * w + j; B = baseAddress[offset * 4]; G = baseAddress[offset * 4 + 1]; R = baseAddress[offset * 4 + 2]; bt2020_rgb2yuv10bit_TV(R, G, B, Y, U, V); y_planeData[offset] = Y; //隔行扫描 偶数行的偶数列取U 奇数行的偶数列取V if (j % 2 == 0) &#123; (i % 2 == 0) ? u_planeData[++u_offset] = U : v_planeData[++v_offset] = V; &#125; &#125; &#125; for (int i = 0; i &lt; plane_h1; i ++) &#123; memcpy(y + i * y_stride / 2, y_planeData + i * w, 2 * w); if (i &lt; plane_h2) &#123; for (int j = 0 ; j &lt; w ; j ++) &#123; //NV12 和 NV21 格式都属于 YUV420SP 类型。它也是先存储了 Y 分量，但接下来并不是再存储所有的 U 或者 V 分量，而是把 UV 分量交替连续存储。 //NV12 是 IOS 中有的模式，它的存储顺序是先存 Y 分量，再 UV 进行交替存储。 memcpy(uv + i * y_stride / 2 + 2*j, u_planeData + i * w/2 + j, 2); memcpy(uv + i * y_stride / 2 + 2*j + 1, v_planeData + i * w/2 + j, 2); &#125; &#125; &#125; free(y_planeData); free(u_planeData); free(v_planeData); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); return pixelBufferCopy; &#125; Formula BT.601 BT.709 BT.2020 a 0.299 0.2126 0.2627 b 0.587 0.7152 0.6780 c 0.114 0.0722 0.0593 d 1.772 1.8556 1.8814 e 1.402 1.5748 1.4747 123Y = a * R + b * G + c * BCb = (B - Y) / dCr = (R - Y) / e 123R = Y + e * CrG = Y - (a * e / b) * Cr - (c * d / b) * CbB = Y + d * Cb 123a+b+c = 1e = 2 * (1 - a)d = 2* (a + b) https://www.itu.int/rec/R-REC-BT.601 https://www.itu.int/rec/R-REC-BT.709 https://www.itu.int/rec/R-REC-BT.2020 Range Deduce1234567891011121314[16/255, 16/255, 16/255, 1.0][235/255, 240/255, 240/255, 1.0]x[255/219, 0, 0, 0][0, 255/224, 0, 0][0, 0, 255/224, 0][-16/219, -128/224, -128/224, 1]=[0, -0.5, -0.5, 1.0][1, 0.5, 0.5, 1.0] 将 videorange 通过 齐次矩阵 转换为 fullrange Other大部分图像捕捉设备在保存图像时会自动加上伽马校正，也就是说图像中存储的是非线性空间中的颜色 非线性的RGB转换为YUV也是非线性 OpenGL 无法直接对 10bit YUV 进行处理，需要先转换为 8bit YUV tone mapping 需要在线性RGB空间进行 References搞清楚编程中YUV和RGB间的相互转换YUV - RGB colorconversion推导视频YUV转RGB矩阵齐次坐标# Gamma校正# 我理解的伽马校正Colour gamut conversion from Recommendation ITU-R BT.2020 to Recommendation ITU-R BT.709Colour conversion from Recommendation ITU-R BT.709 to Recommendation ITU-R BT.2020]]></content>
      <categories>
        <category>HDR</category>
      </categories>
      <tags>
        <tag>HDR</tag>
        <tag>YUV</tag>
        <tag>OPENGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coreimage with metal 笔记]]></title>
    <url>%2F2022%2F08%2F08%2Fcoreimage-with-metal-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[xcode近期有自定义CoreImage的CIFilter的需求，前期通过CIKL 定义 CIKernel完成了任务，后面了解到CoreImage新特性支持metal的方式直接自定义 CIKernel，提高效率。 CIKL的方式，存在两个问题： 编写 kernel 的时候，没有报错提示，哪怕是参数名错误都无法检查处理。效率极低。 翻译转换，编译，都是发生到运行时，导致第一次使用滤镜的时候，耗时较久。 Metal: 在build阶段 就可以编译 链接 .metal文件 参考苹果的官方文档 Metal Shading Language for CoreImage Kernels ,在xcode integration 部分提到在build setting 设置 Other Metal Compiler Flags, 文档已经很老了（2018年的），新版的xcode已经没有这个选项了，如果不做处理，会有报错 &quot;/air-lld:1:1: symbol(s) not found for target &#39;air64-apple-ios12.0.0&#39;&quot; and &quot;air-lld command failed with exit code 1 (use -v to see invocation)&quot; build rules新版xcode中可以通过配置build rules解决上面的报错 *.metal 1xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel &quot;$&#123;INPUT_FILE_PATH&#125;&quot; -o &quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;&quot; output files : $(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib *.air 1xcrun metallib -cikernel &quot;$&#123;INPUT_FILE_PATH&#125;&quot; -o &quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;&quot; output files : $(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air 如图： cocoapodsbuild rules 的方式存在一个问题，如果metal shader文件在pod库中，在主工程从配置build rules无法针对pod中的resouce 生效，虽然可以手动针对pod target 配置build rules解决问题，但是这样配置是一次性的，无法提交保存，下一次pod update就清空了，所以到了这里就很自然的能想到通过pod 的post hook 来解决问题，接下来就是怎么用ruby 来写 pod hook 脚本了 通过之前在主工程配置build rules, 可以看到project.pbxproj文件的变更情况 123456789101112131415161718192021222324252627282930/* Begin PBXBuildRule section */ BF25E98B28A0A91A00188AE3 /* PBXBuildRule */ = &#123; isa = PBXBuildRule; compilerSpec = com.apple.compilers.proxy.script; filePatterns = &quot;*.metal&quot;; fileType = pattern.proxy; inputFiles = ( ); isEditable = 1; outputFiles = ( &quot;$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air&quot;, ); runOncePerArchitecture = 0; script = &quot;# Type a script or drag a script file from your workspace to insert its path.\nxcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \&quot;$&#123;INPUT_FILE_PATH&#125;\&quot; -o \&quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;\&quot;\n&quot;; &#125;; BF25E98C28A0A92200188AE3 /* PBXBuildRule */ = &#123; isa = PBXBuildRule; compilerSpec = com.apple.compilers.proxy.script; filePatterns = &quot;*.air&quot;; fileType = pattern.proxy; inputFiles = ( ); isEditable = 1; outputFiles = ( &quot;$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib&quot;, ); runOncePerArchitecture = 0; script = &quot;# Type a script or drag a script file from your workspace to insert its path.\nxcrun metallib -cikernel \&quot;$&#123;INPUT_FILE_PATH&#125;\&quot; -o \&quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;\&quot;\n&quot;; &#125;;/* End PBXBuildRule section */ 哈哈 ，这正是我们需要的build rule的字段 最终的 MetalBuildRule.rb 文件如下 ： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/usr/bin/ruby# -*- coding: UTF-8 -*-def add_build_rule(target_name, project) project.targets.each do |target| if target.name == target_name# puts "#&#123;target.name&#125; has #&#123;target.build_rules.count&#125; build rule." if target.build_rules.count &gt;= 2 puts "#&#123;target.name&#125; already has 2 build rule." return end puts "Updating #&#123;target.name&#125; build rules" metal_rule = project.new(Xcodeproj::Project::Object::PBXBuildRule) metal_rule.name = 'Metal Build Rule' metal_rule.compiler_spec = 'com.apple.compilers.proxy.script' metal_rule.file_patterns = '*.metal' metal_rule.file_type = 'pattern.proxy' metal_rule.is_editable = '1' metal_rule.run_once_per_architecture = '0' metal_rule.output_files = ["$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air"] metal_rule.input_files = [] metal_rule.output_files_compiler_flags = [] metal_rule.script = "xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(metal_rule) air_rule = project.new(Xcodeproj::Project::Object::PBXBuildRule) air_rule.name = 'Air Build Rule' air_rule.compiler_spec = 'com.apple.compilers.proxy.script' air_rule.file_patterns = '*.air' air_rule.file_type = 'pattern.proxy' air_rule.is_editable = '1' air_rule.run_once_per_architecture = '0' air_rule.output_files = ["$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib"] air_rule.input_files = [] air_rule.output_files_compiler_flags = [] air_rule.script = "xcrun metallib -cikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(air_rule) project.objects_by_uuid[metal_rule.uuid] = metal_rule project.objects_by_uuid[air_rule.uuid] = air_rule project.save() end endend podfile 文件里加载 MetalBuildRule.rb, 配置hook 123post_install do |installer| add_build_rule("your-target-name", installer.pods_project)end framework如果pod库是通过cocoapods-packager插件 打.a 或者 .framework的方式提供给主工程使用的话，发现还是会遇到上文提到的错误 &quot;/air-lld:1:1: symbol(s) not found for target &#39;air64-apple-ios12.0.0&#39;&quot; and &quot;air-lld command failed with exit code 1 (use -v to see invocation)&quot; 这里需要简单了解下 cocoapods-packager 的原理，浅析 Cocoapods-Packager 实现.因为 cocoapods-packager 会重新生成一个podfile 来构造一个打包用的工程，所以这个错误的出现跟文章最开始提到的情况是一模一样的，解法是不是也可以通过配置build rule来解呢，不过打包工程我们看起来好像无法干预，怎么解呢？ 还是要回到cocoapods-packager插件来解决问题。 1https://github.com/CocoaPods/cocoapods-packager git 代码拉下来 通过ide(vscode/rubymine) 打开插件工程 配置好工程ruby环境 DEBUG 代码，找到干预点 设置build rule 生成packager gem，安装 这里涉及到ruby gem bundle等ruby环境的基本命令/用法，可以自行google一下。 通过刚刚提到的插件原理，很容易找到干预点 1234pod_utils.rbdef install_pod(platform_name, sandbox)...end 修改： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def install_pod(platform_name, sandbox) # 判断resource_bundle 是否有.metal文件 metal = false if @spec.attributes_hash["resource_bundle"] metal = @spec.attributes_hash["resource_bundle"][@spec.name].include?("metal") end if @spec.attributes_hash["resource_bundles"] if @spec.attributes_hash["resource_bundles"][@spec.name] @spec.attributes_hash["resource_bundles"][@spec.name].each &#123; |res| metal ||= res.include?("metal") &#125; end end ... unless static_installer.nil? static_installer.pods_project.targets.each do |target| # 如果有.metal文件 &amp;&amp; target匹配 -&gt; 设置 build rule if metal &amp;&amp; target.name.start_with?(@spec.name) UI.puts "#&#123;target.name&#125; has #&#123;target.build_rules.count&#125; build rule." if target.build_rules.count &gt;= 2 UI.puts "#&#123;target.name&#125; already has 2 build rule." return end metal_rule = static_installer.pods_project.new(Xcodeproj::Project::Object::PBXBuildRule) UI.puts "Updating #&#123;target.name&#125; rules" metal_rule.name = 'Metal Build Rule' metal_rule.compiler_spec = 'com.apple.compilers.proxy.script' metal_rule.file_patterns = '*.metal' metal_rule.file_type = 'pattern.proxy' metal_rule.is_editable = '1' metal_rule.run_once_per_architecture = '0' metal_rule.output_files = ["$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air"] metal_rule.input_files = [] metal_rule.output_files_compiler_flags = [] metal_rule.script = "xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(metal_rule) air_rule = static_installer.pods_project.new(Xcodeproj::Project::Object::PBXBuildRule) UI.puts "Updating #&#123;target.name&#125; rules" air_rule.name = 'Air Build Rule' air_rule.compiler_spec = 'com.apple.compilers.proxy.script' air_rule.file_patterns = '*.air' air_rule.file_type = 'pattern.proxy' air_rule.is_editable = '1' air_rule.run_once_per_architecture = '0' air_rule.output_files = ["$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib"] air_rule.input_files = [] air_rule.output_files_compiler_flags = [] air_rule.script = "xcrun metallib -cikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(air_rule) static_installer.pods_project.objects_by_uuid[metal_rule.uuid] = metal_rule static_installer.pods_project.objects_by_uuid[air_rule.uuid] = air_rule static_installer.pods_project.save end ... end ... end ... end 最后重新生成、安装gem 12345#!/bin/bashgem uninstall cocoapods-packagergem build cocoapods-packager.gemspecgem install cocoapods-packager podfile使用自定义的cocoapods-packager打出来的二方库 Framework包，包内容里面已经替换成xxx.metallib文件了,所以主工程的podfile pod post hook 要根据二方库的接入方式做下处理。 我这边主工程是用过cocoapod-binary插件管理二方库的加入，源码&amp;静态Framework，一般Release模式提升编译速度，都是以framework方式，Debug模式有时候需要在主工程Debug二方库，可以选择是源码方式接入。 最终的逻辑如下： 1234567891011121314151617post_install do |installer| installer.pods_project.targets.each do |target| ... target.build_configurations.each do |config| ... #Release model, no need execute if config.name != 'Release' &amp;&amp; target.name == 'your target name' puts "===================&gt; #&#123;config.name&#125;" eval(File.open('MetalBuildRule.rb').read) if File.exist? 'MetalBuildRule.rb' # metal shader build rule add_build_rule(target, installer.pods_project) end end endend DONE referencesMetalCIKLReferenceAdd custom build rule with Podfile post_install hookxcodeprojxcode工程文件解析CocoaPods源码与插件断点调试]]></content>
      <categories>
        <category>metal</category>
      </categories>
      <tags>
        <tag>metal</tag>
        <tag>coreimage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter 1.17共享engine]]></title>
    <url>%2F2020%2F06%2F03%2Fflutter-1-17%E5%85%B1%E4%BA%ABengine%2F</url>
    <content type="text"><![CDATA[前言flutter 升级到 1.17之后，app ios 线上遇到一个crash ,通过官方的 符号表文件 flutter.dsym 还原出堆栈如下 1234560 auto fml::internal::CopyableLambda&lt;flutter::Shell::OnPlatformViewCreated(std::__1::unique_ptr&lt;flutter::Surface, std::__1::default_delete&lt;flutter::Surface&gt; &gt;)::$_8&gt;::operator()&lt;&gt;() const (in Flutter) (make_copyable.h:24)1 auto fml::internal::CopyableLambda&lt;flutter::Shell::OnPlatformViewCreated(std::__1::unique_ptr&lt;flutter::Surface, std::__1::default_delete&lt;flutter::Surface&gt; &gt;)::$_8&gt;::operator()&lt;&gt;() const (in Flutter) (make_copyable.h:24)2 fml::MessageLoopImpl::FlushTasks(fml::FlushType) (in Flutter) (message_loop_impl.cc:129)3 fml::MessageLoopDarwin::OnTimerFire(__CFRunLoopTimer*, fml::MessageLoopDarwin*) (in Flutter) (message_loop_darwin.mm:76)9 fml::MessageLoopDarwin::Run() (in Flutter) (message_loop_darwin.mm:47)10 void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, fml::Thread::Thread(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)::$_0&gt; &gt;(void*) (in Flutter) (thread:352) 这里还只能看到crash在engine的c++代码中，具体原因未知 定位我们根据crash 用户的 埋点日志 分析crash前的 使用路径，基本都是打开push 落地到一个flutter页面app 的 第一个tab 也是个 flutter 页面，所以是push 唤起app，连续打开两个flutter页面。手动打开app，点击进到flutter页面是不会crash的（这么简单的路径，如果crash，那就该死了）很快我们就可以通过这个 路径 复现 crash ，能复现就好说。 debug engine源码，可以定位到更具体的地方 surface_ 为 null ，EXC_BAD_ACCESS 野指针 分析定位到了具体的代码位置，接下来分析下野指针的原因xcode中 crash 的时候，看到主线程的 堆栈记录 是从 application 的 didbecomeactive 的 notification发起的由于是 push 唤起app ，有这个通知是对的，crash 是在 共享engine 的 raster 线程。 看代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#pragma mark - Application lifecycle notifications// app 的 首页 flutter 页面会 执行 surfaceUpdated 方法- (void)applicationBecameActive:(NSNotification*)notification &#123; TRACE_EVENT0("flutter", "applicationBecameActive"); if (_viewportMetrics.physical_width) [self surfaceUpdated:YES]; [self goToApplicationLifecycle:@"AppLifecycleState.resumed"];&#125;#pragma mark - Surface creation and teardown updates- (void)surfaceUpdated:(BOOL)appeared &#123; // NotifyCreated/NotifyDestroyed are synchronous and require hops between the UI and raster // thread. if (appeared) &#123; [self installFirstFrameCallback]; [_engine.get() platformViewsController] -&gt; SetFlutterView(_flutterView.get()); [_engine.get() platformViewsController] -&gt; SetFlutterViewController(self); // 这里 [_engine.get() platformView] -&gt; NotifyCreated(); &#125; else &#123; self.displayingFlutterUI = NO; [_engine.get() platformView] -&gt; NotifyDestroyed(); [_engine.get() platformViewsController] -&gt; SetFlutterView(nullptr); [_engine.get() platformViewsController] -&gt; SetFlutterViewController(nullptr); &#125;&#125;void PlatformView::NotifyCreated() &#123; std::unique_ptr&lt;Surface&gt; surface; // Threading: We want to use the platform view on the non-platform thread. // Using the weak pointer is illegal. But, we are going to introduce a latch // so that the platform view is not collected till the surface is obtained. auto* platform_view = this; fml::ManualResetWaitableEvent latch; fml::TaskRunner::RunNowOrPostTask( task_runners_.GetRasterTaskRunner(), [platform_view, &amp;surface, &amp;latch]() &#123; surface = platform_view-&gt;CreateRenderingSurface(); latch.Signal(); &#125;); latch.Wait(); //这里 delegate_.OnPlatformViewCreated(std::move(surface));&#125;// |PlatformView::Delegate|void Shell::OnPlatformViewCreated(std::unique_ptr&lt;Surface&gt; surface) &#123; TRACE_EVENT0("flutter", "Shell::OnPlatformViewCreated"); FML_DCHECK(is_setup_); FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); // Note: // This is a synchronous operation because certain platforms depend on // setup/suspension of all activities that may be interacting with the GPU in // a synchronous fashion. fml::AutoResetWaitableEvent latch; auto raster_task = fml::MakeCopyable([&amp; waiting_for_first_frame = waiting_for_first_frame_, rasterizer = rasterizer_-&gt;GetWeakPtr(), // surface = std::move(surface), // &amp;latch]() mutable &#123; if (rasterizer) &#123; //这里 rasterizer-&gt;Setup(std::move(surface)); &#125; waiting_for_first_frame.store(true); // Step 3: All done. Signal the latch that the platform thread is // waiting on. latch.Signal(); &#125;); ... &#125;void Rasterizer::Setup(std::unique_ptr&lt;Surface&gt; surface) &#123; surface_ = std::move(surface); if (max_cache_bytes_.has_value()) &#123; SetResourceCacheMaxBytes(max_cache_bytes_.value(), user_override_resource_cache_bytes_); &#125; compositor_context_-&gt;OnGrContextCreated(); // surface_ null，BAD_ACCESS if (surface_-&gt;GetExternalViewEmbedder()) &#123; const auto platform_id = task_runners_.GetPlatformTaskRunner()-&gt;GetTaskQueueId(); const auto gpu_id = task_runners_.GetRasterTaskRunner()-&gt;GetTaskQueueId(); raster_thread_merger_ = fml::MakeRefCounted&lt;fml::RasterThreadMerger&gt;(platform_id, gpu_id); &#125;&#125; 这么一路看下来，surface_怎么会变成null呢？一般情况是，执行 [self surfaceUpdated:NO] 的时候会销毁surface，断点根本都没进去。继续看代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107// push 落地页 flutter 页面 init 的时候，会重新attach 到 engine，会执行setViewController方法- (void)setViewController:(FlutterViewController*)viewController &#123; FML_DCHECK(self.iosPlatformView); _viewController = viewController ? [viewController getWeakPtr] : fml::WeakPtr&lt;FlutterViewController&gt;(); //这里 self.iosPlatformView-&gt;SetOwnerViewController(_viewController); [self maybeSetupPlatformViewChannels]; if (viewController) &#123; __block FlutterEngine* blockSelf = self; self.flutterViewControllerWillDeallocObserver = [[NSNotificationCenter defaultCenter] addObserverForName:FlutterViewControllerWillDealloc object:viewController queue:[NSOperationQueue mainQueue] usingBlock:^(NSNotification* note) &#123; [blockSelf notifyViewControllerDeallocated]; &#125;]; &#125; else &#123; self.flutterViewControllerWillDeallocObserver = nil; &#125;&#125;void PlatformViewIOS::SetOwnerViewController(fml::WeakPtr&lt;FlutterViewController&gt; owner_controller) &#123; FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); std::lock_guard&lt;std::mutex&gt; guard(ios_surface_mutex_); // 重点是这里 if (ios_surface_ || !owner_controller) &#123; // 这里会销毁 NotifyDestroyed(); ios_surface_.reset(); accessibility_bridge_.reset(); &#125; owner_controller_ = owner_controller; // Add an observer that will clear out the owner_controller_ ivar and // the accessibility_bridge_ in case the view controller is deleted. dealloc_view_controller_observer_.reset( [[[NSNotificationCenter defaultCenter] addObserverForName:FlutterViewControllerWillDealloc object:owner_controller_.get() queue:[NSOperationQueue mainQueue] usingBlock:^(NSNotification* note) &#123; // Implicit copy of 'this' is fine. accessibility_bridge_.reset(); owner_controller_.reset(); &#125;] retain]); if (owner_controller_ &amp;&amp; [owner_controller_.get() isViewLoaded]) &#123; this-&gt;attachView(); &#125; // Do not call `NotifyCreated()` here - let FlutterViewController take care // of that when its Viewport is sized. If `NotifyCreated()` is called here, // it can occasionally get invoked before the viewport is sized resulting in // a framebuffer that will not be able to completely attach.&#125;void PlatformView::NotifyDestroyed() &#123; delegate_.OnPlatformViewDestroyed();&#125;// |PlatformView::Delegate|void Shell::OnPlatformViewDestroyed() &#123; TRACE_EVENT0("flutter", "Shell::OnPlatformViewDestroyed"); FML_DCHECK(is_setup_); FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); // Note: // This is a synchronous operation because certain platforms depend on // setup/suspension of all activities that may be interacting with the GPU in // a synchronous fashion. fml::AutoResetWaitableEvent latch; auto io_task = [io_manager = io_manager_.get(), &amp;latch]() &#123; // Execute any pending Skia object deletions while GPU access is still // allowed. io_manager-&gt;GetIsGpuDisabledSyncSwitch()-&gt;Execute( fml::SyncSwitch::Handlers().SetIfFalse( [&amp;] &#123; io_manager-&gt;GetSkiaUnrefQueue()-&gt;Drain(); &#125;)); // Step 3: All done. Signal the latch that the platform thread is waiting // on. latch.Signal(); &#125;; auto raster_task = [rasterizer = rasterizer_-&gt;GetWeakPtr(), io_task_runner = task_runners_.GetIOTaskRunner(), io_task]() &#123; if (rasterizer) &#123; // 这里 rasterizer-&gt;Teardown(); &#125; // Step 2: Next, tell the IO thread to complete its remaining work. fml::TaskRunner::RunNowOrPostTask(io_task_runner, io_task); &#125;; ...void Rasterizer::Teardown() &#123; compositor_context_-&gt;OnGrContextDestroyed(); // 这里 reset surface_.reset(); last_layer_tree_.reset();&#125; 所以原因 就是 落地页 init 的时候 重新attach 引擎，NotifyDestroyed 方法 最终会销毁 surface，这时候正好raster线程使用 surface_做方法调用。 修复定位到原因，修复就简单了，做下空判断就好了,如果为空 就直接return 123456789101112131415161718192021void Rasterizer::Setup(std::unique_ptr&lt;Surface&gt; surface) &#123; surface_ = std::move(surface); if (!surface_) &#123; FML_DLOG(INFO) &lt;&lt; "Rasterizer::Setup called with no surface."; return; &#125; if (max_cache_bytes_.has_value()) &#123; SetResourceCacheMaxBytes(max_cache_bytes_.value(), user_override_resource_cache_bytes_); &#125; compositor_context_-&gt;OnGrContextCreated(); if (surface_-&gt;GetExternalViewEmbedder()) &#123; const auto platform_id = task_runners_.GetPlatformTaskRunner()-&gt;GetTaskQueueId(); const auto gpu_id = task_runners_.GetRasterTaskRunner()-&gt;GetTaskQueueId(); raster_thread_merger_ = fml::MakeRefCounted&lt;fml::RasterThreadMerger&gt;(platform_id, gpu_id); &#125;&#125; 这里是直接修改了引擎的代码，所以需要重新编译engine 产物，替换掉就搞定了 其他1.17之前的版本 1.12.13 的时候，不确定engine存不存在这个问题，有空再看看。后面github提issue、PR，看看官方怎么看待这个问题，修复应该还有其他办法。]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter 1.17升级]]></title>
    <url>%2F2020%2F05%2F21%2Fflutter-1-17%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[升级最近官方发布了flutter 稳定版本1.17.0 ，记录下升级1.17 ios上碰到的的问题 App产物在 1.12.13 的时候，为了支持模拟器运行，会进行 debug 产物 跟 release 产物的merge （lipo create …）debug 产物 x86 、release 产物 arm64 arm7 升级到1.17.0 之后 ，merge报错 lipo 查看下 发现针对模拟器的debug产物 含有arm64 Debug Flutter tool源码， build 里面进行了两次createStubAppFramework（iphone &amp;&amp; simulator）然后做了merge，实际上environment参数里面 iosArchs只有 arch x86，所以问题出在这里 DebugUniveralFramework1234567891011121314151617181920212223242526272829303132333435363738394041@overrideFuture&lt;void&gt; build(Environment environment) async &#123; // Generate a trivial App.framework. final Set&lt;DarwinArch&gt; iosArchs = environment.defines[kIosArchs] ?.split(' ') ?.map(getIOSArchForName) ?.toSet() ?? &lt;DarwinArch&gt;&#123;DarwinArch.arm64&#125;; final File iphoneFile = environment.buildDir.childFile('iphone_framework'); final File simulatorFile = environment.buildDir.childFile('simulator_framework'); final File lipoOutputFile = environment.buildDir.childFile('App'); final RunResult iphoneResult = await createStubAppFramework( iphoneFile, SdkType.iPhone, // Only include 32bit if it is contained in the active architectures. include32Bit: iosArchs.contains(DarwinArch.armv7) ); final RunResult simulatorResult = await createStubAppFramework( simulatorFile, SdkType.iPhoneSimulator, ); if (iphoneResult.exitCode != 0 || simulatorResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125; final List&lt;String&gt; lipoCommand = &lt;String&gt;[ 'xcrun', 'lipo', '-create', iphoneFile.path, simulatorFile.path, '-output', lipoOutputFile.path ]; final RunResult lipoResult = await processUtils.run( lipoCommand, ); if (lipoResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125;&#125; 解决办法 可以通过archs 判断下具体执行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@override Future&lt;void&gt; build(Environment environment) async &#123; // Generate a trivial App.framework. final Set&lt;DarwinArch&gt; iosArchs = environment.defines[kIosArchs] ?.split(' ') ?.map(getIOSArchForName) ?.toSet() ?? &lt;DarwinArch&gt;&#123;DarwinArch.arm64&#125;; final File iphoneFile = environment.buildDir.childFile('iphone_framework'); final File simulatorFile = environment.buildDir.childFile('simulator_framework'); final File lipoOutputFile = environment.buildDir.childFile('App'); RunResult iphoneResult; if(iosArchs.contains(DarwinArch.arm64) || iosArchs.contains(DarwinArch.armv7)) &#123; iphoneResult = await createStubAppFramework( iphoneFile, SdkType.iPhone, // Only include 32bit if it is contained in the active architectures. include32Bit: iosArchs.contains(DarwinArch.armv7) ); if (iphoneResult.exitCode != 0) &#123; throw Exception('(iphoneResult)Failed to create App.framework.'); &#125; &#125; RunResult simulatorResult; if(iosArchs.contains(DarwinArch.x86_64)) &#123; simulatorResult = await createStubAppFramework( simulatorFile, SdkType.iPhoneSimulator, ); if (simulatorResult.exitCode != 0) &#123; throw Exception('(simulatorResult)Failed to create App.framework.'); &#125; &#125; if(simulatorResult == null) &#123; iphoneFile.copySync(lipoOutputFile.path); return; &#125; if(iphoneResult == null) &#123; simulatorFile.copySync(lipoOutputFile.path); return; &#125; final List&lt;String&gt; lipoCommand = &lt;String&gt;[ 'xcrun', 'lipo', '-create', iphoneFile.path, simulatorFile.path, '-output', lipoOutputFile.path ]; final RunResult lipoResult = await processUtils.run( lipoCommand, ); if (lipoResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125; &#125; 突然想到 既然1.17 对debug 产物做了arm64的支持，那我们收集产物是不是可以不用自己做merge，发现是不可以的因为除了App.framework,还有plugin native代码生成的pod静态库 libxxx.a。静态库 是哪里生成的呢？ 这里 build_ios.dart -&gt; buildXcodeProject 123456789101112131415161718...final List&lt;String&gt; buildCommands = &lt;String&gt;[ &apos;/usr/bin/env&apos;, &apos;xcrun&apos;, &apos;xcodebuild&apos;, &apos;-configuration&apos;, configuration,]; ...if (buildForDevice) &#123; buildCommands.addAll(&lt;String&gt;[&apos;-sdk&apos;, &apos;iphoneos&apos;]);&#125; else &#123; buildCommands.addAll(&lt;String&gt;[&apos;-sdk&apos;, &apos;iphonesimulator&apos;, &apos;-arch&apos;, &apos;x86_64&apos;]);&#125;... 这里只针对x86做了xcode build，所以还是要自己merge的…]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter tool debug]]></title>
    <url>%2F2020%2F03%2F13%2Fflutter-tool-debug%2F</url>
    <content type="text"><![CDATA[Flutterflutter 开发过程中，少不了会运行一些 flutter 命令 ，比如 flutter build xxx 、 flutter run 等等看下 bin/flutter 脚本，背后都是 flutter_tool 在执行各种操作。 12345678FLUTTER_TOOLS_DIR="$FLUTTER_ROOT/packages/flutter_tools"SNAPSHOT_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.snapshot"STAMP_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.stamp"SCRIPT_PATH="$FLUTTER_TOOLS_DIR/bin/flutter_tools.dart"DART_SDK_PATH="$FLUTTER_ROOT/bin/cache/dart-sdk""$DART" --packages="$FLUTTER_TOOLS_DIR/.packages" $FLUTTER_TOOL_ARGS "$SNAPSHOT_PATH" "$@" 关于 debug flutter_tool,就不说的了，自行google一下。。 这里记录下 debug 过程中遇到的问题 Exception指定端口导出FLUTTER_TOOL_ARGS 环境变量 export FLUTTER_TOOL_ARGS=&quot;--pause_isolates_on_start --enable-vm-service:65432&quot; run 起来 会停在这里… Observatory listening on http://127.0.0.1:65432/ZbWg3veM6kY=/ IDE 中打开flutter tool 项目，配置 dart remote debug attach 出现以下错误信息 123Failed to connect to the VM observatory service: java.io.IOException: Failed to connect: ws://127.0.0.1:65432/wsCaused by: de.roderick.weberknecht.WebSocketException: error while creating socket to ws://127.0.0.1:65432/wsCaused by: java.net.ConnectException: Connection refused (Connection refused) 原因 ： http://127.0.0.1:65432/ZbWg3veM6kY=/ 后面多了个ZbWg3veM6kY，这是一种认证码，是为了安全原因，防止应用被远程调试。可以通过参数–disable-service-auth-codes进行关闭。 export FLUTTER_TOOL_ARGS=&quot;--pause_isolates_on_start --enable-vm-service:65432 --disable-service-auth-codes&quot;]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter共享engine]]></title>
    <url>%2F2020%2F03%2F10%2Fflutter%E5%85%B1%E4%BA%ABengine%2F</url>
    <content type="text"><![CDATA[flutter 共享引擎 问题记录共享引擎，就是只有一个 flutter engine，每个页面一个 flutterviewcontroller。flutter页面 切换，引擎会相应的 detach atach 最近 升级 flutter 到 v1.12.13 版本后，贡献引擎遇到的几个问题 记录下 present flutter 页面这个其实不是v1.12.13出现的问题 flutterPageA present flutterPageB 会 出现 pageA的 viewDidDisappear 比 pageB的 viewDidAppear 后执行 12345678910- (void)viewDidDisappear:(BOOL)animated &#123; [super viewDidDisappear:animated]; //处理下present 页面卡死的情况 if ([WD_FLUTTER_ENGINE flutterViewController] != self) &#123; [WD_FLUTTER_ENGINE resume]; [(WDFlutterViewContainer *)[WD_FLUTTER_ENGINE flutterViewController] surfaceUpdated:YES]; &#125; else &#123; [WD_FLUTTER_ENGINE detach]; &#125;&#125; 从flutter 返回到 native页面在 v.1.12.13之前 flutter popto native 无需处理v.1.12.13 出现crash 123456789101112131415161718192021222324[VERBOSE-2:FlutterObservatoryPublisher.mm(131)] Could not register as server for FlutterObservatoryPublisher. Check your network settings and relaunch the application.SCNetworkReachabilitySetDispatchQueue() failed: Invalid argumentSCNetworkReachabilitySetDispatchQueue() failed: Invalid argumentlocalConnectionInitializedStatus:2localConnectionInitializedStatus:2SCNetworkReachabilitySetDispatchQueue() failed: Invalid argument[Bugly] Fatal signal(11) raised.[Bugly] Trapped fatal signal &apos;SIGSEGV(11)&apos; ( &quot;0 Flutter 0x000000010516f92c _ZNK3fml8internal14CopyableLambdaIZN7flutter5Shell21OnPlatformViewCreatedENSt3__110unique_ptrINS2_7SurfaceENS4_14default_deleteIS6_EEEEE3$_8EclIJEEEDaDpOT_ + 236&quot;, &quot;1 Flutter 0x000000010516f928 _ZNK3fml8internal14CopyableLambdaIZN7flutter5Shell21OnPlatformViewCreatedENSt3__110unique_ptrINS2_7SurfaceENS4_14default_deleteIS6_EEEEE3$_8EclIJEEEDaDpOT_ + 232&quot;, &quot;2 Flutter 0x0000000105123cf4 _ZN3fml15MessageLoopImpl10FlushTasksENS_9FlushTypeE + 1700&quot;, &quot;3 Flutter 0x0000000105126000 _ZN3fml17MessageLoopDarwin11OnTimerFireEP16__CFRunLoopTimerPS0_ + 32&quot;, &quot;4 CoreFoundation 0x0000000184cd3aa8 0x0000000184be5000 + 977576&quot;, &quot;5 CoreFoundation 0x0000000184cd376c 0x0000000184be5000 + 976748&quot;, &quot;6 CoreFoundation 0x0000000184cd3010 0x0000000184be5000 + 974864&quot;, &quot;7 CoreFoundation 0x0000000184cd0b60 0x0000000184be5000 + 965472&quot;, &quot;8 CoreFoundation 0x0000000184bf0da8 CFRunLoopRunSpecific + 552&quot;, &quot;9 Flutter 0x0000000105125edc _ZN3fml17MessageLoopDarwin3RunEv + 88&quot;, &quot;10 Flutter 0x0000000105125684 _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN3fml6ThreadC1ERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEEE3$_0EEEEEPvSJ_ + 200&quot;, &quot;11 libsystem_pthread.dylib 0x0000000184951220 0x000000018494f000 + 8736&quot;, &quot;12 libsystem_pthread.dylib 0x0000000184951110 0x000000018494f000 + 8464&quot;)Application finished. 处理办法： flutterVc dealloc 或者 disappear 的时候 执行 flutterEngine detach]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter engine 定制]]></title>
    <url>%2F2020%2F03%2F05%2Fflutter-engine-%E5%AE%9A%E5%88%B6%2F</url>
    <content type="text"><![CDATA[…使用Flutter开发的时候最直接接触的并不是 Flutter Engine 而是 Flutter Framework(https://github.com/flutter/flutter)在flutter framework 的 目录里面 有编译好的engine 产物 简单说就是， 编译引擎 替换 产物文件就好了 路径 flutter_path/bin/cache/artifacts/engine/ios 参考Flutter Engine定制流程Flutter Engine 编译指北]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter eventChannel crash on ios]]></title>
    <url>%2F2019%2F10%2F21%2Fflutter-eventChannel-crash-on-ios%2F</url>
    <content type="text"><![CDATA[flutter issuecrash 记录一次困扰了很久的 flutter event channel crash EventChannel dart1234567891011121314151617181920212223242526272829303132333435363738394041Stream&lt;dynamic&gt; receiveBroadcastStream([ dynamic arguments ]) &#123; final MethodChannel methodChannel = MethodChannel(name, codec); StreamController&lt;dynamic&gt; controller; controller = StreamController&lt;dynamic&gt;.broadcast(onListen: () async &#123; defaultBinaryMessenger.setMessageHandler(name, (ByteData reply) async &#123; if (reply == null) &#123; controller.close(); &#125; else &#123; try &#123; controller.add(codec.decodeEnvelope(reply)); &#125; on PlatformException catch (e) &#123; controller.addError(e); &#125; &#125; return null; &#125;); try &#123; await methodChannel.invokeMethod&lt;void&gt;('listen', arguments); &#125; catch (exception, stack) &#123; FlutterError.reportError(FlutterErrorDetails( exception: exception, stack: stack, library: 'services library', context: ErrorDescription('while activating platform stream on channel $name'), )); &#125; &#125;, onCancel: () async &#123; defaultBinaryMessenger.setMessageHandler(name, null); try &#123; await methodChannel.invokeMethod&lt;void&gt;('cancel', arguments); &#125; catch (exception, stack) &#123; FlutterError.reportError(FlutterErrorDetails( exception: exception, stack: stack, library: 'services library', context: ErrorDescription('while de-activating platform stream on channel $name'), )); &#125; &#125;); return controller.stream; &#125; FlutterChannel.mm1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static void SetStreamHandlerMessageHandlerOnChannel(NSObject&lt;FlutterStreamHandler&gt;* handler, NSString* name, NSObject&lt;FlutterBinaryMessenger&gt;* messenger, NSObject&lt;FlutterMethodCodec&gt;* codec) &#123; __block FlutterEventSink currentSink = nil; FlutterBinaryMessageHandler messageHandler = ^(NSData* message, FlutterBinaryReply callback) &#123; FlutterMethodCall* call = [codec decodeMethodCall:message]; if ([call.method isEqual:@"listen"]) &#123; if (currentSink) &#123; FlutterError* error = [handler onCancelWithArguments:nil]; if (error) NSLog(@"Failed to cancel existing stream: %@. %@ (%@)", error.code, error.message, error.details); &#125; currentSink = ^(id event) &#123; if (event == FlutterEndOfEventStream) [messenger sendOnChannel:name message:nil]; else if ([event isKindOfClass:[FlutterError class]]) [messenger sendOnChannel:name message:[codec encodeErrorEnvelope:(FlutterError*)event]]; else [messenger sendOnChannel:name message:[codec encodeSuccessEnvelope:event]]; &#125;; FlutterError* error = [handler onListenWithArguments:call.arguments eventSink:currentSink]; if (error) callback([codec encodeErrorEnvelope:error]); else callback([codec encodeSuccessEnvelope:nil]); &#125; else if ([call.method isEqual:@"cancel"]) &#123; if (!currentSink) &#123; callback( [codec encodeErrorEnvelope:[FlutterError errorWithCode:@"error" message:@"No active stream to cancel" details:nil]]); return; &#125; currentSink = nil; FlutterError* error = [handler onCancelWithArguments:call.arguments]; if (error) callback([codec encodeErrorEnvelope:error]); else callback([codec encodeSuccessEnvelope:nil]); &#125; else &#123; callback(nil); &#125; &#125;; [messenger setMessageHandlerOnChannel:name binaryMessageHandler:messageHandler];&#125; EventSink正常结束stream流 eventSink(FlutterEndOfEventStream) ，异常结束stream流 eventSink(FlutterError) 都会回调执行 onCancel 参考Flutter 与 Native(iOS) 通信原理深入Flutter技术内幕:Platform Channel设计与实现]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter ios 13 dark mode]]></title>
    <url>%2F2019%2F10%2F14%2Fflutter-ios-13-dark-mode%2F</url>
    <content type="text"><![CDATA[前言ios 13 开启 dark model,flutter页面status bar文字一直是白色 flutter issues 1SystemChrome.setSystemUIOverlayStyle(SystemUiOverlayStyle.dark); 设置dark style 并没有用 SystemChrome12345678910111213141516171819202122232425static void setSystemUIOverlayStyle(SystemUiOverlayStyle style) &#123; assert(style != null); if (_pendingStyle != null) &#123; // The microtask has already been queued; just update the pending value. _pendingStyle = style; return; &#125; if (style == _latestStyle) &#123; // Trivial success: no microtask has been queued and the given style is // already in effect, so no need to queue a microtask. return; &#125; _pendingStyle = style; scheduleMicrotask(() &#123; assert(_pendingStyle != null); if (_pendingStyle != _latestStyle) &#123; SystemChannels.platform.invokeMethod&lt;void&gt;( 'SystemChrome.setSystemUIOverlayStyle', _pendingStyle._toMap(), ); _latestStyle = _pendingStyle; &#125; _pendingStyle = null; &#125;); &#125; FlutterPlatformPlugin1234567891011121314151617181920212223242526272829- (void)setSystemChromeSystemUIOverlayStyle:(NSDictionary*)message &#123; NSString* style = message[@"statusBarBrightness"]; if (style == (id)[NSNull null]) return; UIStatusBarStyle statusBarStyle; if ([style isEqualToString:@"Brightness.dark"]) statusBarStyle = UIStatusBarStyleLightContent; else if ([style isEqualToString:@"Brightness.light"]) statusBarStyle = UIStatusBarStyleDefault; else return; NSNumber* infoValue = [[NSBundle mainBundle] objectForInfoDictionaryKey:@"UIViewControllerBasedStatusBarAppearance"]; Boolean delegateToViewController = (infoValue == nil || [infoValue boolValue]); if (delegateToViewController) &#123; // This notification is respected by the iOS embedder [[NSNotificationCenter defaultCenter] postNotificationName:@(kOverlayStyleUpdateNotificationName) object:nil userInfo:@&#123;@(kOverlayStyleUpdateNotificationKey) : @(statusBarStyle)&#125;]; &#125; else &#123; // Note: -[UIApplication setStatusBarStyle] is deprecated in iOS9 // in favor of delegating to the view controller [[UIApplication sharedApplication] setStatusBarStyle:statusBarStyle]; &#125;&#125; engine 源码中 可以看到 没有 UIStatusBarStyleDarkContent 尝试 去掉 info.plist 中的 UIViewControllerBasedStatusBarAppearance 然后 监听 通知 1234567[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(appStatusBar:) name:@"io.flutter.plugin.platform.SystemChromeOverlayNotificationName" object:nil];- (void)appStatusBar:(id)notification &#123; if (@available(iOS 13.0, *)) &#123; [UIApplication sharedApplication].statusBarStyle = UIStatusBarStyleDarkContent; &#125;&#125;]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mustache之dart]]></title>
    <url>%2F2019%2F05%2F31%2Fmustache%E4%B9%8Bdart%2F</url>
    <content type="text"><![CDATA[前言Mustache 是一个 logic-less （轻逻辑）模板解析引擎，可以应用在 js、PHP、Python、Perl 等多种编程语言中。这里主要是看dart中的应用。 模板语法很简单 看这里1234567&#123;&#123;keyName&#125;&#125; &#123;&#123;#keyName&#125;&#125; &#123;&#123;/keyName&#125;&#125;&#123;&#123;^keyName&#125;&#125; &#123;&#123;/keyName&#125;&#125;&#123;&#123;.&#125;&#125;&#123;&#123;&gt;partials&#125;&#125;&#123;&#123;&#123;keyName&#125;&#125;&#125;&#123;&#123;!comments&#125;&#125; 使用在flutter项目中，使用annation router注解的方式生成路由表管理类 RouterManager ，以及业务相关的类文件（ios android）在使用mustache之前，是通过stringbuff 的方式拼接字符串，也可以完成，但是阅读性比较差。 之前123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/// 生成路由表类 StringBuffer sb = new StringBuffer(); sb..write(_createImport())..write(_createClazz(element.name)); return sb.toString();/// 生成路由表类的 import 信息 String _createImport() &#123; StringBuffer sb = new StringBuffer(); sb ..writeln("import 'package:flutter/material.dart';") ..writeln("import 'package:hybrid_router/hybrid_router.dart';"); /// import page collector.importClazzList.forEach((clazz) &#123; sb.writeln("import '$clazz';"); &#125;); return sb.toString(); &#125; /// 生成路由表类的 clazz 信息 String _createClazz(String className) &#123; StringBuffer sb = new StringBuffer(); /// start class sb.writeln("class \$$className &#123;"); /// generateRoute function sb ..writeln(" Map&lt;String, HybridWidgetBuilder&gt; generateRoutes()&#123;") ..writeln(" return &#123;"); collector.routeMap.forEach((key, value) &#123; String flutterPath = value.flutterPath; sb ..writeln(" '$flutterPath': (BuildContext context, Object args) &#123;") ..writeln(" $&#123;_createInstance(value)&#125;") ..writeln(" &#125;,"); &#125;); sb..writeln(" &#125;;")..writeln(" &#125;"); /// generateSpm function sb ..writeln(" Map&lt;String, String&gt; generateSpm() &#123;") ..writeln(" return &#123;"); collector.routeMap.forEach((key, value) &#123; String flutterSpm = value.flutterSpm; String flutterPath = value.flutterPath; if (flutterSpm?.isNotEmpty == true) &#123; sb.writeln(" '$flutterPath': '$flutterSpm',"); &#125; &#125;); sb..writeln(" &#125;;")..writeln(" &#125;"); /// end class sb.writeln("&#125;"); return sb.toString(); &#125; 之后1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/// 生成路由表类 _createRouteManager(element.name); String _createRouteManager(String className) &#123; return VDTemplate.routerManagerTemplate().renderString(&#123; 'classname':className, 'classes':collector.importClazzList, 'routes':collector.routeMap.values, 'createInstance':(LambdaContext ctx) &#123; CollectorItem item = ctx.lookup('.'); return _createInstance(item); &#125; &#125;); &#125; static Template routerManagerTemplate() &#123; var source = '''&#123;&#123;&gt; import&#125;&#125;&#123;&#123;&gt; clazz&#125;&#125; '''; Map&lt;String,Template&gt; map = &#123; "import":VDTemplate.importTemplate(), "clazz":VDTemplate.claszzTemplate() &#125;; return new Template(source,partialResolver: (String name) =&gt; map[name]); &#125; static Template importTemplate() &#123; var source = '''import 'package:flutter/material.dart';import 'package:hybrid_router/hybrid_router.dart';&#123;&#123;# classes &#125;&#125;import '&#123;&#123;&#123;.&#125;&#125;&#125;';&#123;&#123;/ classes &#125;&#125; '''; return new Template(source); &#125; static Template claszzTemplate() &#123; var source = '''class \$&#123;&#123;classname&#125;&#125; &#123; Map&lt;String, HybridWidgetBuilder&gt; generateRoutes()&#123; return &#123; &#123;&#123;#routes&#125;&#125; '&#123;&#123;flutterPath&#125;&#125;': (BuildContext context, Object args) &#123; &#123;&#123;createInstance&#125;&#125; &#125;, &#123;&#123;/routes&#125;&#125; &#125;; &#125; Map&lt;String, String&gt; generateSpm() &#123; return &#123; &#123;&#123;#routes&#125;&#125; &#123;&#123;# flutterSpm&#125;&#125; '&#123;&#123;flutterPath&#125;&#125;': '&#123;&#123;flutterSpm&#125;&#125;', &#123;&#123;/ flutterSpm&#125;&#125; &#123;&#123;/routes&#125;&#125; &#125;; &#125; &#125; '''; return new Template(source); &#125; 对比看下，使用mustache之后，可读性好很多，基本保持了代码结构 总结 字符串数组，可以使用{{.}} 对象数据，可以跟普通的hash一样，直接用{{对象的属性}}，mustache内部通过dart反射拿到属性值 使用partials 拆分template 增加可读性 通过lambda函数执行dart方法，也可以做到拆分的作用 lambdaContext.loopup(&quot;.&quot;) 可以获取对象实例，进而可以参数传递 mustache内部renderstring也是通过stringbuff的方式实现 参考链接mustache 1.1.1Flutter路由管理]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>mustache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm调优]]></title>
    <url>%2F2019%2F05%2F23%2Fjvm%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[jps输出JVM中运行的进程状态信息 1234-q 不输出类名、Jar名和传入main方法的参数-m 输出传入main方法的参数-l 输出main类或Jar的全限名-v 输出传入JVM的参数 jps -mlv找到java应用的pid jstack根据java应用pid ,查看进程中线程堆栈信息 jstack pid top找出该进程内最耗费CPU的线程 top -Hp pid 转为十六进制printf &quot;%x\n&quot; 线程id 输出进程的堆栈信息，然后根据线程ID的十六进制值grepjstack pid | grep 十六进制线程id jmap查看堆内存使用状况jmap -heap pid 进程内存使用情况dump到文件中 结合MAT工具分析jmap -dump:format=b,file=dumpFileName pid jhatjhat -port 9998 /tmp/dump.datlocalhost:9998 查看内存对象情况 （不如MAT直观）]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vmstatu]]></title>
    <url>%2F2019%2F05%2F22%2Fvmstatus%2F</url>
    <content type="text"><![CDATA[vmstatuvmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。相比top，可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。 1234$vmstatprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 215220 0 771404 0 0 2 15 0 1 0 0 100 0 0 一般vmstat工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔数，单位是秒，第二个参数是采样的次数 12345$vmstat 2 2procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 202924 0 785780 0 0 2 15 0 1 0 0 100 0 0 0 0 0 203032 0 785812 0 0 0 155 748 1382 0 0 100 0 0 第二个参数如果没有，就会一直采集（ctrl+c 结束） 12345678$vmstat 2procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 194712 0 794728 0 0 2 15 0 1 0 0 100 0 0 0 0 0 194696 0 794768 0 0 0 50 782 1368 0 0 100 0 0 0 0 0 193828 0 794776 0 0 0 108 752 1156 0 0 100 0 0 0 0 0 193952 0 794804 0 0 0 4 601 997 0 0 100 0 0^C 字段procs r 等待运行的进程数 b 处在非中断睡眠状态的进程数 memory （KB） swpd 虚拟内存使用大小 注意：如果swpd的值不为0，但是SI，SO的值长期为0，这种情况不会影响系统性能。 free 空闲的内存 buff 用作缓冲的内存大小 cache 用作缓存的内存大小 注意：如果cache的值大的时候，说明cache处的文件数多，如果频繁访问到的文件都能被cache处，那么磁盘的读IO bi会非常小。 swap si 从交换区写到内存的大小 so 每秒写入交换区的内存大小 内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有些朋友看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的。 io bi 每秒读取的块数 bo 每秒写入的块数 注意：随机磁盘读写的时候，这2个值越大（如超出1024k)，能看到CPU在IO等待的值也会越大。 system in 每秒中断数，包括时钟中断。 cs 每秒上下文切换数。 注意：上面2个值越大，会看到由内核消耗的CPU时间会越大。 cpu us 用户进程执行时间(user time) 注意： us的值比较高时，说明用户进程消耗的CPU时间多，但是如果长期超50%的使用，那么我们就该考虑优化程序算法或者进行加速。 sy 系统进程执行时间(system time) 注意：sy的值高时，说明系统内核消耗的CPU资源多，这并不是良性表现，我们应该检查原因。 id 空闲时间(包括IO等待时间),中央处理器的空闲时间 。以百分比表示。 wa 等待IO时间百分比 注意：wa的值高时，说明IO等待比较严重，这可能由于磁盘大量作随机访问造成，也有可能磁盘出现瓶颈（块操作）。 1234567891011121314151617181920212223242526Procsr: The number of processes waiting for run time.b: The number of processes in uninterruptible sleep.Memoryswpd: the amount of virtual memory used.free: the amount of idle memory.buff: the amount of memory used as buffers.cache: the amount of memory used as cache.inact: the amount of inactive memory. (-a option)active: the amount of active memory. (-a option)Swapsi: Amount of memory swapped in from disk (/s).so: Amount of memory swapped to disk (/s).IObi: Blocks received from a block device (blocks/s).bo: Blocks sent to a block device (blocks/s).Systemin: The number of interrupts per second, including the clock.cs: The number of context switches per second.CPUThese are percentages of total CPU time.us: Time spent running non-kernel code. (user time, including nice time)sy: Time spent running kernel code. (system time)id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time.wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle.st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown.]]></content>
      <tags>
        <tag>java</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter plugin registry]]></title>
    <url>%2F2019%2F05%2F17%2Fflutter-plugin-registry%2F</url>
    <content type="text"><![CDATA[前言首先这里有三个形似得英文单词registry, registrar and registrant分别对应注册局，注册商和注册人。把它们翻译到现实的生活场景中的角色其实是一个“注册人通过注册商，更新注册信息后，注册商把信息传递给注册局进行保存”的过程。 注册人：GeneratedPluginRegistrant注册局：[(FlutterViewController*)rootViewController pluginRegistry] == flutterEngine注册商：FlutterEngineRegistrar Flutter Applicationflutter create -t plugin my_plugin xcode 打开 my_plugin/example/ios路径下的 Runner工程 AppDelegate12345678910@implementation AppDelegate- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions &#123; [GeneratedPluginRegistrant registerWithRegistry:self]; // Override point for customization after application launch. return [super application:application didFinishLaunchingWithOptions:launchOptions];&#125;@end GeneratedPluginRegistrant1234567@implementation GeneratedPluginRegistrant+ (void)registerWithRegistry:(NSObject&lt;FlutterPluginRegistry&gt;*)registry &#123; [MyPlugin registerWithRegistrar:[registry registrarForPlugin:@"MyPlugin"]];&#125;@end AppDelegate继承FlutterAppDelegate FlutterAppDelegate123456789#pragma mark - FlutterPluginRegistry methods. All delegating to the rootViewController- (NSObject&lt;FlutterPluginRegistrar&gt;*)registrarForPlugin:(NSString*)pluginKey &#123; UIViewController* rootViewController = _window.rootViewController; if ([rootViewController isKindOfClass:[FlutterViewController class]]) &#123; return [[(FlutterViewController*)rootViewController pluginRegistry] registrarForPlugin:pluginKey]; &#125; return nil;&#125; FlutterViewController123- (id&lt;FlutterPluginRegistry&gt;)pluginRegistry &#123; return _engine;&#125; FlutterEngineRegistrar1234567891011121314151617181920212223@implementation FlutterEngineRegistrar &#123; NSString* _pluginKey; FlutterEngine* _flutterEngine;&#125;- (instancetype)initWithPlugin:(NSString*)pluginKey flutterEngine:(FlutterEngine*)flutterEngine &#123; self = [super init]; NSAssert(self, @"Super init cannot be nil"); _pluginKey = [pluginKey retain]; _flutterEngine = [flutterEngine retain]; return self;&#125;- (void)addMethodCallDelegate:(NSObject&lt;FlutterPlugin&gt;*)delegate channel:(FlutterMethodChannel*)channel &#123; [channel setMethodCallHandler:^(FlutterMethodCall* call, FlutterResult result) &#123; [delegate handleMethodCall:call result:result]; &#125;];&#125;- (NSObject&lt;FlutterBinaryMessenger&gt;*)messenger &#123; return _flutterEngine;&#125; MyPlugin123456789101112131415161718@implementation MyPlugin+ (void)registerWithRegistrar:(NSObject&lt;FlutterPluginRegistrar&gt;*)registrar &#123; FlutterMethodChannel* channel = [FlutterMethodChannel methodChannelWithName:@"my_plugin" binaryMessenger:[registrar messenger]]; MyPlugin* instance = [[MyPlugin alloc] init]; [registrar addMethodCallDelegate:instance channel:channel];&#125;- (void)handleMethodCall:(FlutterMethodCall*)call result:(FlutterResult)result &#123; if ([@"getPlatformVersion" isEqualToString:call.method]) &#123; result([@"iOS " stringByAppendingString:[[UIDevice currentDevice] systemVersion]]); &#125; else &#123; result(FlutterMethodNotImplemented); &#125;&#125;@end FlutterMethodChannel1234567891011121314151617181920212223242526272829- (instancetype)initWithName:(NSString*)name binaryMessenger:(NSObject&lt;FlutterBinaryMessenger&gt;*)messenger codec:(NSObject&lt;FlutterMethodCodec&gt;*)codec &#123; self = [super init]; NSAssert(self, @"Super init cannot be nil"); _name = [name retain]; _messenger = [messenger retain]; //flutterEngine _codec = [codec retain]; return self;&#125;- (void)setMethodCallHandler:(FlutterMethodCallHandler)handler &#123; if (!handler) &#123; [_messenger setMessageHandlerOnChannel:_name binaryMessageHandler:nil]; return; &#125; FlutterBinaryMessageHandler messageHandler = ^(NSData* message, FlutterBinaryReply callback) &#123; FlutterMethodCall* call = [_codec decodeMethodCall:message]; handler(call, ^(id result) &#123; if (result == FlutterMethodNotImplemented) callback(nil); else if ([result isKindOfClass:[FlutterError class]]) callback([_codec encodeErrorEnvelope:(FlutterError*)result]); else callback([_codec encodeSuccessEnvelope:result]); &#125;); &#125;; [_messenger setMessageHandlerOnChannel:_name binaryMessageHandler:messageHandler];&#125; FlutterEngine123456789101112131415#pragma mark - FlutterPluginRegistry- (NSObject&lt;FlutterPluginRegistrar&gt;*)registrarForPlugin:(NSString*)pluginKey &#123; NSAssert(self.pluginPublications[pluginKey] == nil, @"Duplicate plugin key: %@", pluginKey); self.pluginPublications[pluginKey] = [NSNull null]; return [[FlutterEngineRegistrar alloc] initWithPlugin:pluginKey flutterEngine:self];&#125;- (void)setMessageHandlerOnChannel:(NSString*)channel binaryMessageHandler:(FlutterBinaryMessageHandler)handler &#123; NSAssert(channel, @"The channel must not be null"); FML_DCHECK(_shell &amp;&amp; _shell-&gt;IsSetup()); self.iosPlatformView-&gt;GetPlatformMessageRouter().SetMessageHandler(channel.UTF8String, handler);&#125; FlutterBinaryMessage123456789101112131415161718/** * A message reply callback. * * Used for submitting a binary reply back to a Flutter message sender. Also used * in for handling a binary message reply received from Flutter. * * @param reply The reply. */typedef void (^FlutterBinaryReply)(NSData* _Nullable reply);/** * A strategy for handling incoming binary messages from Flutter and to send * asynchronous replies back to Flutter. * * @param message The message. * @param reply A callback for submitting an asynchronous reply to the sender. */typedef void (^FlutterBinaryMessageHandler)(NSData* _Nullable message, FlutterBinaryReply reply);]]></content>
  </entry>
  <entry>
    <title><![CDATA[sublime text 3]]></title>
    <url>%2F2019%2F05%2F14%2Fsublime-text-3%2F</url>
    <content type="text"><![CDATA[sublime text3 install package controlTools -&gt; Install package control 报错信息工具栏View 点击show console 或者快捷键 ctrl+` 打开控制台看下如下报错信息 1234Visit https://packagecontrol.io/installation for manual instructionsError installing Package Control: HTTPS error encountered, falling back to HTTP - &lt;urlopen error [Errno 60] Operation timed out&gt;Error installing Package Control: HTTP error encountered, giving up - &lt;urlopen error [Errno 60] Operation timed out&gt;error: An error occurred installing Package Control 处理办法绑定域名 1250.116.34.243 sublime.wbond.net50.116.34.243 packagecontrol.io install package报错信息There are no packages available for installation 处理办法下载 channel_v3.json 文件 （google一下） 修改package control.sublime-settings 123&quot;channels&quot;: [ &quot;/path/to/channel_v3.json&quot;]]]></content>
  </entry>
  <entry>
    <title><![CDATA[vim shortcuts]]></title>
    <url>%2F2019%2F05%2F14%2Fvim-shortcuts%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flutter之Dart编译]]></title>
    <url>%2F2019%2F05%2F10%2FFlutter%E4%B9%8BDart%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[前言App中使用flutter已经有段时间了，最近遇到一个bug记录一下。更新flutter module工程pubspec plugin依赖，App工程中pod update之后，从功能表现上看依然是老代码。第一感觉是缓存导致的，xcode clean 以及删除DerivedData目录重新build依然不行，flutter module工程中执行flutter clean然后xcode build是正常的，所以应该是dart编译产物有缓存导致的。接下来看下dart编译过程。 编译12cd path/to/flutter moduleflutter build ios --debug --simulator 进入到flutter module工程目录 执行flutter build ios命令 12345Running Xcode build... ├─Assembling Flutter resources... 3.6s └─Compiling, linking and signing... 25.4sXcode build done. 43.9s 可以看到会进行xcode build，进到.ios目录通过xcode打开Runner工程 可以看到build phases中这样一段脚本，这里就是执行dart代码编译的入口。 xcode_backend.sh进入到脚本所在目录，看下build对应的方法 BuildApp 12345678910111213if [[ $# == 0 ]]; then # Backwards-compatibility: if no args are provided, build. BuildAppelse case $1 in "build") BuildApp ;; "thin") ThinAppFrameworks ;; "embed") EmbedFlutterFrameworks ;; esacfi 123456789101112131415161718192021222324252627282930BuildApp() &#123; ... StreamOutput " ├─Assembling Flutter resources..." RunCommand "$&#123;FLUTTER_ROOT&#125;/bin/flutter" --suppress-analytics \ $&#123;verbose_flag&#125; \ build bundle \ --target-platform=ios \ --target="$&#123;target_path&#125;" \ --$&#123;build_mode&#125; \ --depfile="$&#123;build_dir&#125;/snapshot_blob.bin.d" \ --asset-dir="$&#123;derived_dir&#125;/App.framework/$&#123;assets_path&#125;" \ $&#123;precompilation_flag&#125; \ $&#123;flutter_engine_flag&#125; \ $&#123;local_engine_flag&#125; \ $&#123;track_widget_creation_flag&#125; if [[ $? -ne 0 ]]; then EchoError "Failed to package $&#123;project_path&#125;." exit -1 fi StreamOutput "done" StreamOutput " └─Compiling, linking and signing..." RunCommand popd &gt; /dev/null echo "Project $&#123;project_path&#125; built and packaged successfully." return 0&#125; 可以看到 ├─Assembling Flutter resources… 在build ios 执行过程中出现过，flutter build bundle 就会开始真正的dart编译–depfile 指定参与编译的dart文件路径集合–asset-dir 指定资源产物的目录 flutter命令路径 $FLUTTER_ROOT/bin/flutter 123456789101112...FLUTTER_TOOLS_DIR="$FLUTTER_ROOT/packages/flutter_tools"SNAPSHOT_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.snapshot"STAMP_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.stamp"SCRIPT_PATH="$FLUTTER_TOOLS_DIR/bin/flutter_tools.dart"DART_SDK_PATH="$FLUTTER_ROOT/bin/cache/dart-sdk"DART="$DART_SDK_PATH/bin/dart"PUB="$DART_SDK_PATH/bin/pub""$DART" $FLUTTER_TOOL_ARGS "$SNAPSHOT_PATH" "$@" flutter_toools.snapshot实际上就是$FLUTTER_ROOT/packages/flutter_tools这个项目编译生成的snapshot文件 所以flutter build bundle 就是使用dart来执行flutter_tools项目的main方法 flutter_tools路径 $FLUTTER_ROOT/packages/flutter_tools main方法定义 $FLUTTER_ROOT/packages/flutter_tools/bin/flutter_tools.dart 123void main(List&lt;String&gt; args) &#123; executable.main(args);&#125; 再看看 lib/executable.dart ,在这里会预先创建好每一种命令对应的对象command，通过解析args参数找到对应的command。在BuildCommand类中 123456789BuildCommand(&#123;bool verboseHelp = false&#125;) &#123; addSubcommand(BuildApkCommand(verboseHelp: verboseHelp)); addSubcommand(BuildAppBundleCommand(verboseHelp: verboseHelp)); addSubcommand(BuildAotCommand()); addSubcommand(BuildIOSCommand()); addSubcommand(BuildFlxCommand()); addSubcommand(BuildBundleCommand(verboseHelp: verboseHelp)); addSubcommand(BuildWebCommand()); &#125; 看到BuildIOSCommand 以及 BuildBundleCommand的创建。BuildIOSCommand 就是前面提到的flutter build ios 会执行到的，这里我们重点看下BuildBundleCommand是如何编译dart代码的？编译后生成了哪些资源？这些资源都是些什么？ BuildBundleCommand app.dill : 这就是dart代码编译后的二级制文件 Frontend_server.d : 这里面放的是frontend_server.dart.snapshot的绝对路径，使用该snapshot来编译dart代码生成上面的app.dill snapshot_blob.bin.d : 这里面放的是所有参与编译的dart文件的绝对路径的集合，包括项目的代码和flutterSdk的代码以及pub库中的三方代码。 snapshot_blob.bin.d.fingerprint : 这里面放的是snapshot_blob.bin.d中的所有文件的绝对路径以及每个文件所对应的md5值。使用这个md5来判断该文件是否有修改。在每次编译的时候会判断，如果没有文件修改，则直接跳过编译。 编译Dart资源123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127Future&lt;void&gt; build(&#123; TargetPlatform platform, BuildMode buildMode, String mainPath = defaultMainPath, String manifestPath = defaultManifestPath, String applicationKernelFilePath, String depfilePath, String privateKeyPath = defaultPrivateKeyPath, String assetDirPath, String packagesPath, bool precompiledSnapshot = false, bool reportLicensedPackages = false, bool trackWidgetCreation = false, String compilationTraceFilePath, bool createPatch = false, String buildNumber, String baselineDir, List&lt;String&gt; extraFrontEndOptions = const &lt;String&gt;[], List&lt;String&gt; extraGenSnapshotOptions = const &lt;String&gt;[], List&lt;String&gt; fileSystemRoots, String fileSystemScheme,&#125;) async &#123; // xcode_backend.sh中通过--depfile传入进来的 // 默认是build/snapshot_blob.bin.d文件 depfilePath ??= defaultDepfilePath; // 通过--asset-dir传入 // 该目录中文件就是flutter的产物，最终合并到app.framework中的flutter_assets目录 assetDirPath ??= getAssetBuildDirectory(); packagesPath ??= fs.path.absolute(PackageMap.globalPackagesPath); // app.dill dart代码编译后的二级制文件 applicationKernelFilePath ??= getDefaultApplicationKernelPath(trackWidgetCreation: trackWidgetCreation); final FlutterProject flutterProject = await FlutterProject.current(); if (compilationTraceFilePath != null) &#123; if (buildMode != BuildMode.dynamicProfile &amp;&amp; buildMode != BuildMode.dynamicRelease) &#123; compilationTraceFilePath = null; &#125; else if (compilationTraceFilePath.isEmpty) &#123; // Disable JIT snapshotting if flag is empty. printStatus('Code snapshot will be disabled for this build.'); compilationTraceFilePath = null; &#125; else if (!fs.file(compilationTraceFilePath).existsSync()) &#123; // Be forgiving if compilation trace file is missing. printStatus('No compilation trace available. To optimize performance, consider using --train.'); final File tmp = fs.systemTempDirectory.childFile('flutterEmptyCompilationTrace.txt'); compilationTraceFilePath = (tmp..createSync(recursive: true)).path; &#125; else &#123; printStatus('Code snapshot will use compilation training file $compilationTraceFilePath.'); &#125; &#125; DevFSContent kernelContent; if (!precompiledSnapshot) &#123; if ((extraFrontEndOptions != null) &amp;&amp; extraFrontEndOptions.isNotEmpty) printTrace('Extra front-end options: $extraFrontEndOptions'); ensureDirectoryExists(applicationKernelFilePath); final KernelCompiler kernelCompiler = await kernelCompilerFactory.create(flutterProject); // 编译dart代码，生成app.dill 和 snapshot_blob.bin.d 以及 snapshot_blob.bin.d.fingerprint final CompilerOutput compilerOutput = await kernelCompiler.compile( sdkRoot: artifacts.getArtifactPath(Artifact.flutterPatchedSdkPath), incrementalCompilerByteStorePath: compilationTraceFilePath != null ? null : fs.path.absolute(getIncrementalCompilerByteStoreDirectory()), mainPath: fs.file(mainPath).absolute.path, outputFilePath: applicationKernelFilePath, depFilePath: depfilePath, trackWidgetCreation: trackWidgetCreation, extraFrontEndOptions: extraFrontEndOptions, fileSystemRoots: fileSystemRoots, fileSystemScheme: fileSystemScheme, packagesPath: packagesPath, linkPlatformKernelIn: compilationTraceFilePath != null, ); if (compilerOutput?.outputFilename == null) &#123; throwToolExit('Compiler failed on $mainPath'); &#125; kernelContent = DevFSFileContent(fs.file(compilerOutput.outputFilename)); // 生成 frontend_server.d文件，向文件中写入frontendServerSnapshotForEngineDartSdk的路径 await fs.directory(getBuildDirectory()).childFile('frontend_server.d') .writeAsString('frontend_server.d: $&#123;artifacts.getArtifactPath(Artifact.frontendServerSnapshotForEngineDartSdk)&#125;\n'); if (compilationTraceFilePath != null) &#123; final JITSnapshotter snapshotter = JITSnapshotter(); final int snapshotExitCode = await snapshotter.build( platform: platform, buildMode: buildMode, mainPath: applicationKernelFilePath, outputPath: getBuildDirectory(), packagesPath: packagesPath, compilationTraceFilePath: compilationTraceFilePath, extraGenSnapshotOptions: extraGenSnapshotOptions, createPatch: createPatch, buildNumber: buildNumber, baselineDir: baselineDir, ); if (snapshotExitCode != 0) &#123; throwToolExit('Snapshotting exited with non-zero exit code: $snapshotExitCode'); &#125; &#125; &#125; // 生成 flutter_assets final AssetBundle assets = await buildAssets( manifestPath: manifestPath, assetDirPath: assetDirPath, packagesPath: packagesPath, reportLicensedPackages: reportLicensedPackages, ); if (assets == null) throwToolExit('Error building assets', exitCode: 1); await assemble( buildMode: buildMode, assetBundle: assets, kernelContent: kernelContent, privateKeyPath: privateKeyPath, assetDirPath: assetDirPath, compilationTraceFilePath: compilationTraceFilePath, );&#125; 编译Dart代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class KernelCompiler &#123; const KernelCompiler(); Future&lt;CompilerOutput&gt; compile(&#123; String sdkRoot, String mainPath, String outputFilePath, String depFilePath, TargetModel targetModel = TargetModel.flutter, bool linkPlatformKernelIn = false, bool aot = false, @required bool trackWidgetCreation, List&lt;String&gt; extraFrontEndOptions, String incrementalCompilerByteStorePath, String packagesPath, List&lt;String&gt; fileSystemRoots, String fileSystemScheme, bool targetProductVm = false, String initializeFromDill, &#125;) async &#123; final String frontendServer = artifacts.getArtifactPath( Artifact.frontendServerSnapshotForEngineDartSdk ); FlutterProject flutterProject; if (fs.file('pubspec.yaml').existsSync()) &#123; flutterProject = await FlutterProject.current(); &#125; // TODO(cbracken): eliminate pathFilter. // Currently the compiler emits buildbot paths for the core libs in the // depfile. None of these are available on the local host. Fingerprinter fingerprinter; // 如果snapshot_blob.bin.d文件不为空，则说明有编译缓存 if (depFilePath != null) &#123; // 判断与上次编译对比，是否有文件的md5改变 fingerprinter = Fingerprinter( // snapshot_blob.bin.d.fingerprint文件 fingerprintPath: '$depFilePath.fingerprint', paths: &lt;String&gt;[mainPath], properties: &lt;String, String&gt;&#123; 'entryPoint': mainPath, 'trackWidgetCreation': trackWidgetCreation.toString(), 'linkPlatformKernelIn': linkPlatformKernelIn.toString(), 'engineHash': Cache.instance.engineRevision, 'buildersUsed': '$&#123;flutterProject != null ? flutterProject.hasBuilders : false&#125;', &#125;, depfilePaths: &lt;String&gt;[depFilePath], pathFilter: (String path) =&gt; !path.startsWith('/b/build/slave/'), ); // 判断是否有文件改动，如果没有，则直接返回。 if (await fingerprinter.doesFingerprintMatch()) &#123; printTrace('Skipping kernel compilation. Fingerprint match.'); return CompilerOutput(outputFilePath, 0, /* sources */ null); &#125; &#125; ... // 如果没有上次编译缓存，或者文件有改变，Fingerprinter不匹配，则使用dart重新编译 final List&lt;String&gt; command = &lt;String&gt;[ engineDartPath, frontendServer, '--sdk-root', sdkRoot, '--strong', '--target=$targetModel', ]; ... //参数拼接 final Process server = await processManager .start(command) .catchError((dynamic error, StackTrace stack) &#123; printError('Failed to start frontend server $error, $stack'); &#125;); final StdoutHandler _stdoutHandler = StdoutHandler(); server.stderr .transform&lt;String&gt;(utf8.decoder) .listen(printError); server.stdout .transform&lt;String&gt;(utf8.decoder) .transform&lt;String&gt;(const LineSplitter()) .listen(_stdoutHandler.handler); final int exitCode = await server.exitCode; if (exitCode == 0) &#123; if (fingerprinter != null) &#123; await fingerprinter.writeFingerprint(); &#125; return _stdoutHandler.compilerOutput.future; &#125; return null; &#125;&#125; Fingerprint对比1234567891011121314151617181920212223242526Future&lt;bool&gt; doesFingerprintMatch() async &#123; try &#123; // 获取到当前的 snapshot_blob.bin.d.fingerprint文件 final File fingerprintFile = fs.file(fingerprintPath); if (!fingerprintFile.existsSync()) return false; if (!_depfilePaths.every(fs.isFileSync)) return false; final List&lt;String&gt; paths = await _getPaths(); if (!paths.every(fs.isFileSync)) return false; // 读取缓存的的snapshot_blob.bin.d.fingerprint文件，构建一个老的Fingerprint对象 final Fingerprint oldFingerprint = Fingerprint.fromJson(await fingerprintFile.readAsString()); // 构建一个新的Fingerprint对象 final Fingerprint newFingerprint = await buildFingerprint(); // 对比两次的文件集合中的每个文件的md5是否一样 return oldFingerprint == newFingerprint; &#125; catch (e) &#123; // Log exception and continue, fingerprinting is only a performance improvement. printTrace('Fingerprint check error: $e'); &#125; return false; &#125; 重点 看看 newFingerprint 12345678910111213Future&lt;Fingerprint&gt; buildFingerprint() async &#123; final List&lt;String&gt; paths = await _getPaths(); return Fingerprint.fromBuildInputs(_properties, paths);&#125;Future&lt;List&lt;String&gt;&gt; _getPaths() async &#123; final Set&lt;String&gt; paths = _paths.toSet(); // 使用缓存的snapshot_blob.bin.d文件中的文件集合 for (String depfilePath in _depfilePaths) paths.addAll(await readDepfile(depfilePath)); final FingerprintPathFilter filter = _pathFilter ?? (String path) =&gt; true; return paths.where(filter).toList()..sort();&#125; 可以看到newFingerprint 路径依旧是使用缓存的路径，依次计算路径对应文件的md5，所以问题就在这里了 执行flutter packages upgrade更新pub依赖的时候，build目录下的缓存产物并不会有任何变动，路径依然是老的路径。有一种情况就是module工程 lib 目录下的dat文件有改动，newFingerprint就会跟old不一样，这会重新编译dart，这里又有一个问题，就是如果lib目录下是新增dart文件 则不会被编译进去。 最后综上，执行flutter clean命令，清空build目录缓存文件，build ios 就会重新编译整个dart文件，包括pub依赖中的。 参考链接Flutter深入之flutter-build-bundle命令如何编译Dart?]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
</search>
