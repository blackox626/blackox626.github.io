<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[形状特效]]></title>
    <url>%2F2022%2F10%2F14%2F%E5%BD%A2%E7%8A%B6%E7%89%B9%E6%95%88%2F</url>
    <content type="text"><![CDATA[形状特效初次接触到形状特效是在剪映，剪映中叫蒙版，就是列举了一些常见形状的蒙层效果，除了常规的旋转、缩放、位移等操作外，还有个虚化的效果。可以去剪映感受下。 接触到这东西，觉得效果挺不错，打算研究下，正好手头的编辑工具项目也可以用上。线性、圆形、矩形，这几个规范的，通过平方根公式 跟 三角形余弦定理基本都可以推算出来(可以参考github shader)，重点来了，爱心 &amp; 五角星 怎么搞？ 特殊形状爱心 、五角星，一开始觉得可能是通过 三角函数、关键点、贝塞尔曲线 等数学计算实现的？ 想想觉得这样是不是过于复杂， 一翻google之后，还是没啥思路。放大招抓包，剪映的素材都是下发的，必然会有网络请求的，一顿操作之后，如愿拿到了资源包。 爱心 五角星 看到了这两个，很明显不是通过复杂数学计算得到的。 但是这一层一层的渐变红绿图 是什么鬼？资源包里还有shader文件的😄 float alpha = (col.r * 4.0 * 256.0 + col.g * 255.0) / 781.0; 重点的就是这行计算逻辑，还是懵的。 col.r col.g 对应图上的红绿分量可以理解*4 /781 是什么鬼？ 4 对应 图上的四层? 用颜色取色器取色看看，这一看就明白了。 1234567第1层c00d00 第2层800000~80ff00 第3层400000~40ff00 第4层000000~00ff00R * 4 + G 算下来 从 0 到 781 781 = 12(c0) * 16 * 4 + 13(0d) 每层红色分量固定(00/40/80/c0)，绿色分量渐变0到255(00-ff)，所以整个图层解析下来就是 从0 到 781. alpha 计算下来就有 256 *3 + 1 = 769 个值，最里面的一层最小的，固定为1，保障无虚化的时候也有最基本的形状效果。 搞明白之后，顿时觉得秒啊， 回头一想，为什么要搞的那么多复杂呢？ 反正是为了计算alpha，直接搞个灰度图 从黑到白，不就好了？仔细一想，灰度图一个分量最多256个，最终的效果割裂感会很明显，所以用到了r、g 两个分量，让效果更丝滑，实际上如果你想 也可以用rbg三个分量，重新设计下计算公式 让范围更大更丝滑。 举一反三学以致用，那必须举一反三下了。 花型 用五角星图去找设计师，参考一下做一个花型的。设计师也一下没掌握精髓，照着搞了一个但不是我想要的。所以有了人生第一次指导设计师画图了，一翻操作之后 搞定了 最后的shader 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#iUniform float iReverse = 0.#iUniform float _X = 0.5#iUniform float _Y = 0.5#iUniform float _S = 0.25#iUniform float _A = 0.#iUniform float _F = 0.00#iUniform float iPlatform_iOS = 1.0const float PI = 3.1415926;#iChannel0 &quot;file://followw813654.jpeg&quot;#iChannel1 &quot;file://mask1.png&quot;vec2 rotation(vec2 uv, float angle, float ratio)&#123; vec2 center = vec2(0.5, 0.5); mat2 zRotation = mat2(cos(angle), sin(angle), -sin(angle) * ratio, cos(angle) * ratio); vec2 centeredPoint = uv - center; vec2 newUv = zRotation * centeredPoint; return vec2(newUv.x, newUv.y / ratio) + center;&#125;vec2 scale(vec2 uv, vec2 scale)&#123; vec2 newPos = vec2(0.5) + (uv - vec2(0.5)) / scale; return newPos;&#125;vec2 offset(vec2 uv, vec2 offset)&#123; return uv + offset;&#125;void main()&#123; highp vec2 textureCoordinate = gl_FragCoord.xy/iResolution.xy; float radio = iResolution.x / iResolution.y; bool ls = (radio &gt; 1.0); vec4 base = texture(iChannel0, textureCoordinate); float _Radio = 1.0; float _Scale = _S * 5.; _Scale = ls ? _Scale / radio : _Scale; float _Angle = 360. * PI / 180. * _A; vec2 _Offset = vec2(_X * 2.0 - 1.0, (_Y * 2.0 - 1.0)); vec2 newUV = offset(textureCoordinate, vec2(-_Offset.x, _Offset.y)); newUV = scale(newUV, vec2(1. * _Scale, radio * _Scale)); newUV = rotation(newUV, _Angle, _Radio); newUV.y = (iPlatform_iOS == 1.) ? newUV.y : (1. - newUV.y); vec4 mask = texture(iChannel1, newUV) * step(newUV.x, 1.) * step(newUV.y, 1.) * step(0., newUV.x) * step(0., newUV.y); vec2 col = mask.rg; // 怎么理解 ？ 根据 mask图的 作图颜色搭配 动态调整 // 第1层c00d00 第2层800000~80ff00 第3层400000~40ff00 第4层000000~00ff00 // R * 4 + G 算下来 从 0 到 781 // 781 = 12 * 16 * 4 + 13 float alpha = (col.r * 4.0 * 256.0 + col.g * 255.0) / 781.0; alpha = smoothstep(0.49 - abs(sin(iTime/2.)), 0.51 + abs(sin(iTime/2.)), alpha); if (iReverse &gt; .5) &#123; alpha = 1.0 - alpha; &#125; gl_FragColor = mix(vec4(0, 0, 0, alpha), base, alpha);&#125; 完整效果]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态滤镜]]></title>
    <url>%2F2022%2F10%2F14%2F%E5%8A%A8%E6%80%81%E6%BB%A4%E9%95%9C%2F</url>
    <content type="text"><![CDATA[滤镜音视频编辑工具中，对图像或者视频加滤镜是一种常见的做法，普通滤镜很好理解，一张LUT图就搞定了，LUT图一般设计师会提供好，开发拿过来转成纹理，opengl做下颜色转换就好了，这里就不多说了。 但是今天想讲的是动态滤镜，也有叫特效的，动效等等。比如小红书/剪映 等等,感受一下就大概知道了。 动态滤镜先上一个效果感受下： 视频： 很明显动态滤镜跟普通滤镜根本上就不是一回事了，所以更像是动效 特效了，这里不纠结叫什么，重点是关注下怎么做的呢？直观感受就是静态图片上覆盖了一帧帧的透明图片，确实就是这么一回事。 用一帧帧的带alpha通道的图片集，通过opengl blend一帧帧融合也可以实现， 弊端很明显，虽然这种动态效果一般也只有几秒，按30fps，也需要百来张图，图片资源很大，gif格式也会存在同样的问题。如果用视频呢？ 但是视频没有alpha通道。。 方案参考腾讯动画方案 vap 我们用一种特殊的视频，视频的一半表示rgb 另一半表示alpha通道。 表示alpha通道的 rgb 三个分量值一样，所以是灰度图效果，知道这种特殊的视频构成 接下来就好处理了 123456789101112( precision mediump float; varying highp vec2 textureCoordinate; uniform sampler2D inputImageTexture; void main() &#123; vec4 textureColor = texture2D(inputImageTexture,textureCoordinate); vec4 leftColor = texture2D(inputImageTexture,vec2(textureCoordinate.x / 2.0,textureCoordinate.y)); vec4 rightColor = texture2D(inputImageTexture,vec2(textureCoordinate.x / 2.0 + 0.5,textureCoordinate.y)); gl_FragColor = vec4(leftColor.rgb * rightColor.b ,rightColor.b); &#125;); 实际应用中，借用GPUImageMoive来解码一半一半的视频，然后自定义filter，fragmentshader 就是上面的这段，最后渲染到GPUImageView上，就达到了在静态图片上播放透明视频的效果。 12345678910111213141516171819@interface GPUImageCustomFilter : GPUImageFilter@end@implementation GPUImageCustomFilter- (id)init &#123; if (self = [super initWithFragmentShaderFromString:kGPUImageCustomFragmentShaderString]) &#123; &#125; return self;&#125;/// size width / 2- (void)setInputSize:(CGSize)newSize atIndex:(NSInteger)textureIndex &#123; [super setInputSize:CGSizeMake(newSize.width / 2.0, newSize.height) atIndex:textureIndex];&#125;@end 使用customFilter 123456789101112131415161718NSURL *url = [NSURL fileURLWithPath:videoPath];CGRect frame = previewView.bounds;GPUImageView *gpuImageView = [[GPUImageView alloc] init];gpuImageView.backgroundColor = [UIColor clearColor];gpuImageView.fillMode = kGPUImageFillModePreserveAspectRatioAndFill;gpuImageView.frame = frame;GPUImageMovie *movie = [[GPUImageMovie alloc] initWithURL:url];movie.shouldRepeat = YES;movie.playAtActualSpeed = YES;movie.runBenchmark = YES;GPUImageCustomFilter *customFilter = [[GPUImageCustomFilter alloc] init];[movie addTarget:customFilter];[customFilter addTarget:gpuImageView];[movie startProcessing];]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像描边]]></title>
    <url>%2F2022%2F09%2F10%2F%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[image border最近项目中搞了个图像描边的需求，常见的美图工具App都有类似的功能，典型的如美图秀秀，一开始觉得应该不太复杂，正常评估时间，实际做的时候，发现问题比想象中的复杂多了，结果项目不得不延期 😭，所以有必要搞篇文章来总结下教训。 先说整体的流程： 1、原图 -&gt; 2、抠图 -&gt; 3、边缘检测 -&gt; 4、绘制边缘 -&gt; 5、结果导出 这个流程还是很容易想到，但是除了最后一步相对来说容易点，2、3、4都是一路坑 😞 image matting首先是抠图，就是这样的 跟我们这边的算法同学对接，爬虫收集图像、标注、模型训练 一套组合下来，效果不理想，生产环境不可用，第一步就卡住了 😞 为了赶项目周期，最后使用了阿里云的方案，这里就不多说了，算法同学持续优化模型，待成熟之后替换阿里云。 Edge detection这一步相对来说是最复杂的，这里遇到的问题也是最大，耗时最久 最初的方案大致是这样的：抠图结果-&gt;采样缩放图像-&gt;遍历图像bitmap取满足条件的点，条件简单的理解就是点周围3x3范围的点像素值取平均值。为什么要检测边缘，是因为要做虚线描边，获取连续的边缘点之后然后在画布上连接点画出来。 代码大概是这样的… 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647+ (NSArray *)imageFindContours:(UIImage *)image &#123; NSMutableArray *points = [[NSMutableArray array] init]; UIImage *newImage = [image mediumResolution:CGSizeMake(30, 30)]; CFDataRef imageData = CGDataProviderCopyData(CGImageGetDataProvider(newImage.CGImage)); const uint8_t *data = CFDataGetBytePtr(imageData); int w = newImage.size.width; int h = newImage.size.height; unsigned char *bitmap = malloc(w * h * 4); memcpy(bitmap, data, w * h * 4); CGFloat leftmost = 1; CGFloat rightmost = 0; for (int i = 1; i &lt; h - 1; i += 2) &#123; for (int j = 1; j &lt; w - 1; j += 2) &#123; unsigned int left = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, 0)]; unsigned int right = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, 0)]; unsigned int up = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, -1)]; unsigned int down = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, 1)]; unsigned int leftUp = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, -1)]; unsigned int rightUp = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, -1)]; unsigned int leftDown = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, 1)]; unsigned int rightDown = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, 1)]; unsigned int center = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, 0)]; unsigned int avg = (left + right + up + down + leftUp + rightUp + leftDown + rightDown + center) / 9; int offset = i * w + j; if ((avg &gt;= (255. * 0.4) &amp;&amp; avg &lt;= (255. * 0.9)) &amp;&amp; center &gt; 65) &#123; bitmap[offset * 4] = 255; bitmap[offset * 4 + 1] = 0; bitmap[offset * 4 + 2] = 0; bitmap[offset * 4 + 3] = 255; CGFloat scale = 1.15; CGFloat x = (CGFloat)((((float) j / w) - 0.5) * scale + 0.5); CGFloat y = (CGFloat)((((float) i / h) - 0.5) * scale + 0.5); CGPoint point = CGPointMake(x, y); [points addObject:[NSValue valueWithCGPoint:point]]; if (x &lt;= leftmost) leftmost = x; if (x &gt;= rightmost) rightmost = x; &#125; &#125; &#125; ...&#125; 这个有个致命的问题，就是找到点之后，但是没办法有序的连起来，也试过一些方案，但是图像的边缘情况太复杂了，总是有问题，这里就不多说了。 接下来就需求其他的方案，大名鼎鼎的OpenCV出场了，参考官方文档，编译产物，接入app 调试下来就能获取正确的结果了，这里就不多说了，直接看下代码，对应的节点有注释说明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859+ (NSArray *)findContours:(UIImage *)image &#123; Mat src; Mat src_gray; src = [self cvMatFromUIImage:image]; cvtColor(src, src_gray, COLOR_BGR2GRAY); //UIImage *grayImg = [self UIImageFromCVMat:src_gray]; blur(src_gray, src_gray, cv::Size(3, 3)); //UIImage *blurgrayImg = [self UIImageFromCVMat:src_gray]; /// 利用阈值二值化 threshold(src_gray,src_gray,128,255,cv::THRESH_BINARY); /// 用Canny算子检测边缘 //Canny(src_gray, src_gray, 128, 255 , 3); //UIImage *canny_outputImg = [self UIImageFromCVMat:src_gray]; vector&lt;vector&lt;cv::Point&gt; &gt; contours; vector&lt;Vec4i&gt; hierarchy; /// 寻找轮廓 findContours(src_gray, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, cv::Point(0, 0)); /// 绘出轮廓 Mat drawing = Mat::zeros(src_gray.size(), CV_8UC3); for (int i = 0; i &lt; contours.size(); i++) &#123; Scalar color = Scalar(255, 255, 255); drawContours(drawing, contours, i, color, 1, 8, hierarchy, 0, cv::Point()); &#125; //UIImage *contoursImg = [self UIImageFromCVMat:drawing]; NSMutableArray *array = [NSMutableArray arrayWithCapacity:contours.size()]; for (int i = 0; i &lt; contours.size(); i++) &#123; if (hierarchy[i][3] &gt;= 0 || hierarchy[i][2] &gt;= 0) &#123; continue; &#125; vector&lt;cv::Point&gt; vect = contours[i]; std::vector&lt;cv::Point&gt;::const_iterator it; // declare a read-only iterator it = vect.cbegin(); // assign it to the start of the vector while (it != vect.cend()) &#123; // while it hasn't reach the end //std::cout &lt;&lt; it-&gt;x &lt;&lt;' '&lt;&lt; it-&gt;y &lt;&lt;' '; // print the value of the element it points to [array addObject:@(CGPointMake(it-&gt;x / image.size.width, it-&gt;y / image.size.height))]; ++it; // and iterate to the next element &#125; &#125; return @[array];&#125; 这个方案唯一的缺陷就是需要引入OpenCV静态库，增加包大小，也想过咱只用到了边缘检测，其他的牛逼功能暂时也用不到，裁剪下只保留需要的类是不是就可以，但是大致翻了下，牵扯的太多，最终放弃了, 最后也并没有使用OpenCV的方案。 因为发现了更轻量级的方案，Suzuki边缘检测算法，后面也了解该算法其实就是OpenCV内部的一种边缘检测方案。 恰好Android同学找到了一个开源库，java版本的Suzuki边缘检测算法。代码拉下来结合算法文档来回撸几遍，大致能理解了，android直接java拖进去用上，ios翻译成OC，也不复杂，因为算法核心方法也不过几十行代码，就算不理解，硬翻也能翻译过来。 贴一下翻译成OC的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212+ (NSArray *)findContours:(UIImage *)img threshold:(CGFloat)threshold &#123; img = [self blurImage:img blur:0.2]; /// 记录原图尺寸 int ow = (int) img.size.width; int w = ow; int h = (int) img.size.height; /// 考虑字节对齐 w 要重新计算 CFDataRef imageData = CGDataProviderCopyData(CGImageGetDataProvider(img.CGImage)); const uint8_t *data = CFDataGetBytePtr(imageData); w = (int) CFDataGetLength(imageData) / (h*4); char *F = malloc((size_t) CFDataGetLength(imageData)/4); /// 二值化处理 threshold *= 255.f; for (int i = 0; i &lt; h; i++) &#123; for (int j = 0; j &lt; w; j++) &#123; if (data[(i * w + j) * 4] &gt; threshold) &#123; F[i * w + j] = 1; &#125; else &#123; F[i * w + j] = 0; &#125; &#125; &#125; NSMutableArray&lt;Contour *&gt; *contours = [NSMutableArray array]; for (int i = 1; i &lt; h - 1; i++) &#123; F[i * w] = 0; F[i * w + w - 1] = 0; &#125; for (int i = 0; i &lt; w; i++) &#123; F[i] = 0; F[w * h - 1 - i] = 0; &#125; int nbd = 1; int lnbd = 1; for (int i = 1; i &lt; h - 1; i++) &#123; lnbd = 1; for (int j = 1; j &lt; w - 1; j++) &#123; int i2 = 0, j2 = 0; if (F[i * w + j] == 0) &#123; continue; &#125; //(a) If fij = 1 and fi, j-1 = 0, then decide that the pixel //(i, j) is the border following starting point of an outer //border, increment NBD, and (i2, j2) &lt;- (i, j - 1). if (F[i * w + j] == 1 &amp;&amp; F[i * w + (j - 1)] == 0) &#123; nbd++; i2 = i; j2 = j - 1; //(b) Else if fij &gt;= 1 and fi,j+1 = 0, then decide that the //pixel (i, j) is the border following starting point of a //hole border, increment NBD, (i2, j2) &lt;- (i, j + 1), and //LNBD + fij in case fij &gt; 1. &#125; else if (F[i * w + j] &gt;= 1 &amp;&amp; F[i * w + j + 1] == 0) &#123; nbd++; i2 = i; j2 = j + 1; if (F[i * w + j] &gt; 1) &#123; lnbd = F[i * w + j]; &#125; &#125; else &#123; //(c) Otherwise, go to (4). //(4) If fij != 1, then LNBD &lt;- |fij| and resume the raster //scan from pixel (i,j+1). The algorithm terminates when the //scan reaches the lower right corner of the picture if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; continue; &#125; //(2) Depending on the types of the newly found border //and the border with the sequential number LNBD //(i.e., the last border met on the current row), //decide the parent of the current border as shown in Table 1. // TABLE 1 // Decision Rule for the Parent Border of the Newly Found Border B // ---------------------------------------------------------------- // Type of border B' // \ with the sequential // \ number LNBD // Type of B \ Outer border Hole border // --------------------------------------------------------------- // Outer border The parent border The border B' // of the border B' // // Hole border The border B' The parent border // of the border B' // ---------------------------------------------------------------- Contour *B = [Contour new]; B.points = [NSMutableArray array]; [B.points addObject:[NSValue valueWithCGPoint:CGPointMake(j * 1.f / ow, i * 1.f / h)]]; B.isHole = (j2 == (j + 1)); B.idx = nbd; [contours addObject:B]; Contour *B0 = [Contour new]; for (int c = 0; c &lt; contours.count; c++) &#123; if (contours[c].idx == lnbd) &#123; B0 = contours[c]; break; &#125; &#125; if (B0.isHole) &#123; if (B.isHole) &#123; B.parentIdx = B0.parentIdx; &#125; else &#123; B.parentIdx = lnbd; &#125; &#125; else &#123; if (B.isHole) &#123; B.parentIdx = lnbd; &#125; else &#123; B.parentIdx = B0.parentIdx; &#125; &#125; //(3) From the starting point (i, j), follow the detected border: //this is done by the following substeps (3.1) through (3.5). //(3.1) Starting from (i2, j2), look around clockwise the pixels //in the neigh- borhood of (i, j) and tind a nonzero pixel. //Let (i1, j1) be the first found nonzero pixel. If no nonzero //pixel is found, assign -NBD to fij and go to (4). int i1j1[2] = &#123;-1, -1&#125;; cwNon0(F, w, h, i, j, i2, j2, 0, i1j1); if (i1j1[0] == -1 &amp;&amp; i1j1[1] == -1) &#123; F[i * w + j] = -nbd; //go to (4) if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; continue; &#125; int i1 = i1j1[0]; int j1 = i1j1[1]; // (3.2) (i2, j2) &lt;- (i1, j1) ad (i3,j3) &lt;- (i, j). i2 = i1; j2 = j1; int i3 = i; int j3 = j; while (true) &#123; //(3.3) Starting from the next elementof the pixel (i2, j2) //in the counterclock- wise order, examine counterclockwise //the pixels in the neighborhood of the current pixel (i3, j3) //to find a nonzero pixel and let the first one be (i4, j4). int i4j4[2] = &#123;-1, -1&#125;; ccwNon0(F, w, h, i3, j3, i2, j2, 1, i4j4); int i4 = i4j4[0]; int j4 = i4j4[1]; [contours[contours.count - 1].points addObject:[NSValue valueWithCGPoint:CGPointMake(j4 * 1.f / ow, i4 * 1.f / h)]]; //(a) If the pixel (i3, j3 + 1) is a O-pixel examined in the //substep (3.3) then fi3, j3 &lt;- -NBD. if (F[i3 * w + j3 + 1] == 0) &#123; F[i3 * w + j3] = (char) -nbd; //(b) If the pixel (i3, j3 + 1) is not a O-pixel examined //in the substep (3.3) and fi3,j3 = 1, then fi3,j3 &lt;- NBD. &#125; else if (F[i3 * w + j3] == 1) &#123; F[i3 * w + j3] = (char) nbd; &#125; else &#123; //(c) Otherwise, do not change fi3, j3. &#125; //(3.5) If (i4, j4) = (i, j) and (i3, j3) = (i1, j1) //(coming back to the starting point), then go to (4); if (i4 == i &amp;&amp; j4 == j &amp;&amp; i3 == i1 &amp;&amp; j3 == j1) &#123; if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; break; //otherwise, (i2, j2) + (i3, j3),(i3, j3) + (i4, j4), //and go back to (3.3). &#125; else &#123; i2 = i3; j2 = j3; i3 = i4; j3 = j4; &#125; &#125; &#125; &#125; free(F); ...&#125; SDF上面提到边缘检测找连续的边缘点只是为了解决虚线描边，其他的描边情况其实是用不到这些点的，但是这里也遇到问题了。 一开始的想法跟上面通过3x3范围取平均值，通过条件过滤来做的，kernel code大概是这样 12345678910111213141516171819static NSString *KernelString = @&quot;\kernel vec4 borderDraw(sampler image, sampler mask, sampler source, vec4 rgba, float midpoint, float width) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 sumColor = vec4(0.0);\ float radiu = 2 * width;\ for (float m = -radiu; m &lt;= radiu; m += 2) &#123;\ for (float n = -radiu; n &lt;= radiu; n += 2) &#123;\ vec4 rgba = sample(image, samplerTransform(image, vec2(uv.x + m, uv.y + n)));\ sumColor += rgba;\ &#125;\ &#125;\ float avg = sumColor.a / float(radiu * radiu / 2.0);\ if (color.a &lt; 1.0 &amp;&amp; (avg &gt; .05 &amp;&amp; avg &lt; 1.)) &#123;\ return rgba;\ &#125;\ return color;\&#125;&quot;; 这套方案做demo的时候，感觉效果还行，一点点毛疵，以为是条件判断不严谨，以为后续调整下可以解决，还有个严重的问题，就是这个方案计算量太大，图片分辨率1080左右，表现就有点卡，尤其是拖动滑竿调整，描边粗细 、间距 ，实时渲染有明显的卡顿，但是我们又不能降低图片质量，所以这个方案最终也就是停留在demo阶段了。 因为计算量太大，所以想办法降低像素计算量，SDF出场了，通过距离场，可以生成一张图，这张图可以告知像素边界信息，直接通过边界信息，省去了极大的计算量。 SDF ：signed distance filed 有向距离场sdf有两种方式，一种是循环（横向x纵向） 一种是双线性（横向+纵向），很明显前一种计算量远远大于后一种，联调下来第二种方案实际效果也是相当不错了。 这里还要考虑一个问题，因为描边是有粗细跟间距的，所以可以通过调整距离参数生成图，很好的解决了描边粗细跟间距问题的，SDF方案在虚线描边的case也是有用的，通过把SDF生成图拿去做边缘检测找连续点。 来看下SDF生成图的效果 抠图横向SDF结果接纵向SDF结果 贴一下Metal版本的SDF计算逻辑 横向SDF 12345678910111213141516171819202122232425262728293031323334353637383940extern "C" &#123; namespace coreimage &#123; constant float threshold = 0.5; float source(sampler image,float2 uv) &#123; return image.sample(image.transform(uv)).a - threshold; &#125; float4 sdfhor(sampler image,float width,destination dest) &#123; float D2 = width * 2.0 + 1.0; // 获取当前点坐标 float2 uv = dest.coord(); float s = sign(source(image,uv)); float d = 0.; for(int i= 0; i &lt; width; i++) &#123; d ++; float sp = sign(source(image,float2(uv.x + d, uv.y))); if(s * sp &lt; 0.) &#123; break; &#125; sp = sign(source(image,float2(uv.x - d, uv.y))); if(s * sp &lt; 0.) &#123; break; &#125; &#125; float sd = -s * d / D2 ; return float4(float3(sd),1.0); &#125; &#125;&#125; 纵向SDF 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758extern "C" &#123; namespace coreimage &#123; float sd(sampler image,float2 uv,float width) &#123; float D2 = float(width * 2 + 1); float x = image.sample(image.transform(uv)).x; return x * D2; &#125; float4 sdf(sampler image,float width,destination dest) &#123; // 获取当前点坐标 float2 uv = dest.coord(); float dx = sd(image,uv,width); float dMin = abs(dx); float dy = 0.0; for(int i= 0; i &lt; width; i++)&#123; dy += 1.0; float2 offset = float2(0.0, dy); float dx1 = sd(image,uv+offset,width); //sign switch if(dx1 * dx &lt; 0.)&#123; dMin = dy; break; &#125; dMin = min(dMin, length (float2(dx1, dy))); float dx2 = sd(image,uv-offset,width); //sign switch if(dx2 * dx &lt; 0.)&#123; dMin = dy; break; &#125; dMin = min(dMin, length (float2(dx2, dy))); if(dy &gt; dMin)break; &#125; float D2 = float(width * 2 + 1); dMin *= sign(dx); float d = dMin/D2; d = 1.0 - d; d = smoothstep(0.5 ,1.0, d); return float4(float3(d),1.0); &#125; &#125;&#125; border有了距离场，描边的工作就一下子简单多了。 目前实现的五种描边效果就是这样式的 项目中使用coreimage自定义kernel做的，当然也可以metal搞定 sdfsourceKernelString 对应上面第三个效果 12345678910111213141516171819202122232425262728293031static NSString *sdfsourceKernelString = @"\kernel vec4 borderDraw(sampler image, sampler sdf, sampler source, float d1, float d2) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 sdfcolor = sample(sdf, samplerTransform(sdf, uv));\ vec4 sourcecolor = sample(source, samplerTransform(source, uv));\ if (sdfcolor.x &gt;= d1 &amp;&amp; sdfcolor.x &lt;= d2) &#123;\ return sourcecolor;\ &#125;\ return color;\&#125;";static NSString *sdfKernelString = @"\kernel vec4 borderDraw(sampler image, sampler sdf, sampler source, vec4 rgba, vec2 offset, float d1, float d2) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 offsetColor = sample(image, samplerTransform(image, uv-offset));\ vec4 sdfcolor = sample(sdf, samplerTransform(sdf, uv));\ vec4 sourcecolor = sample(source, samplerTransform(source, uv));\ if(offset.x != 0.0 || offset.y != 0.0) &#123;\ if(color.a &lt; 0.5 &amp;&amp; offsetColor.a &gt; 0.5 ) &#123;\ return mix(rgba,color,color.a);\ &#125;\ &#125; else if (sdfcolor.x &gt;= d1 &amp;&amp; sdfcolor.x &lt;= d2) &#123;\ return mix(rgba,sourcecolor,offsetColor.a);\ &#125;\ return color;\&#125;"; 最后一个虚线描边就是常规的连接边缘点安排画布绘制，然后跟抠图做一个合并导出，就不展开说了。 Othter最后还有一些注意点，比如抠图图像是在边缘，则需要考虑下预留描边空间，判断是否有落在边缘，如果有则补充点空间。还有一个就是最小包围盒，抠图很可能只占据原图的一部分预期，为了展示效果，需要把抠图的最小包围盒找到，找这个最小包围盒，不需要那么精确，找出一个差不多的最小矩形框就行，项目上用的就是粗暴的像素遍历，找四个角的位置就可以了，通过最小包围盒，也能判断抠图是否靠近边缘。 The Last综上，关键的几个步骤基本都尝试了多种方式，分析比较得出最合适项目需求的技术方案， 最终从性能、体验等维度拿到相对不错的结果，单纯从描边功能上来说，对比修复工具也不输 O(∩_∩)O哈哈~ referencePContourSDF双线性SDF]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[camera orientation]]></title>
    <url>%2F2022%2F09%2F09%2Fcamera-orientation%2F</url>
    <content type="text"><![CDATA[camera手机相机录视频或者拍照，需要考虑设备的方向，主要在两方面 采集过程中，无论手机什么方向采集，采集画面都是正着看，不用侧头 录制/或者拍照后，视频/图片在正常手持设备的情况下正常显示，不用横着手机 正常手持一般都是竖着拿手机，所以就算横着拍摄，竖着拿手机也需要正常显示，不理解可以参考下系统相机。 所以这里就存在一个方向修正的问题。 device orientation苹果手机，如果设置里锁定了方向，是没办法通过 [UIDevice currentDevice].orientation这个方法拿到的，同样UIDeviceOrientationDidChangeNotification这个通知也获取不到。同理 [[UIApplication sharedApplication] statusBarOrientation]; 跟 **UIApplicationDidChangeStatusBarOrientationNotification** 也是不可用的。 可以通过 CMMotionManager 监听设备的方向 12345678910111213141516171819202122232425262728293031323334_motionManager = [[CMMotionManager alloc] init];_motionManager.deviceMotionUpdateInterval = 1/15.0;if (!_motionManager.deviceMotionAvailable) &#123; _motionManager = nil; return self;&#125;OBJC_WEAK(self)[_motionManager startDeviceMotionUpdatesToQueue:[NSOperationQueue currentQueue] withHandler: ^(CMDeviceMotion*motion, NSError *error)&#123; [weak_self performSelectorOnMainThread:@selector(handleDeviceMotion:) withObject:motion waitUntilDone:YES];&#125;];- (void)handleDeviceMotion:(CMDeviceMotion *)deviceMotion &#123; double x = deviceMotion.gravity.x; double y = deviceMotion.gravity.y; if (fabs(y) &gt;= fabs(x)) &#123; if (y &gt;= 0) &#123; _deviceOrientation = UIDeviceOrientationPortraitUpsideDown; _videoOrientation = AVCaptureVideoOrientationPortraitUpsideDown; &#125; else &#123; _deviceOrientation = UIDeviceOrientationPortrait; _videoOrientation = AVCaptureVideoOrientationPortrait; &#125; &#125; else &#123; if (x &gt;= 0) &#123; _deviceOrientation = UIDeviceOrientationLandscapeRight; _videoOrientation = AVCaptureVideoOrientationLandscapeRight; &#125; else &#123; _deviceOrientation = UIDeviceOrientationLandscapeLeft; _videoOrientation = AVCaptureVideoOrientationLandscapeLeft; &#125; &#125;&#125; capture项目中需要支持设置滤镜、特效等等，所以显示采集的图像需要自己渲染，通过官方的AVCaptureSession配置好采集流程，不管是竖着还是横着采集，获取到的pixelBuffer都是横向的，因为默认就是home键在右边采集图像，但是显示的视图一般都是竖向，所以这里要做一个90°的旋转处理,OPENGL 就是一个旋转矩阵。 123456789101112void main()&#123; // z mat4 rotationMatrix = mat4(cos(angle) , sin(angle) , 0.0, 0.0, -sin(angle) , cos(angle) , 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0); vec4 outPosition = rotationMatrix * position; gl_Position = outPosition; textureCoordinate = inputTextureCoordinate.xy;&#125; 如果只是解决采集画面显示，也可以通过设置connection.videoOrientation = [self currentVideoOrientation]; 来解决项目中没有用这个，因为后面还需要录制视频导出视频文件。 12345678910111213141516171819202122// 当前设备取向- (AVCaptureVideoOrientation)currentVideoOrientation&#123; AVCaptureVideoOrientation orientation; switch (self.motionManager.deviceOrientation) &#123; case UIDeviceOrientationPortrait: orientation = AVCaptureVideoOrientationPortrait; break; case UIDeviceOrientationLandscapeLeft: orientation = AVCaptureVideoOrientationLandscapeRight; break; case UIDeviceOrientationLandscapeRight: orientation = AVCaptureVideoOrientationLandscapeLeft; break; case UIDeviceOrientationPortraitUpsideDown: orientation = AVCaptureVideoOrientationPortraitUpsideDown; break; default: orientation = AVCaptureVideoOrientationPortrait; break; &#125; return orientation;&#125; Record录制过程通过AVAssetWriterInput *writerInput = [AVAssetWriterInput assetWriterInputWithMediaType:obj outputSettings:options]; 和[self.mPixelBufferAdaptor appendPixelBuffer:pixelBuffer withPresentationTime:ts] 来积累pixelBuffer， 为了保障导出的视频可以正常显示，需要记录视频的方向，通过设置writerInput.transform[ = self transformFromCurrentVideoOrientationToOrientation:AVCaptureVideoOrientationPortrait],系统自动帮我们做转换 1234567891011121314151617181920212223242526272829303132// 旋转视频方向函数实现- (CGAffineTransform)transformFromCurrentVideoOrientationToOrientation:(AVCaptureVideoOrientation)orientation &#123; CGFloat orientationAngleOffset = [self angleOffsetFromPortraitOrientationToOrientation:orientation]; CGFloat videoOrientationAngleOffset = [self angleOffsetFromPortraitOrientationToOrientation:self.currentOrientation]; CGFloat angleOffset; if (self.position == AVCaptureDevicePositionBack) &#123; angleOffset = videoOrientationAngleOffset - orientationAngleOffset + M_PI_2; &#125; else &#123; angleOffset = orientationAngleOffset - videoOrientationAngleOffset + M_PI_2; &#125; CGAffineTransform transform = CGAffineTransformMakeRotation(angleOffset); return transform;&#125;- (CGFloat)angleOffsetFromPortraitOrientationToOrientation:(AVCaptureVideoOrientation)orientation &#123; CGFloat angle = 0.0; switch (orientation) &#123; case AVCaptureVideoOrientationPortrait: angle = 0.0; break; case AVCaptureVideoOrientationPortraitUpsideDown: angle = M_PI; break; case AVCaptureVideoOrientationLandscapeRight: angle = -M_PI_2; break; case AVCaptureVideoOrientationLandscapeLeft: angle = M_PI_2; break; &#125; return angle;&#125; 主要就是通过录制视频时候的设备方向跟显示方向计算一下transform Pic拍照生成图片是同样的道理，直接给出方法，主要就是图片方向的旋转 123456789101112131415161718- (UIImageOrientation)getImageRotationOrientationFromCaptureVideoOrientation:(AVCaptureVideoOrientation)orientation &#123; UIImageOrientation imageOrientation; switch (orientation) &#123; case AVCaptureVideoOrientationPortrait: imageOrientation = UIImageOrientationRight; break; case AVCaptureVideoOrientationPortraitUpsideDown: imageOrientation = UIImageOrientationLeft; break; case AVCaptureVideoOrientationLandscapeRight: imageOrientation = UIImageOrientationUp; break; case AVCaptureVideoOrientationLandscapeLeft: imageOrientation = UIImageOrientationDown; break; &#125; return imageOrientation;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455+ (UIImage *)image:(UIImage *)image rotation:(UIImageOrientation)orientation &#123; long double rotate = 0.0; CGRect rect; float translateX = 0; float translateY = 0; float scaleX = 1.0; float scaleY = 1.0; switch (orientation) &#123; case UIImageOrientationLeft: rotate = M_PI_2; rect = CGRectMake(0, 0, image.size.height, image.size.width); translateX = 0; translateY = -rect.size.width; scaleY = rect.size.width/rect.size.height; scaleX = rect.size.height/rect.size.width; break; case UIImageOrientationRight: rotate = -M_PI_2; rect = CGRectMake(0, 0, image.size.height, image.size.width); translateX = -rect.size.height; translateY = 0; scaleY = rect.size.width/rect.size.height; scaleX = rect.size.height/rect.size.width; break; case UIImageOrientationDown: rotate = M_PI; rect = CGRectMake(0, 0, image.size.width, image.size.height); translateX = -rect.size.width; translateY = -rect.size.height; break; default: rotate = 0.0; rect = CGRectMake(0, 0, image.size.width, image.size.height); translateX = 0; translateY = 0; break; &#125; UIGraphicsBeginImageContext(rect.size); CGContextRef context = UIGraphicsGetCurrentContext(); //做CTM变换 CGContextTranslateCTM(context, 0.0, rect.size.height); CGContextScaleCTM(context, 1.0, -1.0); CGContextRotateCTM(context, rotate); CGContextTranslateCTM(context, translateX, translateY); CGContextScaleCTM(context, scaleX, scaleY); //绘制图片 CGContextDrawImage(context, CGRectMake(0, 0, rect.size.width, rect.size.height), image.CGImage); UIImage *newPic = UIGraphicsGetImageFromCurrentImageContext(); return newPic;&#125;]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>camera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[__has_include]]></title>
    <url>%2F2022%2F09%2F06%2Fhas-include%2F</url>
    <content type="text"><![CDATA[VS在搞音视频的过程中，早期为了快速实现功能，会用到一些开源库，比如FFMpeg、OpenCV、LibYUV 等，这些大名鼎鼎的开源库能够解决音视频领域的很多问题，编解码、边缘检测、RGB-YUV格式转换等等。 这几个开源库还都是C/C++的，完美解决跨平台的问题，所以早期可以快速上线，但也会带来增加包大小的问题，这是最明显的，另外开源方案也并非没有BUG。对开源库做裁剪减少大小/提issue等待官方处理、自己处理BUG等这些成本相对也不小。 另一个选择就是使用其他的替代方案：比如原生的技术实现、解决指定问题独立模块算法、DIY \ 编解码 边缘检测 格式转换 开源方案 FFmpeg OpenCV LibYUV 替代方案 VideoToolBox/MediaCodec 边界跟踪算法Suzuki85 DIY 说明 软解-&gt; 硬解 边界跟踪算法Suzuki85 就是 OpenCV 内部的一种边缘检测方案 自己处理像素数据 后期通过替代方案可以解决上面提到的问题，主要是包大小问题。另外也可以自己动手加深了解开源的技术方案 __has_include通过替代方案可以移除开源库，但是接入开源方案实现功能的相关代码就没必要移除了。所以可以通过条件编译做一个区分。正好__has_include这个宏可以满足要求 描述此宏传入一个你想引入文件的名称作为参数，如果该文件能够被引入则返回1，否则返回0。 用法 12345#if __has_include(&lt;XXX/XXX.h&gt;)#import &lt;XXX/XXX.h&gt;#else#import "YYY.h"#endif 项目中的实际用法： 123456789#if __cplusplus &amp;&amp; __has_include(&lt;Libyuv/libyuv.h&gt;)#include &lt;Libyuv/libyuv.h&gt;#endif#if __cplusplus &amp;&amp; __has_include(&lt;Libyuv/libyuv.h&gt;)...#else...#endif 12345#if __has_include(&lt;opencv2/imgcodecs/ios.h&gt;) contourArr = [OpenCVWrapper findContours:image];#else contourArr = [PContour findContours:image threshold:threshold];#endif 12345678910111213141516#if __cplusplus &amp;&amp; __has_include(&lt;ffmpeg/avformat.h&gt;)extern "C" &#123;#include "ffmpeg/timestamp.h"#include "ffmpeg/avformat.h"#include "ffmpeg/bsf.h"#include "ffmpeg/swscale.h"#include "ffmpeg/swresample.h"#include "ffmpeg/avformat.h"#include "ffmpeg/imgutils.h"#include "ffmpeg/samplefmt.h"&#125;#endif ...#endif]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RGB2YUV]]></title>
    <url>%2F2022%2F09%2F05%2FRGB2YUV%2F</url>
    <content type="text"><![CDATA[RGB2YUV处理音视频，对YUV RGB肯定不陌生，这里记录下经过OpenGL处理后的 RGB 格式的pixelBuffer 转成 YUV 格式导出视频。 DIY自己处理矩阵的计算，YUV的存储格式，可以加深对YUV的理解， 类似的反过来处理，或者处理其他格式的YUV 422 444等，也是一样的。所以还是有必要了解一下，DO IT YOURSELF. 重点是注意内存对齐，YUV420采样存储方式就好了，其他没什么复杂的。 转换后 如果出现这种像素错位的情况，一般是内存对齐 导致的补位没有考虑到，参考下代码里面的 stride字段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109CFDictionaryRef CreateCFDictionary(CFTypeRef* keys, CFTypeRef* values, size_t size) &#123; return CFDictionaryCreate(kCFAllocatorDefault, keys, values, size, &amp;kCFTypeDictionaryKeyCallBacks, &amp;kCFTypeDictionaryValueCallBacks); &#125;static void bt709_rgb2yuv8bit_TV(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.183 * R + 0.614 * G + 0.062 * B + 16; U = -0.101 * R - 0.339 * G + 0.439 * B + 128; V = 0.439 * R - 0.399 * G - 0.040 * B + 128; &#125;CVPixelBufferRef RGB2YCbCr8Bit(CVPixelBufferRef pixelBuffer) &#123; CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); int stride = (int) CVPixelBufferGetBytesPerRow(pixelBuffer) / 4; OSType pixelFormat = kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 1; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey, &#125;; CFDictionaryRef io_surface_value = CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;; CFDictionaryRef attributes = CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 0); size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 1); int plane_h1 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 0); int plane_h2 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 1); uint8_t *y = (uint8_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 0); memset(y, 0x80, plane_h1 * y_stride); uint8_t *uv = (uint8_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 1); memset(uv, 0x80, plane_h2 * uv_stride); int y_bufferSize = w * h; int uv_bufferSize = w * h / 4; uint8_t *y_planeData = (uint8_t *) malloc(y_bufferSize * sizeof(uint8_t)); uint8_t *u_planeData = (uint8_t *) malloc(uv_bufferSize * sizeof(uint8_t)); uint8_t *v_planeData = (uint8_t *) malloc(uv_bufferSize * sizeof(uint8_t)); int u_offset = 0; int v_offset = 0; uint8_t R, G, B; uint8_t Y, U, V; for (int i = 0; i &lt; h; i ++) &#123; for (int j = 0; j &lt; w; j ++) &#123; int offset = i * stride + j; B = baseAddress[offset * 4]; G = baseAddress[offset * 4 + 1]; R = baseAddress[offset * 4 + 2]; bt709_rgb2yuv8bit_TV(R, G, B, Y, U, V); y_planeData[i * w + j] = Y; //隔行扫描 偶数行的偶数列取U 奇数行的偶数列取V if (j % 2 == 0) &#123; (i % 2 == 0) ? u_planeData[u_offset++] = U : v_planeData[v_offset++] = V; &#125; &#125; &#125; for (int i = 0; i &lt; plane_h1; i ++) &#123; memcpy(y + i * y_stride, y_planeData + i * w, w); if (i &lt; plane_h2) &#123; for (int j = 0 ; j &lt; w ; j+=2) &#123; //NV12 和 NV21 格式都属于 YUV420SP 类型。它也是先存储了 Y 分量，但接下来并不是再存储所有的 U 或者 V 分量，而是把 UV 分量交替连续存储。 //NV12 是 IOS 中有的模式，它的存储顺序是先存 Y 分量，再 UV 进行交替存储。 memcpy(uv + i * y_stride + j, u_planeData + i * w/2 + j/2, 1); memcpy(uv + i * y_stride + j + 1, v_planeData + i * w/2 + j/2, 1); &#125; &#125; &#125; free(y_planeData); free(u_planeData); free(v_planeData); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); return pixelBufferCopy; &#125; LibYUV看过了上一种方式，LibYUV就更好理解了，这里主要通过pod 依赖下 LibYUV-ios， 就不自己编译了。 pod &#39;Libyuv&#39;,&#39;1703&#39; LibYUV 不能直接RGB转成NV12 ,需要通过I420过度下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); size_t bgraStride = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer,0); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); OSType pixelFormat = kCVPixelFormatType_420YpCbCr8Planar; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 1; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey &#125;; CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;; CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); unsigned char* y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,0); unsigned char* u = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,1); unsigned char* v = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,2); int32_t width = (int32_t)CVPixelBufferGetWidth(pixelBufferCopy); int32_t height = (int32_t)CVPixelBufferGetHeight(pixelBufferCopy); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,0); size_t u_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,1); size_t v_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,2); libyuv::ARGBToI420(baseAddress, (int)bgraStride, y, (int)y_stride, u, (int)u_stride, v, (int)v_stride, width, height); CVPixelBufferRef pixelBufferNV12 = NULL; const size_t size = 1; CFTypeRef _keys[size] = &#123; kCVPixelBufferIOSurfacePropertiesKey &#125;; CFDictionaryRef _io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef _values[size] = &#123;_io_surface_value&#125;; CFDictionaryRef _attributes = vtc::CreateCFDictionary(_keys, _values, size); CVReturn _status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, _attributes, &amp;pixelBufferNV12); if (_status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (_attributes) &#123; CFRelease(_attributes); _attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferNV12, 0); unsigned char* _y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,0); unsigned char* _uv = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,1); size_t _y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 0); size_t _uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 1); int32_t _width = (int32_t)CVPixelBufferGetWidth(pixelBufferNV12); int32_t _height = (int32_t)CVPixelBufferGetHeight(pixelBufferNV12); libyuv::I420ToNV12(y, (int)y_stride, u, (int)u_stride, v, (int)v_stride, _y, (int)_y_stride, _uv, (int)_uv_stride, _width, _height); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); CVPixelBufferUnlockBaseAddress(pixelBufferNV12, 0); CVPixelBufferRelease(pixelBufferCopy); RGB-&gt;NV21更新下:上次使用libyuv的时候 看漏了，其实有argb转nv21的 123456789101112131415161718192021222324252627282930313233343536373839404142CVPixelBufferLockBaseAddress(pixelBuffer, 0);uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer);size_t bgraStride = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer,0);int w = (int) CVPixelBufferGetWidth(pixelBuffer);int h = (int) CVPixelBufferGetHeight(pixelBuffer);CVPixelBufferRef pixelBufferNV12 = NULL;const size_t attributes_size = 1;CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey&#125;;CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0);CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;;CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size);CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, attributes, &amp;pixelBufferNV12);if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr;&#125;if (attributes) &#123; CFRelease(attributes); attributes = nullptr;&#125;CVPixelBufferLockBaseAddress(pixelBufferNV12, 0);unsigned char* y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,0);unsigned char* uv = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,1);size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 0);size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 1);int32_t width = (int32_t)CVPixelBufferGetWidth(pixelBufferNV12);int32_t height = (int32_t)CVPixelBufferGetHeight(pixelBufferNV12);// ARGB-&gt;NV21libyuv::ARGBToNV12(baseAddress, (int)bgraStride, y, (int)y_stride, uv, (int)uv_stride, width, height)CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);CVPixelBufferUnlockBaseAddress(pixelBufferNV12, 0);return pixelBufferNV12;]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>YUV</tag>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter Display P3]]></title>
    <url>%2F2022%2F08%2F31%2Fflutter-Display-P3%2F</url>
    <content type="text"><![CDATA[Display P3Display P3 是苹果手机相机使用的一种色域，查看图片信息可以看到， 然而在flutter中 渲染 Display P3格式的图片 bitmap颜色失真,不了解色域的参考下前面HDR相关的文章 失真效果： 修复效果： 原因就是Flutter 直接把Display P3社区的当做sRGB色域的图像处理了，而没有做色域转换 FlutterFlutter 跟 Native 图片打通，常见有两种方式：bitmap传递 &amp;&amp; 外接纹理，这边文章针对的是前者，外接纹理也会有这种问题的，无非就是pixelBuffer 转纹理，pixelBuffer一样也是有色域问题的，解决方案可以参考 Flutter HDR,原理是一样的。 针对图片的bitmap做色域转换，方案有很多，这里列出常见的两种： ImageIO123456789101112131415161718192021222324252627282930CGImageSourceRef src = CGImageSourceCreateWithData((__bridge CFDataRef) imageData, NULL); NSUInteger frameCount = CGImageSourceGetCount(src); if (frameCount &gt; 0) &#123; NSDictionary *options = @&#123;(__bridge NSString *)kCGImageSourceShouldCache : @YES, (__bridge NSString *)kCGImageSourceShouldCacheImmediately : @NO &#125;; NSDictionary *props = (NSDictionary *) CFBridgingRelease(CGImageSourceCopyPropertiesAtIndex(src, (size_t) 0, (__bridge CFDictionaryRef)options)); NSString *profileName = [props objectForKey:(NSString *) kCGImagePropertyProfileName]; if ([profileName isEqualToString:@"Display P3"]) &#123; NSMutableData *data = [NSMutableData data]; CGImageDestinationRef destRef = CGImageDestinationCreateWithData((__bridge CFMutableDataRef)data, kUTTypePNG, 1, NULL); NSMutableDictionary *properties = [NSMutableDictionary dictionary]; properties[(__bridge NSString *)kCGImageDestinationLossyCompressionQuality] = @(1); properties[(__bridge NSString *)kCGImageDestinationEmbedThumbnail] = @(0); properties[(__bridge NSString *)kCGImagePropertyNamedColorSpace] = (__bridge id _Nullable)(kCGColorSpaceSRGB); properties[(__bridge NSString *)kCGImageDestinationOptimizeColorForSharing] = @(YES); CGImageDestinationAddImageFromSource(destRef, src, 0, (__bridge CFDictionaryRef)properties); CGImageDestinationFinalize(destRef); CFRelease(destRef); return data; &#125; &#125; return imageData; 核心就是这个属性，很好理解吧 12345/* Create an image using a colorspace, that has is compatible with older devices * The value should be kCFBooleanTrue or kCFBooleanFalse * Defaults to kCFBooleanFalse = don&apos;t do any color conversion */IMAGEIO_EXTERN const CFStringRef kCGImageDestinationOptimizeColorForSharing IMAGEIO_AVAILABLE_STARTING(10.12, 9.3); 重新Render一张图1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980CGImageSourceRef src = CGImageSourceCreateWithData((__bridge CFDataRef) imageData, NULL); NSUInteger frameCount = CGImageSourceGetCount(src); if (frameCount &gt; 0) &#123; NSDictionary *frameProperties = (NSDictionary *) CFBridgingRelease(CGImageSourceCopyPropertiesAtIndex(src, (size_t) 0, NULL)); NSString *profileName = [frameProperties objectForKey:(NSString *) kCGImagePropertyProfileName]; if ([profileName isEqualToString:@"Display P3"]) &#123; CGImageRef imageRef = CGImageSourceCreateImageAtIndex(src, (size_t) 0, NULL); CIImage *image = [CIImage imageWithCGImage:imageRef]; CIContext *context = [[CIContext alloc] init]; float w = image.extent.size.width; float h = image.extent.size.height; unsigned char *bitmap = malloc(w * h * 4); CIRenderDestination *destination = [[CIRenderDestination alloc] initWithBitmapData:bitmap width:w height:h bytesPerRow:w * 4 format:kCIFormatBGRA8]; NSError *error = nil; [context startTaskToRender:image toDestination:destination error:&amp;error]; if (error) &#123; CFRelease(src); return imageData; &#125; CFRelease(src); UIImage *newImage = [FlutterImagePlugin imageFromBRGABytes:bitmap imageSize:image.extent.size]; free(bitmap); CGImageRelease(imageRef); if (newImage == nil) &#123; return imageData; &#125; return UIImagePNGRepresentation(newImage); &#125; &#125;+ (UIImage *)imageFromBRGABytes:(unsigned char *)imageBytes imageSize:(CGSize)imageSize &#123; CGImageRef imageRef = [self imageRefFromBGRABytes:imageBytes imageSize:imageSize]; if (imageRef == NULL) &#123; return nil; &#125; UIImage *image = [UIImage imageWithCGImage:imageRef]; CGImageRelease(imageRef); return image;&#125;+ (CGImageRef)imageRefFromBGRABytes:(unsigned char *)imageBytes imageSize:(CGSize)imageSize &#123; CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB(); CGContextRef context = CGBitmapContextCreate(imageBytes, imageSize.width, imageSize.height, 8, imageSize.width * 4, colorSpaceSDImage, kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst); if (context == NULL) &#123; return NULL; &#125; CGImageRef imageRef = CGBitmapContextCreateImage(context); if (imageRef == NULL) &#123; CGContextRelease(context); return NULL; &#125; CGContextRelease(context); return imageRef;&#125; 从代码量跟性能考量，无脑选前者 😝]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter-hdr]]></title>
    <url>%2F2022%2F08%2F24%2Fflutter-hdr%2F</url>
    <content type="text"><![CDATA[flutterflutter 播放 HDR 视频，色彩跟亮度都有问题 ，github 也有反馈这个issue Video Player HDR Problem 跟原生的对比可以很明显看到差距，官方看起来好像也不重视这个问题 😕 关于 HDR 格式，可以看下前面的文章 HDR笔记 这里记录下Flutter video player plugin 中处理视频色彩的方法，亮度提升需要通过硬件激活，Flutter中好像没法处理。 颜色的处理核心就是做了一个HDR-&gt;SDR的tonemap,恰好CIImage中提供了这样的Filter，处理就方便多了，看过HDR笔记 这篇文章的话，应该也可以自己通过 EOTF + 色域映射 来处理。暂时不知道 CIImage中是怎么处理的，猜测是差不多的。 12345678910111213141516171819202122232425262728293031323334353637383940- (CVPixelBufferRef)pixelBufferFormCIImage:(CIImage *)image &#123; NSDictionary *options = [NSDictionary dictionaryWithObjectsAndKeys: @&#123;&#125;, kCVPixelBufferIOSurfacePropertiesKey, @YES, kCVPixelBufferCGImageCompatibilityKey, @YES, kCVPixelBufferCGBitmapContextCompatibilityKey, nil]; CVPixelBufferRef pixelBufferCopy = NULL; CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, image.extent.size.width, image.extent.size.height, kCVPixelFormatType_32BGRA, (__bridge CFDictionaryRef) options, &amp;pixelBufferCopy); if (status == kCVReturnSuccess) &#123; CIRenderDestination *destination = [[CIRenderDestination alloc] initWithPixelBuffer:pixelBufferCopy]; [self.mContext startTaskToRender:image toDestination:destination error:nil]; &#125; return pixelBufferCopy;&#125;- (CVPixelBufferRef)copyPixelBuffer &#123; CMTime outputItemTime = [_videoOutput itemTimeForHostTime:CACurrentMediaTime()]; if ([_videoOutput hasNewPixelBufferForItemTime:outputItemTime]) &#123; CVPixelBufferRef p = [_videoOutput copyPixelBufferForItemTime:outputItemTime itemTimeForDisplay:NULL]; CFTypeRef colorPrimaries = CVBufferGetAttachment(p, kCVImageBufferTransferFunctionKey, NULL); if (colorPrimaries &amp;&amp; CFEqual(colorPrimaries, kCVImageBufferTransferFunction_ITU_R_2100_HLG)) &#123; if (@available(iOS 14.1, *)) &#123; CIImage *image = [CIImage imageWithCVPixelBuffer:p options:@&#123;kCIImageToneMapHDRtoSDR : @(YES)&#125;]; CVPixelBufferRef newP = [self pixelBufferFormCIImage:image]; CVPixelBufferRelease(p); return newP; &#125; &#125; return p; &#125; else &#123; return NULL; &#125;&#125;]]></content>
      <categories>
        <category>HDR</category>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>HDR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDR笔记]]></title>
    <url>%2F2022%2F08%2F18%2FHDR%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Noun explanationITU：国际电信联盟 International Telecommunication UnionITU-R：国际电信联盟无线电通信部门 ITU Radiocommunication Sector CIE :国际照明协会 （英文：International Commission on Illumination ，法文：Commission internationale de l&#39;éclairage ，采用法文缩写：CIE ） SMPTE：电影电视工程师协会 Society of Motion Picture and Television Engineers Hue：色调 色彩 色相Chroma：色调饱和度 浓度Luminance：亮度 明度 HDR ConceptHDR:High Dynamic Range 字面上是动态范围，一般指亮度上可以表达更大的亮度范围，呈现更大的亮度对比度。但是实际实际上HDR的技术和标准涉及色彩相关的一组属性的改善，可以带来更多的颜色、更大的亮度对比度、更高精度的量化。 OETF/EOTF: Optical-Electro/Electro-Optical Transfer Function 光电/电光转换函数 人对亮度的感知是非线性的，对暗部细节敏感，对亮部细节不敏感，利用这个特点设计了非线性的光电转换和电光转换的函数。这样的处理不仅可以节省带宽，也可以基本满足用户体验需求。光电转换的时候做了特殊的非线性编码，为暗部细节分配更多的码率，亮部细节进行了压缩或者截断减少码率的分配。电光转换进行显示还原的时候，通过应用一个逆的非线性变化，还原出线性光。 Info通过mediainfo 查看一个HDR视频信息 Video ID 1 Format HEVC Format/Info High Efficiency Video Coding Format profile Main 10@L5@High Codec ID hvc1 Codec ID/Info High Efficiency Video Coding Duration 14 s 0 ms Bit rate 123 kb/s Width 3 840 pixels Height 2 160 pixels Display aspect ratio 16:9 Frame rate mode Constant Frame rate 30.000 FPS Color space YUV Chroma subsampling 4:2:0 Bit depth 10 bits Scan type Progressive Bits/(PixelFrame)* 0.000 Stream size 211 KiB (98%) Title Core Media Video Encoded date UTC 2020-06-14 21:45:40 Tagged date UTC 2020-06-14 21:47:33 Color range Limited Color primaries BT.2020 Transfer characteristics HLG Matrix coefficients BT.2020 non-constant Codec configuration box hvcC 下面几个参数属于元数据 ，可能没有 后文会讲到 Video Mastering display color primaries Display P3 / R: x=0.677980 y=0.321980, G: x=0.245000 y=0.703000, B: x=0.137980 y=0.052000, White point: x=0.312680 y=0.328980 Mastering display luminance min: 0.0001 cd/m2, max: 1000 cd/m2 Maximum Content Light Level 1000 cd/m2 Maximum Frame-Average Light Level 400 cd/m2 重点关注 ： Color range ：色彩范围 Color primaries ：色彩原色 Transfer characteristics ：传输特性 Matrix coefficients ：矩阵系数 元数据字段 Mastering display color primaries Mastering display luminance Maximum Content Light Level Maximum Frame-Average Light Level 部分参数可以通过ffprobe查看对应的选项 ffprobe -h &gt;&gt; ffprobe.txt Color range色彩范围主要是两个： Full range （PC range ） Video range（limited range，tv range） Full Range 就是我们所熟悉的 [0, 255]，而在 Limited Range 中，Y’ 的值被限制在 [16, 235]，Cb 和 Cr 的值被限制在 [16, 240] （针对8bit的）。 HDR一个重要属性就是量化精度。 SDR技术使用8bit进行颜色的表达，而HDR使用10bit/12bit进行颜色的表示，从而减少了8bit容易出现的人为条带效应。 ios 中的精度 &amp; 色彩范围 ： 123kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange = &apos;420v&apos;, /* Bi-Planar Component Y&apos;CbCr 8-bit 4:2:0, video-range (luma=[16,235] chroma=[16,240]). baseAddr points to a big-endian CVPlanarPixelBufferInfo_YCbCrBiPlanar struct */kCVPixelFormatType_420YpCbCr10BiPlanarVideoRange = &apos;x420&apos;, /* 2 plane YCbCr10 4:2:0, each 10 bits in the MSBs of 16bits, video-range (luma=[64,940] chroma=[64,960]) */ 123kCVPixelFormatType_420YpCbCr8BiPlanarFullRange = &apos;420f&apos;, /* Bi-Planar Component Y&apos;CbCr 8-bit 4:2:0, full-range (luma=[0,255] chroma=[1,255]). baseAddr points to a big-endian CVPlanarPixelBufferInfo_YCbCrBiPlanar struct */ kCVPixelFormatType_420YpCbCr10BiPlanarFullRange = &apos;xf20&apos;, /* 2 plane YCbCr10 4:2:0, each 10 bits in the MSBs of 16bits, full-range (Y range 0-1023) */ 关于为什么要将YUV量化为tv range 16-235 ？ 以下是维基百科摘抄的一段， 意思是tv range是为了解决滤波（模数转换）后的过冲现象， Y′ values are conventionally shifted and scaled to the range [16, 235] (referred to as studio swing or “TV levels”) rather than using the full range of [0, 255] (referred to as full swing or “PC levels”). This practice was standardized in SMPTE-125M in order to accommodate signal overshoots (“ringing”) due to filtering. The value 235 accommodates a maximal black-to-white overshoot of 255 − 235 = 20, or 20 / (235 − 16) = 9.1%, which is slightly larger than the theoretical maximal overshoot (Gibbs phenomenon) of about 8.9% of the maximal step. The toe-room is smaller, allowing only 16 / 219 = 7.3% overshoot, which is less than the theoretical maximal overshoot of 8.9%. This is why 16 is added to Y′ and why the Y′ coefficients in the basic transform sum to 220 instead of 255.^[9]^ U and V values, which may be positive or negative, are summed with 128 to make them always positive, giving a studio range of 16–240 for U and V. (These ranges are important in video editing and production, since using the wrong range will result either in an image with “clipped” blacks and whites, or a low-contrast image.) Color primaries一般理解为色域，色域指可以显示的所有颜色的范围，常见的有Rec.709（全高清广播标准）、Rec.2020（4K/8K广播标准BT.2020）、Adobe RGB、P3等。 bt709 unknown reserved bt470m bt470bg smpte170m smpte240m film bt2020 smpte428 smpte431 smpte432 下图显示了人眼能够感知的所有RGB值的范围。三角形表示色域：三角形越大，可以显示的颜色越多。 这张马蹄形的图上面可以看到HDR使用的色域是BT2020， SDR使用的是BT709，可以明显看到HDR的色域大于SDR。 理解颜色空间： 颜色空间 由 颜色模型 跟 色域 共同定义。颜色模型的概念为：一种抽象数学模型，通过一组数字来描述颜色（例如RGB使用三元组、CMYK使用四元组）例如Adobe RGB和sRGB都基于RGB颜色模型，但它们是两个不同的颜色空间，因为色域不一样 Color Transfer描述光电转换过程的视频属性也叫颜色传输函数Color Transfer 就是上面表格里面的 Transfer characteristics 常见的hdr转换曲线为HLG和PQ，其中，smpte2084为PQ曲线（感知量化），arib-std-b67为HLG曲线（混合对数伽玛） bt709 unknown reserved bt470m bt470bg smpte170m smpte240m linear log100 log316 iec61966-2-4 bt1361e iec61966-2-1 bt2020-10 bt2020-12 smpte2084 smpte428 arib-std-b67 传统的SDR视频使用的BT709的光电转换函数，对高亮部分进行了截断，可以表达的亮度动态范围有限，最大亮度只有100nit。而HDR视频，增加了高亮部分细节的表达，很大的扩展亮度的动态范围。 不同HDR的设计初衷不同，其中PQ的设计更接近人眼的特点，亮度表达更准确，可以表示高达10000nit的亮度。而HLG的设计考虑了老设备的兼容性，和传统bt709的传输函数有部分是重合的，天然的对老设备具有一定兼容性 MetadataHDR元数据分为两种，静态元数据 和动态元数据 ； 使用PQ曲线的HDR10是采用静态元数据的，但是杜比公司提出来的杜比视界和三星的HDR10+，尽管使用了PQ曲线，但是他们使用的是动态元数据，HLG没有元数据。 其中 DolbyVision 等价于SMPTE ST 2094-10, HDR10+ 等价于 SMPTE ST 2094-40 静态元数据规定了整个片子像素级别最大亮度上限，在ST 2086中有标准化的定义。静态元数据的缺点是必须做全局的色调映射，没有足够的调节空间，兼容性不好。 动态元数据可以很好地解决这个问题。动态元数据主要有两个方面的作用：与静态元数据相比，它可以在每一个场景或者每一帧画面，给调色师一个发挥的空间，以展现更丰富的细节；另一个方面，通过动态元数据，在目标显示亮度上做色调映射，可以最大程度在目标显示器上呈现作者的创作意图。 最大内容亮度（MaxCLL）：整个视频流中最亮像素的亮度。 最大帧平均亮度（MaxFALL）：整个视频流中最亮帧的平均亮度 另外解释一下多出现的几个参数:progressive,SAR,DAR. progressive,其实就是扫描方式,逐行扫描.另外的一种方式就是隔行扫描:interlaced.我们平时所谓的1080p,这个p就是progressive,表示的是1080尺寸的逐行扫描视频.DAR - display aspect ratio就是视频播放时，我们看到的图像宽高的比例，缩放视频也要按这个比例来，否则会使图像看起来被压扁或者拉长了似的。 SAR - storage aspect ratio就是对图像采集时，横向采集与纵向采集构成的点阵，横向点数与纵向点数的比值。比如VGA图像640/480 = 4:3，D-1 PAL图像720/576 = 5:4 PAR - pixel aspect ratio大多数情况为1:1,就是一个正方形像素，否则为长方形像素这三者的关系PAR x SAR = DAR或者PAR = DAR/SAR Tone Mapping色调映射的目的是使高动态范围HDR图像能够适应低动态范围LDR显示器。 色调映射算法的目的在于将HDR图像的亮度进行压缩，进而映射到LDR显示设备的显示范围之内，同时，在映射的过程中要尽量保持原HDR图像的细节与颜色等重要信息。 所以色调映射算法需要具有两方面的性质： 能够将图像亮度进行压缩。 能够保持图像细节与颜色。 EOTF/OETF HLG 1234567891011121314151617181920212223242526272829float ARIB_B67_A = 0.17883277;float ARIB_B67_B = 0.28466892;float ARIB_B67_C = 0.55991073;highp float arib_b67_inverse_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt;= (1.0/2.0)) x = (x * x) * (1.0 / 3.0); else x = (exp((x - ARIB_B67_C) / ARIB_B67_A) + ARIB_B67_B) / 12.0; return x;&#125;highp float arib_b67_ootf(highp float x)&#123; return x &lt; 0.0 ? x : pow(x, 1.2);&#125;highp float arib_b67_eotf(highp float x)&#123; return arib_b67_ootf(arib_b67_inverse_oetf(x));&#125;highp float arib_b67_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt;= (1.0 / 12.0)) x = sqrt(3.0 * x); else x = ARIB_B67_A * log(12.0 * x - ARIB_B67_B) + ARIB_B67_C; return x;&#125; PQ 12345678910111213highp float ST2084_M1 = 0.1593017578125;const float ST2084_M2 = 78.84375;const float ST2084_C1 = 0.8359375;const float ST2084_C2 = 18.8515625;const float ST2084_C3 = 18.6875;highp float FLT_MIN = 1.17549435082228750797e-38;highp float st_2084_eotf(highp float x)&#123; highp float xpow = pow(x, float(1.0 / ST2084_M2)); highp float num = max(xpow - ST2084_C1, 0.0); highp float den = max(ST2084_C2 - ST2084_C3 * xpow, FLT_MIN); return pow(num/den, 1.0 / ST2084_M1);&#125; BT.709 1234567891011const float REC709_ALPHA = 1.09929682680944;const float REC709_BETA = 0.018053968510807;highp float rec_709_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt; REC709_BETA ) x = x * 4.5; else x = REC709_ALPHA * pow(x, 0.45) - (REC709_ALPHA - 1.0); return x;&#125; bt2020 -&gt; bt709 1231.6605, -0.5876, -0.0728-0.1246, 1.1329, -0.0083-0.0182, -0.1006, 1.1187 bt709 -&gt; bt2020 1230.6274, 0.3293, 0.04330.0691, 0.9195, 0.01140.0164, 0.0880, 0.8956 Matrix coefficientsYCbCr-&gt;RGB Video Range 1234567891011121314151617181920// BT.601, which is the standard for SDTV.static const GLfloat kColorConversion601[] = &#123; 1.164, 1.164, 1.164, 0.0, -0.392, 2.017, 1.596, -0.813, 0.0,&#125;;// BT.709, which is the standard for HDTV.static const GLfloat kColorConversion709[] = &#123; 1.164, 1.164, 1.164, 0.0, -0.213, 2.112, 1.793, -0.533, 0.0,&#125;;// BT.2020 (which is the standard for UHDTV, ITU_R_2020 )static const GLfloat kColorConversion2020[] = &#123; 1.1644f, 1.1644f, 1.1644f, 0.0f, -0.1881, 2.1501, 1.6853, -0.6529, 0.0f,&#125;; Full Range 1234567891011121314151617181920// BT.601 full range static const GLfloat kColorConversion601FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.343, 1.765,1.4, -0.711, 0.0,&#125;;// BT.709 full range static const GLfloat kColorConversion709FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.187, 1.855,1.574, -0.468, 0.0,&#125;;// BT.2020 full range static const GLfloat kColorConversion2020FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.1645, 1.8814,1.4746, -0.5713, 0.0,&#125;; OpenGL Video Range 1234567891011121314151617precision mediump float; varying mediump vec2 textureCoordinate; uniform sampler2D luminanceTexture; uniform sampler2D chrominanceTexture; uniform highp mat3 colorConversionMatrix; void main() &#123; mediump vec3 yuv; highp vec3 rgb; yuv.x = texture2D(luminanceTexture, textureCoordinate).r - (16.0/255.0); yuv.yz = texture2D(chrominanceTexture, textureCoordinate).ra - vec2(0.5, 0.5); rgb = colorConversionMatrix * yuv; gl_FragColor = vec4(rgb, 1.); &#125; Full Range 12345678910111213141516precision mediump float; varying mediump vec2 textureCoordinate; uniform sampler2D luminanceTexture; uniform sampler2D chrominanceTexture; uniform highp mat3 colorConversionMatrix; void main() &#123; mediump vec3 yuv; highp vec3 rgb; yuv.x = texture2D(luminanceTexture, textureCoordinate).r; yuv.yz = texture2D(chrominanceTexture, textureCoordinate).ra - vec2(0.5, 0.5); rgb = colorConversionMatrix * yuv; gl_FragColor = vec4(rgb, 1.); &#125; RGB-YUV123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687// FULL RANGE static void bt2020_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2627 * R + 0.6780 * G + 0.0593 * B; U = -0.1396 * R - 0.3604 * G + 0.5000 * B + 128; V = 0.5000 * R - 0.4598 * G - 0.0402 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt2020_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2256 * R + 0.5823 * G + 0.0509 * B + 16; U = -0.1222 * R - 0.3154 * G + 0.4375 * B + 128; V = 0.4375 * R - 0.4023 * G - 0.0352 * B + 128; /// 8 分解为2 + 6 /// &lt;&lt;2 ：对应 把 yuv 从 (luma=[16,235] chroma=[16,240]) 拉到 (luma=[64,940] chroma=[64,960]) /// &lt;&lt;6 ：10bit 按字节为单位需要两个字节，因为按照大端模式存储（低地址到高地址的顺序存放数据的高位字节到低位字节）后面6个bit是padding 补0 Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt709_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2126 * R + 0.7152 * G + 0.0722 * B; U = -0.1146 * R - 0.3854 * G + 0.5000 * B + 128; V = 0.5000 * R - 0.4542 * G - 0.0458 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt709_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.183 * R + 0.614 * G + 0.062 * B + 16; U = -0.101 * R - 0.339 * G + 0.439 * B + 128; V = 0.439 * R - 0.399 * G - 0.040 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt601_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.169 * R - 0.331 * G + 0.500 * B + 128; V = 0.500 * R - 0.419 * G - 0.081 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt601_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.257 * R + 0.504 * G + 0.098 * B + 16; U = -0.148 * R - 0.291 * G + 0.439 * B + 128; V = 0.439 * R - 0.368 * G - 0.071 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt601_rgb2yuv_PC(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.169 * R - 0.331 * G + 0.500 * B + 128; V = 0.500 * R - 0.419 * G - 0.081 * B + 128; &#125; // VIDEO RANGE static void bt601_rgb2yuv_TV(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.257 * R + 0.504 * G + 0.098 * B + 16; U = -0.148 * R - 0.291 * G + 0.439 * B + 128; V = 0.439 * R - 0.368 * G - 0.071 * B + 128; &#125; 12345678910111213141516171819202122inline CFDictionaryRef HDRAttachmentsOfMedium(void) &#123; const size_t attributes_size = 6; CFTypeRef keys[attributes_size] = &#123; kCVImageBufferFieldCountKey, kCVImageBufferChromaLocationBottomFieldKey, kCVImageBufferChromaLocationTopFieldKey, kCVImageBufferTransferFunctionKey, kCVImageBufferColorPrimariesKey, kCVImageBufferYCbCrMatrixKey &#125;; CFTypeRef values[attributes_size] = &#123; kCFBooleanTrue, kCVImageBufferChromaLocation_Left, kCVImageBufferChromaLocation_Left, kCVImageBufferTransferFunction_ITU_R_2100_HLG, kCVImageBufferColorPrimaries_ITU_R_2020, kCVImageBufferYCbCrMatrix_ITU_R_2020 &#125;; return CreateCFDictionary(keys, values, attributes_size); &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495CVPixelBufferRef RGB2YCbCr10Bit(CVPixelBufferRef pixelBuffer, CFDictionaryRef dic) &#123; CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); //int stride = (int) CVPixelBufferGetBytesPerRow(pixelBuffer) / 4; OSType pixelFormat = kCVPixelFormatType_420YpCbCr10BiPlanarVideoRange; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 5; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey, kCVPixelBufferExtendedPixelsBottomKey, kCVPixelBufferExtendedPixelsTopKey, kCVPixelBufferExtendedPixelsRightKey, kCVPixelBufferExtendedPixelsLeftKey &#125;; CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value, kCFBooleanFalse, kCFBooleanFalse, kCFBooleanFalse, kCFBooleanFalse&#125;; CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; int plane_h1 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 0); int plane_h2 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 1); CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); if (dic == nullptr || CFDictionaryGetCount(dic) &lt;= 0) &#123; dic = HDRAttachmentsOfMedium(); &#125; CVBufferSetAttachments(pixelBufferCopy, dic, kCVAttachmentMode_ShouldPropagate); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 0); //size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 1); unsigned long y_bufferSize = w * h; unsigned long uv_bufferSize = w * h / 4; uint16_t *y_planeData = (uint16_t *) malloc(y_bufferSize * sizeof(uint16_t)); uint16_t *u_planeData = (uint16_t *) malloc(uv_bufferSize * sizeof(uint16_t)); uint16_t *v_planeData = (uint16_t *) malloc(uv_bufferSize * sizeof(uint16_t)); uint16_t *y = (uint16_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 0); uint16_t *uv = (uint16_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 1); int u_offset = 0; int v_offset = 0; uint8_t R, G, B; uint16_t Y, U, V; for (int i = 0; i &lt; h; i ++) &#123; for (int j = 0; j &lt; w; j ++) &#123; int offset = i * w + j; B = baseAddress[offset * 4]; G = baseAddress[offset * 4 + 1]; R = baseAddress[offset * 4 + 2]; bt2020_rgb2yuv10bit_TV(R, G, B, Y, U, V); y_planeData[offset] = Y; //隔行扫描 偶数行的偶数列取U 奇数行的偶数列取V if (j % 2 == 0) &#123; (i % 2 == 0) ? u_planeData[++u_offset] = U : v_planeData[++v_offset] = V; &#125; &#125; &#125; for (int i = 0; i &lt; plane_h1; i ++) &#123; memcpy(y + i * y_stride / 2, y_planeData + i * w, 2 * w); if (i &lt; plane_h2) &#123; for (int j = 0 ; j &lt; w ; j ++) &#123; //NV12 和 NV21 格式都属于 YUV420SP 类型。它也是先存储了 Y 分量，但接下来并不是再存储所有的 U 或者 V 分量，而是把 UV 分量交替连续存储。 //NV12 是 IOS 中有的模式，它的存储顺序是先存 Y 分量，再 UV 进行交替存储。 memcpy(uv + i * y_stride / 2 + 2*j, u_planeData + i * w/2 + j, 2); memcpy(uv + i * y_stride / 2 + 2*j + 1, v_planeData + i * w/2 + j, 2); &#125; &#125; &#125; free(y_planeData); free(u_planeData); free(v_planeData); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); return pixelBufferCopy; &#125; Formula BT.601 BT.709 BT.2020 a 0.299 0.2126 0.2627 b 0.587 0.7152 0.6780 c 0.114 0.0722 0.0593 d 1.772 1.8556 1.8814 e 1.402 1.5748 1.4747 123Y = a * R + b * G + c * BCb = (B - Y) / dCr = (R - Y) / e 123R = Y + e * CrG = Y - (a * e / b) * Cr - (c * d / b) * CbB = Y + d * Cb 123a+b+c = 1e = 2 * (1 - a)d = 2* (a + b) https://www.itu.int/rec/R-REC-BT.601 https://www.itu.int/rec/R-REC-BT.709 https://www.itu.int/rec/R-REC-BT.2020 Range Deduce1234567891011121314[16/255, 16/255, 16/255, 1.0][235/255, 240/255, 240/255, 1.0]x[255/219, 0, 0, 0][0, 255/224, 0, 0][0, 0, 255/224, 0][-16/219, -128/224, -128/224, 1]=[0, -0.5, -0.5, 1.0][1, 0.5, 0.5, 1.0] 将 videorange 通过 齐次矩阵 转换为 fullrange Other大部分图像捕捉设备在保存图像时会自动加上伽马校正，也就是说图像中存储的是非线性空间中的颜色 非线性的RGB转换为YUV也是非线性 OpenGL 无法直接对 10bit YUV 进行处理，需要先转换为 8bit YUV tone mapping 需要在线性RGB空间进行 References搞清楚编程中YUV和RGB间的相互转换YUV - RGB colorconversion推导视频YUV转RGB矩阵齐次坐标# Gamma校正# 我理解的伽马校正Colour gamut conversion from Recommendation ITU-R BT.2020 to Recommendation ITU-R BT.709Colour conversion from Recommendation ITU-R BT.709 to Recommendation ITU-R BT.2020]]></content>
      <categories>
        <category>HDR</category>
      </categories>
      <tags>
        <tag>YUV</tag>
        <tag>HDR</tag>
        <tag>OPENGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coreimage with metal 笔记]]></title>
    <url>%2F2022%2F08%2F08%2Fcoreimage-with-metal-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[xcode近期有自定义CoreImage的CIFilter的需求，前期通过CIKL 定义 CIKernel完成了任务，后面了解到CoreImage新特性支持metal的方式直接自定义 CIKernel，提高效率。 CIKL的方式，存在两个问题： 编写 kernel 的时候，没有报错提示，哪怕是参数名错误都无法检查处理。效率极低。 翻译转换，编译，都是发生到运行时，导致第一次使用滤镜的时候，耗时较久。 Metal: 在build阶段 就可以编译 链接 .metal文件 参考苹果的官方文档 Metal Shading Language for CoreImage Kernels ,在xcode integration 部分提到在build setting 设置 Other Metal Compiler Flags, 文档已经很老了（2018年的），新版的xcode已经没有这个选项了，如果不做处理，会有报错 &quot;/air-lld:1:1: symbol(s) not found for target &#39;air64-apple-ios12.0.0&#39;&quot; and &quot;air-lld command failed with exit code 1 (use -v to see invocation)&quot; build rules新版xcode中可以通过配置build rules解决上面的报错 *.metal 1xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel &quot;$&#123;INPUT_FILE_PATH&#125;&quot; -o &quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;&quot; output files : $(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib *.air 1xcrun metallib -cikernel &quot;$&#123;INPUT_FILE_PATH&#125;&quot; -o &quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;&quot; output files : $(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air 如图： cocoapodsbuild rules 的方式存在一个问题，如果metal shader文件在pod库中，在主工程从配置build rules无法针对pod中的resouce 生效，虽然可以手动针对pod target 配置build rules解决问题，但是这样配置是一次性的，无法提交保存，下一次pod update就清空了，所以到了这里就很自然的能想到通过pod 的post hook 来解决问题，接下来就是怎么用ruby 来写 pod hook 脚本了 通过之前在主工程配置build rules, 可以看到project.pbxproj文件的变更情况 123456789101112131415161718192021222324252627282930/* Begin PBXBuildRule section */ BF25E98B28A0A91A00188AE3 /* PBXBuildRule */ = &#123; isa = PBXBuildRule; compilerSpec = com.apple.compilers.proxy.script; filePatterns = &quot;*.metal&quot;; fileType = pattern.proxy; inputFiles = ( ); isEditable = 1; outputFiles = ( &quot;$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air&quot;, ); runOncePerArchitecture = 0; script = &quot;# Type a script or drag a script file from your workspace to insert its path.\nxcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \&quot;$&#123;INPUT_FILE_PATH&#125;\&quot; -o \&quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;\&quot;\n&quot;; &#125;; BF25E98C28A0A92200188AE3 /* PBXBuildRule */ = &#123; isa = PBXBuildRule; compilerSpec = com.apple.compilers.proxy.script; filePatterns = &quot;*.air&quot;; fileType = pattern.proxy; inputFiles = ( ); isEditable = 1; outputFiles = ( &quot;$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib&quot;, ); runOncePerArchitecture = 0; script = &quot;# Type a script or drag a script file from your workspace to insert its path.\nxcrun metallib -cikernel \&quot;$&#123;INPUT_FILE_PATH&#125;\&quot; -o \&quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;\&quot;\n&quot;; &#125;;/* End PBXBuildRule section */ 哈哈 ，这正是我们需要的build rule的字段 最终的 MetalBuildRule.rb 文件如下 ： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/usr/bin/ruby# -*- coding: UTF-8 -*-def add_build_rule(target_name, project) project.targets.each do |target| if target.name == target_name# puts "#&#123;target.name&#125; has #&#123;target.build_rules.count&#125; build rule." if target.build_rules.count &gt;= 2 puts "#&#123;target.name&#125; already has 2 build rule." return end puts "Updating #&#123;target.name&#125; build rules" metal_rule = project.new(Xcodeproj::Project::Object::PBXBuildRule) metal_rule.name = 'Metal Build Rule' metal_rule.compiler_spec = 'com.apple.compilers.proxy.script' metal_rule.file_patterns = '*.metal' metal_rule.file_type = 'pattern.proxy' metal_rule.is_editable = '1' metal_rule.run_once_per_architecture = '0' metal_rule.output_files = ["$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air"] metal_rule.input_files = [] metal_rule.output_files_compiler_flags = [] metal_rule.script = "xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(metal_rule) air_rule = project.new(Xcodeproj::Project::Object::PBXBuildRule) air_rule.name = 'Air Build Rule' air_rule.compiler_spec = 'com.apple.compilers.proxy.script' air_rule.file_patterns = '*.air' air_rule.file_type = 'pattern.proxy' air_rule.is_editable = '1' air_rule.run_once_per_architecture = '0' air_rule.output_files = ["$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib"] air_rule.input_files = [] air_rule.output_files_compiler_flags = [] air_rule.script = "xcrun metallib -cikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(air_rule) project.objects_by_uuid[metal_rule.uuid] = metal_rule project.objects_by_uuid[air_rule.uuid] = air_rule project.save() end endend podfile 文件里加载 MetalBuildRule.rb, 配置hook 123post_install do |installer| add_build_rule("your-target-name", installer.pods_project)end framework如果pod库是通过cocoapods-packager插件 打.a 或者 .framework的方式提供给主工程使用的话，发现还是会遇到上文提到的错误 &quot;/air-lld:1:1: symbol(s) not found for target &#39;air64-apple-ios12.0.0&#39;&quot; and &quot;air-lld command failed with exit code 1 (use -v to see invocation)&quot; 这里需要简单了解下 cocoapods-packager 的原理，浅析 Cocoapods-Packager 实现.因为 cocoapods-packager 会重新生成一个podfile 来构造一个打包用的工程，所以这个错误的出现跟文章最开始提到的情况是一模一样的，解法是不是也可以通过配置build rule来解呢，不过打包工程我们看起来好像无法干预，怎么解呢？ 还是要回到cocoapods-packager插件来解决问题。 1https://github.com/CocoaPods/cocoapods-packager git 代码拉下来 通过ide(vscode/rubymine) 打开插件工程 配置好工程ruby环境 DEBUG 代码，找到干预点 设置build rule 生成packager gem，安装 这里涉及到ruby gem bundle等ruby环境的基本命令/用法，可以自行google一下。 通过刚刚提到的插件原理，很容易找到干预点 1234pod_utils.rbdef install_pod(platform_name, sandbox)...end 修改： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def install_pod(platform_name, sandbox) # 判断resource_bundle 是否有.metal文件 metal = false if @spec.attributes_hash["resource_bundle"] metal = @spec.attributes_hash["resource_bundle"][@spec.name].include?("metal") end if @spec.attributes_hash["resource_bundles"] if @spec.attributes_hash["resource_bundles"][@spec.name] @spec.attributes_hash["resource_bundles"][@spec.name].each &#123; |res| metal ||= res.include?("metal") &#125; end end ... unless static_installer.nil? static_installer.pods_project.targets.each do |target| # 如果有.metal文件 &amp;&amp; target匹配 -&gt; 设置 build rule if metal &amp;&amp; target.name.start_with?(@spec.name) UI.puts "#&#123;target.name&#125; has #&#123;target.build_rules.count&#125; build rule." if target.build_rules.count &gt;= 2 UI.puts "#&#123;target.name&#125; already has 2 build rule." return end metal_rule = static_installer.pods_project.new(Xcodeproj::Project::Object::PBXBuildRule) UI.puts "Updating #&#123;target.name&#125; rules" metal_rule.name = 'Metal Build Rule' metal_rule.compiler_spec = 'com.apple.compilers.proxy.script' metal_rule.file_patterns = '*.metal' metal_rule.file_type = 'pattern.proxy' metal_rule.is_editable = '1' metal_rule.run_once_per_architecture = '0' metal_rule.output_files = ["$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air"] metal_rule.input_files = [] metal_rule.output_files_compiler_flags = [] metal_rule.script = "xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(metal_rule) air_rule = static_installer.pods_project.new(Xcodeproj::Project::Object::PBXBuildRule) UI.puts "Updating #&#123;target.name&#125; rules" air_rule.name = 'Air Build Rule' air_rule.compiler_spec = 'com.apple.compilers.proxy.script' air_rule.file_patterns = '*.air' air_rule.file_type = 'pattern.proxy' air_rule.is_editable = '1' air_rule.run_once_per_architecture = '0' air_rule.output_files = ["$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib"] air_rule.input_files = [] air_rule.output_files_compiler_flags = [] air_rule.script = "xcrun metallib -cikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(air_rule) static_installer.pods_project.objects_by_uuid[metal_rule.uuid] = metal_rule static_installer.pods_project.objects_by_uuid[air_rule.uuid] = air_rule static_installer.pods_project.save end ... end ... end ... end 最后重新生成、安装gem 12345#!/bin/bashgem uninstall cocoapods-packagergem build cocoapods-packager.gemspecgem install cocoapods-packager podfile使用自定义的cocoapods-packager打出来的二方库 Framework包，包内容里面已经替换成xxx.metallib文件了,所以主工程的podfile pod post hook 要根据二方库的接入方式做下处理。 我这边主工程是用过cocoapod-binary插件管理二方库的加入，源码&amp;静态Framework，一般Release模式提升编译速度，都是以framework方式，Debug模式有时候需要在主工程Debug二方库，可以选择是源码方式接入。 最终的逻辑如下： 1234567891011121314151617post_install do |installer| installer.pods_project.targets.each do |target| ... target.build_configurations.each do |config| ... #Release model, no need execute if config.name != 'Release' &amp;&amp; target.name == 'your target name' puts "===================&gt; #&#123;config.name&#125;" eval(File.open('MetalBuildRule.rb').read) if File.exist? 'MetalBuildRule.rb' # metal shader build rule add_build_rule(target, installer.pods_project) end end endend DONE referencesMetalCIKLReferenceAdd custom build rule with Podfile post_install hookxcodeprojxcode工程文件解析CocoaPods源码与插件断点调试]]></content>
      <categories>
        <category>metal</category>
      </categories>
      <tags>
        <tag>metal</tag>
        <tag>coreimage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter 1.17共享engine]]></title>
    <url>%2F2020%2F06%2F03%2Fflutter-1-17%E5%85%B1%E4%BA%ABengine%2F</url>
    <content type="text"><![CDATA[前言flutter 升级到 1.17之后，app ios 线上遇到一个crash ,通过官方的 符号表文件 flutter.dsym 还原出堆栈如下 1234560 auto fml::internal::CopyableLambda&lt;flutter::Shell::OnPlatformViewCreated(std::__1::unique_ptr&lt;flutter::Surface, std::__1::default_delete&lt;flutter::Surface&gt; &gt;)::$_8&gt;::operator()&lt;&gt;() const (in Flutter) (make_copyable.h:24)1 auto fml::internal::CopyableLambda&lt;flutter::Shell::OnPlatformViewCreated(std::__1::unique_ptr&lt;flutter::Surface, std::__1::default_delete&lt;flutter::Surface&gt; &gt;)::$_8&gt;::operator()&lt;&gt;() const (in Flutter) (make_copyable.h:24)2 fml::MessageLoopImpl::FlushTasks(fml::FlushType) (in Flutter) (message_loop_impl.cc:129)3 fml::MessageLoopDarwin::OnTimerFire(__CFRunLoopTimer*, fml::MessageLoopDarwin*) (in Flutter) (message_loop_darwin.mm:76)9 fml::MessageLoopDarwin::Run() (in Flutter) (message_loop_darwin.mm:47)10 void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, fml::Thread::Thread(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)::$_0&gt; &gt;(void*) (in Flutter) (thread:352) 这里还只能看到crash在engine的c++代码中，具体原因未知 定位我们根据crash 用户的 埋点日志 分析crash前的 使用路径，基本都是打开push 落地到一个flutter页面app 的 第一个tab 也是个 flutter 页面，所以是push 唤起app，连续打开两个flutter页面。手动打开app，点击进到flutter页面是不会crash的（这么简单的路径，如果crash，那就该死了）很快我们就可以通过这个 路径 复现 crash ，能复现就好说。 debug engine源码，可以定位到更具体的地方 surface_ 为 null ，EXC_BAD_ACCESS 野指针 分析定位到了具体的代码位置，接下来分析下野指针的原因xcode中 crash 的时候，看到主线程的 堆栈记录 是从 application 的 didbecomeactive 的 notification发起的由于是 push 唤起app ，有这个通知是对的，crash 是在 共享engine 的 raster 线程。 看代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#pragma mark - Application lifecycle notifications// app 的 首页 flutter 页面会 执行 surfaceUpdated 方法- (void)applicationBecameActive:(NSNotification*)notification &#123; TRACE_EVENT0("flutter", "applicationBecameActive"); if (_viewportMetrics.physical_width) [self surfaceUpdated:YES]; [self goToApplicationLifecycle:@"AppLifecycleState.resumed"];&#125;#pragma mark - Surface creation and teardown updates- (void)surfaceUpdated:(BOOL)appeared &#123; // NotifyCreated/NotifyDestroyed are synchronous and require hops between the UI and raster // thread. if (appeared) &#123; [self installFirstFrameCallback]; [_engine.get() platformViewsController] -&gt; SetFlutterView(_flutterView.get()); [_engine.get() platformViewsController] -&gt; SetFlutterViewController(self); // 这里 [_engine.get() platformView] -&gt; NotifyCreated(); &#125; else &#123; self.displayingFlutterUI = NO; [_engine.get() platformView] -&gt; NotifyDestroyed(); [_engine.get() platformViewsController] -&gt; SetFlutterView(nullptr); [_engine.get() platformViewsController] -&gt; SetFlutterViewController(nullptr); &#125;&#125;void PlatformView::NotifyCreated() &#123; std::unique_ptr&lt;Surface&gt; surface; // Threading: We want to use the platform view on the non-platform thread. // Using the weak pointer is illegal. But, we are going to introduce a latch // so that the platform view is not collected till the surface is obtained. auto* platform_view = this; fml::ManualResetWaitableEvent latch; fml::TaskRunner::RunNowOrPostTask( task_runners_.GetRasterTaskRunner(), [platform_view, &amp;surface, &amp;latch]() &#123; surface = platform_view-&gt;CreateRenderingSurface(); latch.Signal(); &#125;); latch.Wait(); //这里 delegate_.OnPlatformViewCreated(std::move(surface));&#125;// |PlatformView::Delegate|void Shell::OnPlatformViewCreated(std::unique_ptr&lt;Surface&gt; surface) &#123; TRACE_EVENT0("flutter", "Shell::OnPlatformViewCreated"); FML_DCHECK(is_setup_); FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); // Note: // This is a synchronous operation because certain platforms depend on // setup/suspension of all activities that may be interacting with the GPU in // a synchronous fashion. fml::AutoResetWaitableEvent latch; auto raster_task = fml::MakeCopyable([&amp; waiting_for_first_frame = waiting_for_first_frame_, rasterizer = rasterizer_-&gt;GetWeakPtr(), // surface = std::move(surface), // &amp;latch]() mutable &#123; if (rasterizer) &#123; //这里 rasterizer-&gt;Setup(std::move(surface)); &#125; waiting_for_first_frame.store(true); // Step 3: All done. Signal the latch that the platform thread is // waiting on. latch.Signal(); &#125;); ... &#125;void Rasterizer::Setup(std::unique_ptr&lt;Surface&gt; surface) &#123; surface_ = std::move(surface); if (max_cache_bytes_.has_value()) &#123; SetResourceCacheMaxBytes(max_cache_bytes_.value(), user_override_resource_cache_bytes_); &#125; compositor_context_-&gt;OnGrContextCreated(); // surface_ null，BAD_ACCESS if (surface_-&gt;GetExternalViewEmbedder()) &#123; const auto platform_id = task_runners_.GetPlatformTaskRunner()-&gt;GetTaskQueueId(); const auto gpu_id = task_runners_.GetRasterTaskRunner()-&gt;GetTaskQueueId(); raster_thread_merger_ = fml::MakeRefCounted&lt;fml::RasterThreadMerger&gt;(platform_id, gpu_id); &#125;&#125; 这么一路看下来，surface_怎么会变成null呢？一般情况是，执行 [self surfaceUpdated:NO] 的时候会销毁surface，断点根本都没进去。继续看代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107// push 落地页 flutter 页面 init 的时候，会重新attach 到 engine，会执行setViewController方法- (void)setViewController:(FlutterViewController*)viewController &#123; FML_DCHECK(self.iosPlatformView); _viewController = viewController ? [viewController getWeakPtr] : fml::WeakPtr&lt;FlutterViewController&gt;(); //这里 self.iosPlatformView-&gt;SetOwnerViewController(_viewController); [self maybeSetupPlatformViewChannels]; if (viewController) &#123; __block FlutterEngine* blockSelf = self; self.flutterViewControllerWillDeallocObserver = [[NSNotificationCenter defaultCenter] addObserverForName:FlutterViewControllerWillDealloc object:viewController queue:[NSOperationQueue mainQueue] usingBlock:^(NSNotification* note) &#123; [blockSelf notifyViewControllerDeallocated]; &#125;]; &#125; else &#123; self.flutterViewControllerWillDeallocObserver = nil; &#125;&#125;void PlatformViewIOS::SetOwnerViewController(fml::WeakPtr&lt;FlutterViewController&gt; owner_controller) &#123; FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); std::lock_guard&lt;std::mutex&gt; guard(ios_surface_mutex_); // 重点是这里 if (ios_surface_ || !owner_controller) &#123; // 这里会销毁 NotifyDestroyed(); ios_surface_.reset(); accessibility_bridge_.reset(); &#125; owner_controller_ = owner_controller; // Add an observer that will clear out the owner_controller_ ivar and // the accessibility_bridge_ in case the view controller is deleted. dealloc_view_controller_observer_.reset( [[[NSNotificationCenter defaultCenter] addObserverForName:FlutterViewControllerWillDealloc object:owner_controller_.get() queue:[NSOperationQueue mainQueue] usingBlock:^(NSNotification* note) &#123; // Implicit copy of 'this' is fine. accessibility_bridge_.reset(); owner_controller_.reset(); &#125;] retain]); if (owner_controller_ &amp;&amp; [owner_controller_.get() isViewLoaded]) &#123; this-&gt;attachView(); &#125; // Do not call `NotifyCreated()` here - let FlutterViewController take care // of that when its Viewport is sized. If `NotifyCreated()` is called here, // it can occasionally get invoked before the viewport is sized resulting in // a framebuffer that will not be able to completely attach.&#125;void PlatformView::NotifyDestroyed() &#123; delegate_.OnPlatformViewDestroyed();&#125;// |PlatformView::Delegate|void Shell::OnPlatformViewDestroyed() &#123; TRACE_EVENT0("flutter", "Shell::OnPlatformViewDestroyed"); FML_DCHECK(is_setup_); FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); // Note: // This is a synchronous operation because certain platforms depend on // setup/suspension of all activities that may be interacting with the GPU in // a synchronous fashion. fml::AutoResetWaitableEvent latch; auto io_task = [io_manager = io_manager_.get(), &amp;latch]() &#123; // Execute any pending Skia object deletions while GPU access is still // allowed. io_manager-&gt;GetIsGpuDisabledSyncSwitch()-&gt;Execute( fml::SyncSwitch::Handlers().SetIfFalse( [&amp;] &#123; io_manager-&gt;GetSkiaUnrefQueue()-&gt;Drain(); &#125;)); // Step 3: All done. Signal the latch that the platform thread is waiting // on. latch.Signal(); &#125;; auto raster_task = [rasterizer = rasterizer_-&gt;GetWeakPtr(), io_task_runner = task_runners_.GetIOTaskRunner(), io_task]() &#123; if (rasterizer) &#123; // 这里 rasterizer-&gt;Teardown(); &#125; // Step 2: Next, tell the IO thread to complete its remaining work. fml::TaskRunner::RunNowOrPostTask(io_task_runner, io_task); &#125;; ...void Rasterizer::Teardown() &#123; compositor_context_-&gt;OnGrContextDestroyed(); // 这里 reset surface_.reset(); last_layer_tree_.reset();&#125; 所以原因 就是 落地页 init 的时候 重新attach 引擎，NotifyDestroyed 方法 最终会销毁 surface，这时候正好raster线程使用 surface_做方法调用。 修复定位到原因，修复就简单了，做下空判断就好了,如果为空 就直接return 123456789101112131415161718192021void Rasterizer::Setup(std::unique_ptr&lt;Surface&gt; surface) &#123; surface_ = std::move(surface); if (!surface_) &#123; FML_DLOG(INFO) &lt;&lt; "Rasterizer::Setup called with no surface."; return; &#125; if (max_cache_bytes_.has_value()) &#123; SetResourceCacheMaxBytes(max_cache_bytes_.value(), user_override_resource_cache_bytes_); &#125; compositor_context_-&gt;OnGrContextCreated(); if (surface_-&gt;GetExternalViewEmbedder()) &#123; const auto platform_id = task_runners_.GetPlatformTaskRunner()-&gt;GetTaskQueueId(); const auto gpu_id = task_runners_.GetRasterTaskRunner()-&gt;GetTaskQueueId(); raster_thread_merger_ = fml::MakeRefCounted&lt;fml::RasterThreadMerger&gt;(platform_id, gpu_id); &#125;&#125; 这里是直接修改了引擎的代码，所以需要重新编译engine 产物，替换掉就搞定了 其他1.17之前的版本 1.12.13 的时候，不确定engine存不存在这个问题，有空再看看。后面github提issue、PR，看看官方怎么看待这个问题，修复应该还有其他办法。]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter 1.17升级]]></title>
    <url>%2F2020%2F05%2F21%2Fflutter-1-17%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[升级最近官方发布了flutter 稳定版本1.17.0 ，记录下升级1.17 ios上碰到的的问题 App产物在 1.12.13 的时候，为了支持模拟器运行，会进行 debug 产物 跟 release 产物的merge （lipo create …）debug 产物 x86 、release 产物 arm64 arm7 升级到1.17.0 之后 ，merge报错 lipo 查看下 发现针对模拟器的debug产物 含有arm64 Debug Flutter tool源码， build 里面进行了两次createStubAppFramework（iphone &amp;&amp; simulator）然后做了merge，实际上environment参数里面 iosArchs只有 arch x86，所以问题出在这里 DebugUniveralFramework1234567891011121314151617181920212223242526272829303132333435363738394041@overrideFuture&lt;void&gt; build(Environment environment) async &#123; // Generate a trivial App.framework. final Set&lt;DarwinArch&gt; iosArchs = environment.defines[kIosArchs] ?.split(' ') ?.map(getIOSArchForName) ?.toSet() ?? &lt;DarwinArch&gt;&#123;DarwinArch.arm64&#125;; final File iphoneFile = environment.buildDir.childFile('iphone_framework'); final File simulatorFile = environment.buildDir.childFile('simulator_framework'); final File lipoOutputFile = environment.buildDir.childFile('App'); final RunResult iphoneResult = await createStubAppFramework( iphoneFile, SdkType.iPhone, // Only include 32bit if it is contained in the active architectures. include32Bit: iosArchs.contains(DarwinArch.armv7) ); final RunResult simulatorResult = await createStubAppFramework( simulatorFile, SdkType.iPhoneSimulator, ); if (iphoneResult.exitCode != 0 || simulatorResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125; final List&lt;String&gt; lipoCommand = &lt;String&gt;[ 'xcrun', 'lipo', '-create', iphoneFile.path, simulatorFile.path, '-output', lipoOutputFile.path ]; final RunResult lipoResult = await processUtils.run( lipoCommand, ); if (lipoResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125;&#125; 解决办法 可以通过archs 判断下具体执行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@override Future&lt;void&gt; build(Environment environment) async &#123; // Generate a trivial App.framework. final Set&lt;DarwinArch&gt; iosArchs = environment.defines[kIosArchs] ?.split(' ') ?.map(getIOSArchForName) ?.toSet() ?? &lt;DarwinArch&gt;&#123;DarwinArch.arm64&#125;; final File iphoneFile = environment.buildDir.childFile('iphone_framework'); final File simulatorFile = environment.buildDir.childFile('simulator_framework'); final File lipoOutputFile = environment.buildDir.childFile('App'); RunResult iphoneResult; if(iosArchs.contains(DarwinArch.arm64) || iosArchs.contains(DarwinArch.armv7)) &#123; iphoneResult = await createStubAppFramework( iphoneFile, SdkType.iPhone, // Only include 32bit if it is contained in the active architectures. include32Bit: iosArchs.contains(DarwinArch.armv7) ); if (iphoneResult.exitCode != 0) &#123; throw Exception('(iphoneResult)Failed to create App.framework.'); &#125; &#125; RunResult simulatorResult; if(iosArchs.contains(DarwinArch.x86_64)) &#123; simulatorResult = await createStubAppFramework( simulatorFile, SdkType.iPhoneSimulator, ); if (simulatorResult.exitCode != 0) &#123; throw Exception('(simulatorResult)Failed to create App.framework.'); &#125; &#125; if(simulatorResult == null) &#123; iphoneFile.copySync(lipoOutputFile.path); return; &#125; if(iphoneResult == null) &#123; simulatorFile.copySync(lipoOutputFile.path); return; &#125; final List&lt;String&gt; lipoCommand = &lt;String&gt;[ 'xcrun', 'lipo', '-create', iphoneFile.path, simulatorFile.path, '-output', lipoOutputFile.path ]; final RunResult lipoResult = await processUtils.run( lipoCommand, ); if (lipoResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125; &#125; 突然想到 既然1.17 对debug 产物做了arm64的支持，那我们收集产物是不是可以不用自己做merge，发现是不可以的因为除了App.framework,还有plugin native代码生成的pod静态库 libxxx.a。静态库 是哪里生成的呢？ 这里 build_ios.dart -&gt; buildXcodeProject 123456789101112131415161718...final List&lt;String&gt; buildCommands = &lt;String&gt;[ &apos;/usr/bin/env&apos;, &apos;xcrun&apos;, &apos;xcodebuild&apos;, &apos;-configuration&apos;, configuration,]; ...if (buildForDevice) &#123; buildCommands.addAll(&lt;String&gt;[&apos;-sdk&apos;, &apos;iphoneos&apos;]);&#125; else &#123; buildCommands.addAll(&lt;String&gt;[&apos;-sdk&apos;, &apos;iphonesimulator&apos;, &apos;-arch&apos;, &apos;x86_64&apos;]);&#125;... 这里只针对x86做了xcode build，所以还是要自己merge的…]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter tool debug]]></title>
    <url>%2F2020%2F03%2F13%2Fflutter-tool-debug%2F</url>
    <content type="text"><![CDATA[Flutterflutter 开发过程中，少不了会运行一些 flutter 命令 ，比如 flutter build xxx 、 flutter run 等等看下 bin/flutter 脚本，背后都是 flutter_tool 在执行各种操作。 12345678FLUTTER_TOOLS_DIR="$FLUTTER_ROOT/packages/flutter_tools"SNAPSHOT_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.snapshot"STAMP_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.stamp"SCRIPT_PATH="$FLUTTER_TOOLS_DIR/bin/flutter_tools.dart"DART_SDK_PATH="$FLUTTER_ROOT/bin/cache/dart-sdk""$DART" --packages="$FLUTTER_TOOLS_DIR/.packages" $FLUTTER_TOOL_ARGS "$SNAPSHOT_PATH" "$@" 关于 debug flutter_tool,就不说的了，自行google一下。。 这里记录下 debug 过程中遇到的问题 Exception指定端口导出FLUTTER_TOOL_ARGS 环境变量 export FLUTTER_TOOL_ARGS=&quot;--pause_isolates_on_start --enable-vm-service:65432&quot; run 起来 会停在这里… Observatory listening on http://127.0.0.1:65432/ZbWg3veM6kY=/ IDE 中打开flutter tool 项目，配置 dart remote debug attach 出现以下错误信息 123Failed to connect to the VM observatory service: java.io.IOException: Failed to connect: ws://127.0.0.1:65432/wsCaused by: de.roderick.weberknecht.WebSocketException: error while creating socket to ws://127.0.0.1:65432/wsCaused by: java.net.ConnectException: Connection refused (Connection refused) 原因 ： http://127.0.0.1:65432/ZbWg3veM6kY=/ 后面多了个ZbWg3veM6kY，这是一种认证码，是为了安全原因，防止应用被远程调试。可以通过参数–disable-service-auth-codes进行关闭。 export FLUTTER_TOOL_ARGS=&quot;--pause_isolates_on_start --enable-vm-service:65432 --disable-service-auth-codes&quot;]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter共享engine]]></title>
    <url>%2F2020%2F03%2F10%2Fflutter%E5%85%B1%E4%BA%ABengine%2F</url>
    <content type="text"><![CDATA[flutter 共享引擎 问题记录共享引擎，就是只有一个 flutter engine，每个页面一个 flutterviewcontroller。flutter页面 切换，引擎会相应的 detach atach 最近 升级 flutter 到 v1.12.13 版本后，贡献引擎遇到的几个问题 记录下 present flutter 页面这个其实不是v1.12.13出现的问题 flutterPageA present flutterPageB 会 出现 pageA的 viewDidDisappear 比 pageB的 viewDidAppear 后执行 12345678910- (void)viewDidDisappear:(BOOL)animated &#123; [super viewDidDisappear:animated]; //处理下present 页面卡死的情况 if ([WD_FLUTTER_ENGINE flutterViewController] != self) &#123; [WD_FLUTTER_ENGINE resume]; [(WDFlutterViewContainer *)[WD_FLUTTER_ENGINE flutterViewController] surfaceUpdated:YES]; &#125; else &#123; [WD_FLUTTER_ENGINE detach]; &#125;&#125; 从flutter 返回到 native页面在 v.1.12.13之前 flutter popto native 无需处理v.1.12.13 出现crash 123456789101112131415161718192021222324[VERBOSE-2:FlutterObservatoryPublisher.mm(131)] Could not register as server for FlutterObservatoryPublisher. Check your network settings and relaunch the application.SCNetworkReachabilitySetDispatchQueue() failed: Invalid argumentSCNetworkReachabilitySetDispatchQueue() failed: Invalid argumentlocalConnectionInitializedStatus:2localConnectionInitializedStatus:2SCNetworkReachabilitySetDispatchQueue() failed: Invalid argument[Bugly] Fatal signal(11) raised.[Bugly] Trapped fatal signal &apos;SIGSEGV(11)&apos; ( &quot;0 Flutter 0x000000010516f92c _ZNK3fml8internal14CopyableLambdaIZN7flutter5Shell21OnPlatformViewCreatedENSt3__110unique_ptrINS2_7SurfaceENS4_14default_deleteIS6_EEEEE3$_8EclIJEEEDaDpOT_ + 236&quot;, &quot;1 Flutter 0x000000010516f928 _ZNK3fml8internal14CopyableLambdaIZN7flutter5Shell21OnPlatformViewCreatedENSt3__110unique_ptrINS2_7SurfaceENS4_14default_deleteIS6_EEEEE3$_8EclIJEEEDaDpOT_ + 232&quot;, &quot;2 Flutter 0x0000000105123cf4 _ZN3fml15MessageLoopImpl10FlushTasksENS_9FlushTypeE + 1700&quot;, &quot;3 Flutter 0x0000000105126000 _ZN3fml17MessageLoopDarwin11OnTimerFireEP16__CFRunLoopTimerPS0_ + 32&quot;, &quot;4 CoreFoundation 0x0000000184cd3aa8 0x0000000184be5000 + 977576&quot;, &quot;5 CoreFoundation 0x0000000184cd376c 0x0000000184be5000 + 976748&quot;, &quot;6 CoreFoundation 0x0000000184cd3010 0x0000000184be5000 + 974864&quot;, &quot;7 CoreFoundation 0x0000000184cd0b60 0x0000000184be5000 + 965472&quot;, &quot;8 CoreFoundation 0x0000000184bf0da8 CFRunLoopRunSpecific + 552&quot;, &quot;9 Flutter 0x0000000105125edc _ZN3fml17MessageLoopDarwin3RunEv + 88&quot;, &quot;10 Flutter 0x0000000105125684 _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN3fml6ThreadC1ERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEEE3$_0EEEEEPvSJ_ + 200&quot;, &quot;11 libsystem_pthread.dylib 0x0000000184951220 0x000000018494f000 + 8736&quot;, &quot;12 libsystem_pthread.dylib 0x0000000184951110 0x000000018494f000 + 8464&quot;)Application finished. 处理办法： flutterVc dealloc 或者 disappear 的时候 执行 flutterEngine detach]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter engine 定制]]></title>
    <url>%2F2020%2F03%2F05%2Fflutter-engine-%E5%AE%9A%E5%88%B6%2F</url>
    <content type="text"><![CDATA[…使用Flutter开发的时候最直接接触的并不是 Flutter Engine 而是 Flutter Framework(https://github.com/flutter/flutter)在flutter framework 的 目录里面 有编译好的engine 产物 简单说就是， 编译引擎 替换 产物文件就好了 路径 flutter_path/bin/cache/artifacts/engine/ios 参考Flutter Engine定制流程Flutter Engine 编译指北]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter eventChannel crash on ios]]></title>
    <url>%2F2019%2F10%2F21%2Fflutter-eventChannel-crash-on-ios%2F</url>
    <content type="text"><![CDATA[flutter issuecrash 记录一次困扰了很久的 flutter event channel crash EventChannel dart1234567891011121314151617181920212223242526272829303132333435363738394041Stream&lt;dynamic&gt; receiveBroadcastStream([ dynamic arguments ]) &#123; final MethodChannel methodChannel = MethodChannel(name, codec); StreamController&lt;dynamic&gt; controller; controller = StreamController&lt;dynamic&gt;.broadcast(onListen: () async &#123; defaultBinaryMessenger.setMessageHandler(name, (ByteData reply) async &#123; if (reply == null) &#123; controller.close(); &#125; else &#123; try &#123; controller.add(codec.decodeEnvelope(reply)); &#125; on PlatformException catch (e) &#123; controller.addError(e); &#125; &#125; return null; &#125;); try &#123; await methodChannel.invokeMethod&lt;void&gt;('listen', arguments); &#125; catch (exception, stack) &#123; FlutterError.reportError(FlutterErrorDetails( exception: exception, stack: stack, library: 'services library', context: ErrorDescription('while activating platform stream on channel $name'), )); &#125; &#125;, onCancel: () async &#123; defaultBinaryMessenger.setMessageHandler(name, null); try &#123; await methodChannel.invokeMethod&lt;void&gt;('cancel', arguments); &#125; catch (exception, stack) &#123; FlutterError.reportError(FlutterErrorDetails( exception: exception, stack: stack, library: 'services library', context: ErrorDescription('while de-activating platform stream on channel $name'), )); &#125; &#125;); return controller.stream; &#125; FlutterChannel.mm1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static void SetStreamHandlerMessageHandlerOnChannel(NSObject&lt;FlutterStreamHandler&gt;* handler, NSString* name, NSObject&lt;FlutterBinaryMessenger&gt;* messenger, NSObject&lt;FlutterMethodCodec&gt;* codec) &#123; __block FlutterEventSink currentSink = nil; FlutterBinaryMessageHandler messageHandler = ^(NSData* message, FlutterBinaryReply callback) &#123; FlutterMethodCall* call = [codec decodeMethodCall:message]; if ([call.method isEqual:@"listen"]) &#123; if (currentSink) &#123; FlutterError* error = [handler onCancelWithArguments:nil]; if (error) NSLog(@"Failed to cancel existing stream: %@. %@ (%@)", error.code, error.message, error.details); &#125; currentSink = ^(id event) &#123; if (event == FlutterEndOfEventStream) [messenger sendOnChannel:name message:nil]; else if ([event isKindOfClass:[FlutterError class]]) [messenger sendOnChannel:name message:[codec encodeErrorEnvelope:(FlutterError*)event]]; else [messenger sendOnChannel:name message:[codec encodeSuccessEnvelope:event]]; &#125;; FlutterError* error = [handler onListenWithArguments:call.arguments eventSink:currentSink]; if (error) callback([codec encodeErrorEnvelope:error]); else callback([codec encodeSuccessEnvelope:nil]); &#125; else if ([call.method isEqual:@"cancel"]) &#123; if (!currentSink) &#123; callback( [codec encodeErrorEnvelope:[FlutterError errorWithCode:@"error" message:@"No active stream to cancel" details:nil]]); return; &#125; currentSink = nil; FlutterError* error = [handler onCancelWithArguments:call.arguments]; if (error) callback([codec encodeErrorEnvelope:error]); else callback([codec encodeSuccessEnvelope:nil]); &#125; else &#123; callback(nil); &#125; &#125;; [messenger setMessageHandlerOnChannel:name binaryMessageHandler:messageHandler];&#125; EventSink正常结束stream流 eventSink(FlutterEndOfEventStream) ，异常结束stream流 eventSink(FlutterError) 都会回调执行 onCancel 参考Flutter 与 Native(iOS) 通信原理深入Flutter技术内幕:Platform Channel设计与实现]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter ios 13 dark mode]]></title>
    <url>%2F2019%2F10%2F14%2Fflutter-ios-13-dark-mode%2F</url>
    <content type="text"><![CDATA[前言ios 13 开启 dark model,flutter页面status bar文字一直是白色 flutter issues 1SystemChrome.setSystemUIOverlayStyle(SystemUiOverlayStyle.dark); 设置dark style 并没有用 SystemChrome12345678910111213141516171819202122232425static void setSystemUIOverlayStyle(SystemUiOverlayStyle style) &#123; assert(style != null); if (_pendingStyle != null) &#123; // The microtask has already been queued; just update the pending value. _pendingStyle = style; return; &#125; if (style == _latestStyle) &#123; // Trivial success: no microtask has been queued and the given style is // already in effect, so no need to queue a microtask. return; &#125; _pendingStyle = style; scheduleMicrotask(() &#123; assert(_pendingStyle != null); if (_pendingStyle != _latestStyle) &#123; SystemChannels.platform.invokeMethod&lt;void&gt;( 'SystemChrome.setSystemUIOverlayStyle', _pendingStyle._toMap(), ); _latestStyle = _pendingStyle; &#125; _pendingStyle = null; &#125;); &#125; FlutterPlatformPlugin1234567891011121314151617181920212223242526272829- (void)setSystemChromeSystemUIOverlayStyle:(NSDictionary*)message &#123; NSString* style = message[@"statusBarBrightness"]; if (style == (id)[NSNull null]) return; UIStatusBarStyle statusBarStyle; if ([style isEqualToString:@"Brightness.dark"]) statusBarStyle = UIStatusBarStyleLightContent; else if ([style isEqualToString:@"Brightness.light"]) statusBarStyle = UIStatusBarStyleDefault; else return; NSNumber* infoValue = [[NSBundle mainBundle] objectForInfoDictionaryKey:@"UIViewControllerBasedStatusBarAppearance"]; Boolean delegateToViewController = (infoValue == nil || [infoValue boolValue]); if (delegateToViewController) &#123; // This notification is respected by the iOS embedder [[NSNotificationCenter defaultCenter] postNotificationName:@(kOverlayStyleUpdateNotificationName) object:nil userInfo:@&#123;@(kOverlayStyleUpdateNotificationKey) : @(statusBarStyle)&#125;]; &#125; else &#123; // Note: -[UIApplication setStatusBarStyle] is deprecated in iOS9 // in favor of delegating to the view controller [[UIApplication sharedApplication] setStatusBarStyle:statusBarStyle]; &#125;&#125; engine 源码中 可以看到 没有 UIStatusBarStyleDarkContent 尝试 去掉 info.plist 中的 UIViewControllerBasedStatusBarAppearance 然后 监听 通知 1234567[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(appStatusBar:) name:@"io.flutter.plugin.platform.SystemChromeOverlayNotificationName" object:nil];- (void)appStatusBar:(id)notification &#123; if (@available(iOS 13.0, *)) &#123; [UIApplication sharedApplication].statusBarStyle = UIStatusBarStyleDarkContent; &#125;&#125;]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mustache之dart]]></title>
    <url>%2F2019%2F05%2F31%2Fmustache%E4%B9%8Bdart%2F</url>
    <content type="text"><![CDATA[前言Mustache 是一个 logic-less （轻逻辑）模板解析引擎，可以应用在 js、PHP、Python、Perl 等多种编程语言中。这里主要是看dart中的应用。 模板语法很简单 看这里1234567&#123;&#123;keyName&#125;&#125; &#123;&#123;#keyName&#125;&#125; &#123;&#123;/keyName&#125;&#125;&#123;&#123;^keyName&#125;&#125; &#123;&#123;/keyName&#125;&#125;&#123;&#123;.&#125;&#125;&#123;&#123;&gt;partials&#125;&#125;&#123;&#123;&#123;keyName&#125;&#125;&#125;&#123;&#123;!comments&#125;&#125; 使用在flutter项目中，使用annation router注解的方式生成路由表管理类 RouterManager ，以及业务相关的类文件（ios android）在使用mustache之前，是通过stringbuff 的方式拼接字符串，也可以完成，但是阅读性比较差。 之前123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/// 生成路由表类 StringBuffer sb = new StringBuffer(); sb..write(_createImport())..write(_createClazz(element.name)); return sb.toString();/// 生成路由表类的 import 信息 String _createImport() &#123; StringBuffer sb = new StringBuffer(); sb ..writeln("import 'package:flutter/material.dart';") ..writeln("import 'package:hybrid_router/hybrid_router.dart';"); /// import page collector.importClazzList.forEach((clazz) &#123; sb.writeln("import '$clazz';"); &#125;); return sb.toString(); &#125; /// 生成路由表类的 clazz 信息 String _createClazz(String className) &#123; StringBuffer sb = new StringBuffer(); /// start class sb.writeln("class \$$className &#123;"); /// generateRoute function sb ..writeln(" Map&lt;String, HybridWidgetBuilder&gt; generateRoutes()&#123;") ..writeln(" return &#123;"); collector.routeMap.forEach((key, value) &#123; String flutterPath = value.flutterPath; sb ..writeln(" '$flutterPath': (BuildContext context, Object args) &#123;") ..writeln(" $&#123;_createInstance(value)&#125;") ..writeln(" &#125;,"); &#125;); sb..writeln(" &#125;;")..writeln(" &#125;"); /// generateSpm function sb ..writeln(" Map&lt;String, String&gt; generateSpm() &#123;") ..writeln(" return &#123;"); collector.routeMap.forEach((key, value) &#123; String flutterSpm = value.flutterSpm; String flutterPath = value.flutterPath; if (flutterSpm?.isNotEmpty == true) &#123; sb.writeln(" '$flutterPath': '$flutterSpm',"); &#125; &#125;); sb..writeln(" &#125;;")..writeln(" &#125;"); /// end class sb.writeln("&#125;"); return sb.toString(); &#125; 之后1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/// 生成路由表类 _createRouteManager(element.name); String _createRouteManager(String className) &#123; return VDTemplate.routerManagerTemplate().renderString(&#123; 'classname':className, 'classes':collector.importClazzList, 'routes':collector.routeMap.values, 'createInstance':(LambdaContext ctx) &#123; CollectorItem item = ctx.lookup('.'); return _createInstance(item); &#125; &#125;); &#125; static Template routerManagerTemplate() &#123; var source = '''&#123;&#123;&gt; import&#125;&#125;&#123;&#123;&gt; clazz&#125;&#125; '''; Map&lt;String,Template&gt; map = &#123; "import":VDTemplate.importTemplate(), "clazz":VDTemplate.claszzTemplate() &#125;; return new Template(source,partialResolver: (String name) =&gt; map[name]); &#125; static Template importTemplate() &#123; var source = '''import 'package:flutter/material.dart';import 'package:hybrid_router/hybrid_router.dart';&#123;&#123;# classes &#125;&#125;import '&#123;&#123;&#123;.&#125;&#125;&#125;';&#123;&#123;/ classes &#125;&#125; '''; return new Template(source); &#125; static Template claszzTemplate() &#123; var source = '''class \$&#123;&#123;classname&#125;&#125; &#123; Map&lt;String, HybridWidgetBuilder&gt; generateRoutes()&#123; return &#123; &#123;&#123;#routes&#125;&#125; '&#123;&#123;flutterPath&#125;&#125;': (BuildContext context, Object args) &#123; &#123;&#123;createInstance&#125;&#125; &#125;, &#123;&#123;/routes&#125;&#125; &#125;; &#125; Map&lt;String, String&gt; generateSpm() &#123; return &#123; &#123;&#123;#routes&#125;&#125; &#123;&#123;# flutterSpm&#125;&#125; '&#123;&#123;flutterPath&#125;&#125;': '&#123;&#123;flutterSpm&#125;&#125;', &#123;&#123;/ flutterSpm&#125;&#125; &#123;&#123;/routes&#125;&#125; &#125;; &#125; &#125; '''; return new Template(source); &#125; 对比看下，使用mustache之后，可读性好很多，基本保持了代码结构 总结 字符串数组，可以使用{{.}} 对象数据，可以跟普通的hash一样，直接用{{对象的属性}}，mustache内部通过dart反射拿到属性值 使用partials 拆分template 增加可读性 通过lambda函数执行dart方法，也可以做到拆分的作用 lambdaContext.loopup(&quot;.&quot;) 可以获取对象实例，进而可以参数传递 mustache内部renderstring也是通过stringbuff的方式实现 参考链接mustache 1.1.1Flutter路由管理]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>mustache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm调优]]></title>
    <url>%2F2019%2F05%2F23%2Fjvm%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[jps输出JVM中运行的进程状态信息 1234-q 不输出类名、Jar名和传入main方法的参数-m 输出传入main方法的参数-l 输出main类或Jar的全限名-v 输出传入JVM的参数 jps -mlv找到java应用的pid jstack根据java应用pid ,查看进程中线程堆栈信息 jstack pid top找出该进程内最耗费CPU的线程 top -Hp pid 转为十六进制printf &quot;%x\n&quot; 线程id 输出进程的堆栈信息，然后根据线程ID的十六进制值grepjstack pid | grep 十六进制线程id jmap查看堆内存使用状况jmap -heap pid 进程内存使用情况dump到文件中 结合MAT工具分析jmap -dump:format=b,file=dumpFileName pid jhatjhat -port 9998 /tmp/dump.datlocalhost:9998 查看内存对象情况 （不如MAT直观）]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vmstatu]]></title>
    <url>%2F2019%2F05%2F22%2Fvmstatus%2F</url>
    <content type="text"><![CDATA[vmstatuvmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。相比top，可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。 1234$vmstatprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 215220 0 771404 0 0 2 15 0 1 0 0 100 0 0 一般vmstat工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔数，单位是秒，第二个参数是采样的次数 12345$vmstat 2 2procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 202924 0 785780 0 0 2 15 0 1 0 0 100 0 0 0 0 0 203032 0 785812 0 0 0 155 748 1382 0 0 100 0 0 第二个参数如果没有，就会一直采集（ctrl+c 结束） 12345678$vmstat 2procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 194712 0 794728 0 0 2 15 0 1 0 0 100 0 0 0 0 0 194696 0 794768 0 0 0 50 782 1368 0 0 100 0 0 0 0 0 193828 0 794776 0 0 0 108 752 1156 0 0 100 0 0 0 0 0 193952 0 794804 0 0 0 4 601 997 0 0 100 0 0^C 字段procs r 等待运行的进程数 b 处在非中断睡眠状态的进程数 memory （KB） swpd 虚拟内存使用大小 注意：如果swpd的值不为0，但是SI，SO的值长期为0，这种情况不会影响系统性能。 free 空闲的内存 buff 用作缓冲的内存大小 cache 用作缓存的内存大小 注意：如果cache的值大的时候，说明cache处的文件数多，如果频繁访问到的文件都能被cache处，那么磁盘的读IO bi会非常小。 swap si 从交换区写到内存的大小 so 每秒写入交换区的内存大小 内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有些朋友看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的。 io bi 每秒读取的块数 bo 每秒写入的块数 注意：随机磁盘读写的时候，这2个值越大（如超出1024k)，能看到CPU在IO等待的值也会越大。 system in 每秒中断数，包括时钟中断。 cs 每秒上下文切换数。 注意：上面2个值越大，会看到由内核消耗的CPU时间会越大。 cpu us 用户进程执行时间(user time) 注意： us的值比较高时，说明用户进程消耗的CPU时间多，但是如果长期超50%的使用，那么我们就该考虑优化程序算法或者进行加速。 sy 系统进程执行时间(system time) 注意：sy的值高时，说明系统内核消耗的CPU资源多，这并不是良性表现，我们应该检查原因。 id 空闲时间(包括IO等待时间),中央处理器的空闲时间 。以百分比表示。 wa 等待IO时间百分比 注意：wa的值高时，说明IO等待比较严重，这可能由于磁盘大量作随机访问造成，也有可能磁盘出现瓶颈（块操作）。 1234567891011121314151617181920212223242526Procsr: The number of processes waiting for run time.b: The number of processes in uninterruptible sleep.Memoryswpd: the amount of virtual memory used.free: the amount of idle memory.buff: the amount of memory used as buffers.cache: the amount of memory used as cache.inact: the amount of inactive memory. (-a option)active: the amount of active memory. (-a option)Swapsi: Amount of memory swapped in from disk (/s).so: Amount of memory swapped to disk (/s).IObi: Blocks received from a block device (blocks/s).bo: Blocks sent to a block device (blocks/s).Systemin: The number of interrupts per second, including the clock.cs: The number of context switches per second.CPUThese are percentages of total CPU time.us: Time spent running non-kernel code. (user time, including nice time)sy: Time spent running kernel code. (system time)id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time.wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle.st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown.]]></content>
      <tags>
        <tag>java</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter plugin registry]]></title>
    <url>%2F2019%2F05%2F17%2Fflutter-plugin-registry%2F</url>
    <content type="text"><![CDATA[前言首先这里有三个形似得英文单词registry, registrar and registrant分别对应注册局，注册商和注册人。把它们翻译到现实的生活场景中的角色其实是一个“注册人通过注册商，更新注册信息后，注册商把信息传递给注册局进行保存”的过程。 注册人：GeneratedPluginRegistrant注册局：[(FlutterViewController*)rootViewController pluginRegistry] == flutterEngine注册商：FlutterEngineRegistrar Flutter Applicationflutter create -t plugin my_plugin xcode 打开 my_plugin/example/ios路径下的 Runner工程 AppDelegate12345678910@implementation AppDelegate- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions &#123; [GeneratedPluginRegistrant registerWithRegistry:self]; // Override point for customization after application launch. return [super application:application didFinishLaunchingWithOptions:launchOptions];&#125;@end GeneratedPluginRegistrant1234567@implementation GeneratedPluginRegistrant+ (void)registerWithRegistry:(NSObject&lt;FlutterPluginRegistry&gt;*)registry &#123; [MyPlugin registerWithRegistrar:[registry registrarForPlugin:@"MyPlugin"]];&#125;@end AppDelegate继承FlutterAppDelegate FlutterAppDelegate123456789#pragma mark - FlutterPluginRegistry methods. All delegating to the rootViewController- (NSObject&lt;FlutterPluginRegistrar&gt;*)registrarForPlugin:(NSString*)pluginKey &#123; UIViewController* rootViewController = _window.rootViewController; if ([rootViewController isKindOfClass:[FlutterViewController class]]) &#123; return [[(FlutterViewController*)rootViewController pluginRegistry] registrarForPlugin:pluginKey]; &#125; return nil;&#125; FlutterViewController123- (id&lt;FlutterPluginRegistry&gt;)pluginRegistry &#123; return _engine;&#125; FlutterEngineRegistrar1234567891011121314151617181920212223@implementation FlutterEngineRegistrar &#123; NSString* _pluginKey; FlutterEngine* _flutterEngine;&#125;- (instancetype)initWithPlugin:(NSString*)pluginKey flutterEngine:(FlutterEngine*)flutterEngine &#123; self = [super init]; NSAssert(self, @"Super init cannot be nil"); _pluginKey = [pluginKey retain]; _flutterEngine = [flutterEngine retain]; return self;&#125;- (void)addMethodCallDelegate:(NSObject&lt;FlutterPlugin&gt;*)delegate channel:(FlutterMethodChannel*)channel &#123; [channel setMethodCallHandler:^(FlutterMethodCall* call, FlutterResult result) &#123; [delegate handleMethodCall:call result:result]; &#125;];&#125;- (NSObject&lt;FlutterBinaryMessenger&gt;*)messenger &#123; return _flutterEngine;&#125; MyPlugin123456789101112131415161718@implementation MyPlugin+ (void)registerWithRegistrar:(NSObject&lt;FlutterPluginRegistrar&gt;*)registrar &#123; FlutterMethodChannel* channel = [FlutterMethodChannel methodChannelWithName:@"my_plugin" binaryMessenger:[registrar messenger]]; MyPlugin* instance = [[MyPlugin alloc] init]; [registrar addMethodCallDelegate:instance channel:channel];&#125;- (void)handleMethodCall:(FlutterMethodCall*)call result:(FlutterResult)result &#123; if ([@"getPlatformVersion" isEqualToString:call.method]) &#123; result([@"iOS " stringByAppendingString:[[UIDevice currentDevice] systemVersion]]); &#125; else &#123; result(FlutterMethodNotImplemented); &#125;&#125;@end FlutterMethodChannel1234567891011121314151617181920212223242526272829- (instancetype)initWithName:(NSString*)name binaryMessenger:(NSObject&lt;FlutterBinaryMessenger&gt;*)messenger codec:(NSObject&lt;FlutterMethodCodec&gt;*)codec &#123; self = [super init]; NSAssert(self, @"Super init cannot be nil"); _name = [name retain]; _messenger = [messenger retain]; //flutterEngine _codec = [codec retain]; return self;&#125;- (void)setMethodCallHandler:(FlutterMethodCallHandler)handler &#123; if (!handler) &#123; [_messenger setMessageHandlerOnChannel:_name binaryMessageHandler:nil]; return; &#125; FlutterBinaryMessageHandler messageHandler = ^(NSData* message, FlutterBinaryReply callback) &#123; FlutterMethodCall* call = [_codec decodeMethodCall:message]; handler(call, ^(id result) &#123; if (result == FlutterMethodNotImplemented) callback(nil); else if ([result isKindOfClass:[FlutterError class]]) callback([_codec encodeErrorEnvelope:(FlutterError*)result]); else callback([_codec encodeSuccessEnvelope:result]); &#125;); &#125;; [_messenger setMessageHandlerOnChannel:_name binaryMessageHandler:messageHandler];&#125; FlutterEngine123456789101112131415#pragma mark - FlutterPluginRegistry- (NSObject&lt;FlutterPluginRegistrar&gt;*)registrarForPlugin:(NSString*)pluginKey &#123; NSAssert(self.pluginPublications[pluginKey] == nil, @"Duplicate plugin key: %@", pluginKey); self.pluginPublications[pluginKey] = [NSNull null]; return [[FlutterEngineRegistrar alloc] initWithPlugin:pluginKey flutterEngine:self];&#125;- (void)setMessageHandlerOnChannel:(NSString*)channel binaryMessageHandler:(FlutterBinaryMessageHandler)handler &#123; NSAssert(channel, @"The channel must not be null"); FML_DCHECK(_shell &amp;&amp; _shell-&gt;IsSetup()); self.iosPlatformView-&gt;GetPlatformMessageRouter().SetMessageHandler(channel.UTF8String, handler);&#125; FlutterBinaryMessage123456789101112131415161718/** * A message reply callback. * * Used for submitting a binary reply back to a Flutter message sender. Also used * in for handling a binary message reply received from Flutter. * * @param reply The reply. */typedef void (^FlutterBinaryReply)(NSData* _Nullable reply);/** * A strategy for handling incoming binary messages from Flutter and to send * asynchronous replies back to Flutter. * * @param message The message. * @param reply A callback for submitting an asynchronous reply to the sender. */typedef void (^FlutterBinaryMessageHandler)(NSData* _Nullable message, FlutterBinaryReply reply);]]></content>
  </entry>
  <entry>
    <title><![CDATA[sublime text 3]]></title>
    <url>%2F2019%2F05%2F14%2Fsublime-text-3%2F</url>
    <content type="text"><![CDATA[sublime text3 install package controlTools -&gt; Install package control 报错信息工具栏View 点击show console 或者快捷键 ctrl+` 打开控制台看下如下报错信息 1234Visit https://packagecontrol.io/installation for manual instructionsError installing Package Control: HTTPS error encountered, falling back to HTTP - &lt;urlopen error [Errno 60] Operation timed out&gt;Error installing Package Control: HTTP error encountered, giving up - &lt;urlopen error [Errno 60] Operation timed out&gt;error: An error occurred installing Package Control 处理办法绑定域名 1250.116.34.243 sublime.wbond.net50.116.34.243 packagecontrol.io install package报错信息There are no packages available for installation 处理办法下载 channel_v3.json 文件 （google一下） 修改package control.sublime-settings 123&quot;channels&quot;: [ &quot;/path/to/channel_v3.json&quot;]]]></content>
  </entry>
  <entry>
    <title><![CDATA[vim shortcuts]]></title>
    <url>%2F2019%2F05%2F14%2Fvim-shortcuts%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flutter之Dart编译]]></title>
    <url>%2F2019%2F05%2F10%2FFlutter%E4%B9%8BDart%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[前言App中使用flutter已经有段时间了，最近遇到一个bug记录一下。更新flutter module工程pubspec plugin依赖，App工程中pod update之后，从功能表现上看依然是老代码。第一感觉是缓存导致的，xcode clean 以及删除DerivedData目录重新build依然不行，flutter module工程中执行flutter clean然后xcode build是正常的，所以应该是dart编译产物有缓存导致的。接下来看下dart编译过程。 编译12cd path/to/flutter moduleflutter build ios --debug --simulator 进入到flutter module工程目录 执行flutter build ios命令 12345Running Xcode build... ├─Assembling Flutter resources... 3.6s └─Compiling, linking and signing... 25.4sXcode build done. 43.9s 可以看到会进行xcode build，进到.ios目录通过xcode打开Runner工程 可以看到build phases中这样一段脚本，这里就是执行dart代码编译的入口。 xcode_backend.sh进入到脚本所在目录，看下build对应的方法 BuildApp 12345678910111213if [[ $# == 0 ]]; then # Backwards-compatibility: if no args are provided, build. BuildAppelse case $1 in "build") BuildApp ;; "thin") ThinAppFrameworks ;; "embed") EmbedFlutterFrameworks ;; esacfi 123456789101112131415161718192021222324252627282930BuildApp() &#123; ... StreamOutput " ├─Assembling Flutter resources..." RunCommand "$&#123;FLUTTER_ROOT&#125;/bin/flutter" --suppress-analytics \ $&#123;verbose_flag&#125; \ build bundle \ --target-platform=ios \ --target="$&#123;target_path&#125;" \ --$&#123;build_mode&#125; \ --depfile="$&#123;build_dir&#125;/snapshot_blob.bin.d" \ --asset-dir="$&#123;derived_dir&#125;/App.framework/$&#123;assets_path&#125;" \ $&#123;precompilation_flag&#125; \ $&#123;flutter_engine_flag&#125; \ $&#123;local_engine_flag&#125; \ $&#123;track_widget_creation_flag&#125; if [[ $? -ne 0 ]]; then EchoError "Failed to package $&#123;project_path&#125;." exit -1 fi StreamOutput "done" StreamOutput " └─Compiling, linking and signing..." RunCommand popd &gt; /dev/null echo "Project $&#123;project_path&#125; built and packaged successfully." return 0&#125; 可以看到 ├─Assembling Flutter resources… 在build ios 执行过程中出现过，flutter build bundle 就会开始真正的dart编译–depfile 指定参与编译的dart文件路径集合–asset-dir 指定资源产物的目录 flutter命令路径 $FLUTTER_ROOT/bin/flutter 123456789101112...FLUTTER_TOOLS_DIR="$FLUTTER_ROOT/packages/flutter_tools"SNAPSHOT_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.snapshot"STAMP_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.stamp"SCRIPT_PATH="$FLUTTER_TOOLS_DIR/bin/flutter_tools.dart"DART_SDK_PATH="$FLUTTER_ROOT/bin/cache/dart-sdk"DART="$DART_SDK_PATH/bin/dart"PUB="$DART_SDK_PATH/bin/pub""$DART" $FLUTTER_TOOL_ARGS "$SNAPSHOT_PATH" "$@" flutter_toools.snapshot实际上就是$FLUTTER_ROOT/packages/flutter_tools这个项目编译生成的snapshot文件 所以flutter build bundle 就是使用dart来执行flutter_tools项目的main方法 flutter_tools路径 $FLUTTER_ROOT/packages/flutter_tools main方法定义 $FLUTTER_ROOT/packages/flutter_tools/bin/flutter_tools.dart 123void main(List&lt;String&gt; args) &#123; executable.main(args);&#125; 再看看 lib/executable.dart ,在这里会预先创建好每一种命令对应的对象command，通过解析args参数找到对应的command。在BuildCommand类中 123456789BuildCommand(&#123;bool verboseHelp = false&#125;) &#123; addSubcommand(BuildApkCommand(verboseHelp: verboseHelp)); addSubcommand(BuildAppBundleCommand(verboseHelp: verboseHelp)); addSubcommand(BuildAotCommand()); addSubcommand(BuildIOSCommand()); addSubcommand(BuildFlxCommand()); addSubcommand(BuildBundleCommand(verboseHelp: verboseHelp)); addSubcommand(BuildWebCommand()); &#125; 看到BuildIOSCommand 以及 BuildBundleCommand的创建。BuildIOSCommand 就是前面提到的flutter build ios 会执行到的，这里我们重点看下BuildBundleCommand是如何编译dart代码的？编译后生成了哪些资源？这些资源都是些什么？ BuildBundleCommand app.dill : 这就是dart代码编译后的二级制文件 Frontend_server.d : 这里面放的是frontend_server.dart.snapshot的绝对路径，使用该snapshot来编译dart代码生成上面的app.dill snapshot_blob.bin.d : 这里面放的是所有参与编译的dart文件的绝对路径的集合，包括项目的代码和flutterSdk的代码以及pub库中的三方代码。 snapshot_blob.bin.d.fingerprint : 这里面放的是snapshot_blob.bin.d中的所有文件的绝对路径以及每个文件所对应的md5值。使用这个md5来判断该文件是否有修改。在每次编译的时候会判断，如果没有文件修改，则直接跳过编译。 编译Dart资源123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127Future&lt;void&gt; build(&#123; TargetPlatform platform, BuildMode buildMode, String mainPath = defaultMainPath, String manifestPath = defaultManifestPath, String applicationKernelFilePath, String depfilePath, String privateKeyPath = defaultPrivateKeyPath, String assetDirPath, String packagesPath, bool precompiledSnapshot = false, bool reportLicensedPackages = false, bool trackWidgetCreation = false, String compilationTraceFilePath, bool createPatch = false, String buildNumber, String baselineDir, List&lt;String&gt; extraFrontEndOptions = const &lt;String&gt;[], List&lt;String&gt; extraGenSnapshotOptions = const &lt;String&gt;[], List&lt;String&gt; fileSystemRoots, String fileSystemScheme,&#125;) async &#123; // xcode_backend.sh中通过--depfile传入进来的 // 默认是build/snapshot_blob.bin.d文件 depfilePath ??= defaultDepfilePath; // 通过--asset-dir传入 // 该目录中文件就是flutter的产物，最终合并到app.framework中的flutter_assets目录 assetDirPath ??= getAssetBuildDirectory(); packagesPath ??= fs.path.absolute(PackageMap.globalPackagesPath); // app.dill dart代码编译后的二级制文件 applicationKernelFilePath ??= getDefaultApplicationKernelPath(trackWidgetCreation: trackWidgetCreation); final FlutterProject flutterProject = await FlutterProject.current(); if (compilationTraceFilePath != null) &#123; if (buildMode != BuildMode.dynamicProfile &amp;&amp; buildMode != BuildMode.dynamicRelease) &#123; compilationTraceFilePath = null; &#125; else if (compilationTraceFilePath.isEmpty) &#123; // Disable JIT snapshotting if flag is empty. printStatus('Code snapshot will be disabled for this build.'); compilationTraceFilePath = null; &#125; else if (!fs.file(compilationTraceFilePath).existsSync()) &#123; // Be forgiving if compilation trace file is missing. printStatus('No compilation trace available. To optimize performance, consider using --train.'); final File tmp = fs.systemTempDirectory.childFile('flutterEmptyCompilationTrace.txt'); compilationTraceFilePath = (tmp..createSync(recursive: true)).path; &#125; else &#123; printStatus('Code snapshot will use compilation training file $compilationTraceFilePath.'); &#125; &#125; DevFSContent kernelContent; if (!precompiledSnapshot) &#123; if ((extraFrontEndOptions != null) &amp;&amp; extraFrontEndOptions.isNotEmpty) printTrace('Extra front-end options: $extraFrontEndOptions'); ensureDirectoryExists(applicationKernelFilePath); final KernelCompiler kernelCompiler = await kernelCompilerFactory.create(flutterProject); // 编译dart代码，生成app.dill 和 snapshot_blob.bin.d 以及 snapshot_blob.bin.d.fingerprint final CompilerOutput compilerOutput = await kernelCompiler.compile( sdkRoot: artifacts.getArtifactPath(Artifact.flutterPatchedSdkPath), incrementalCompilerByteStorePath: compilationTraceFilePath != null ? null : fs.path.absolute(getIncrementalCompilerByteStoreDirectory()), mainPath: fs.file(mainPath).absolute.path, outputFilePath: applicationKernelFilePath, depFilePath: depfilePath, trackWidgetCreation: trackWidgetCreation, extraFrontEndOptions: extraFrontEndOptions, fileSystemRoots: fileSystemRoots, fileSystemScheme: fileSystemScheme, packagesPath: packagesPath, linkPlatformKernelIn: compilationTraceFilePath != null, ); if (compilerOutput?.outputFilename == null) &#123; throwToolExit('Compiler failed on $mainPath'); &#125; kernelContent = DevFSFileContent(fs.file(compilerOutput.outputFilename)); // 生成 frontend_server.d文件，向文件中写入frontendServerSnapshotForEngineDartSdk的路径 await fs.directory(getBuildDirectory()).childFile('frontend_server.d') .writeAsString('frontend_server.d: $&#123;artifacts.getArtifactPath(Artifact.frontendServerSnapshotForEngineDartSdk)&#125;\n'); if (compilationTraceFilePath != null) &#123; final JITSnapshotter snapshotter = JITSnapshotter(); final int snapshotExitCode = await snapshotter.build( platform: platform, buildMode: buildMode, mainPath: applicationKernelFilePath, outputPath: getBuildDirectory(), packagesPath: packagesPath, compilationTraceFilePath: compilationTraceFilePath, extraGenSnapshotOptions: extraGenSnapshotOptions, createPatch: createPatch, buildNumber: buildNumber, baselineDir: baselineDir, ); if (snapshotExitCode != 0) &#123; throwToolExit('Snapshotting exited with non-zero exit code: $snapshotExitCode'); &#125; &#125; &#125; // 生成 flutter_assets final AssetBundle assets = await buildAssets( manifestPath: manifestPath, assetDirPath: assetDirPath, packagesPath: packagesPath, reportLicensedPackages: reportLicensedPackages, ); if (assets == null) throwToolExit('Error building assets', exitCode: 1); await assemble( buildMode: buildMode, assetBundle: assets, kernelContent: kernelContent, privateKeyPath: privateKeyPath, assetDirPath: assetDirPath, compilationTraceFilePath: compilationTraceFilePath, );&#125; 编译Dart代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class KernelCompiler &#123; const KernelCompiler(); Future&lt;CompilerOutput&gt; compile(&#123; String sdkRoot, String mainPath, String outputFilePath, String depFilePath, TargetModel targetModel = TargetModel.flutter, bool linkPlatformKernelIn = false, bool aot = false, @required bool trackWidgetCreation, List&lt;String&gt; extraFrontEndOptions, String incrementalCompilerByteStorePath, String packagesPath, List&lt;String&gt; fileSystemRoots, String fileSystemScheme, bool targetProductVm = false, String initializeFromDill, &#125;) async &#123; final String frontendServer = artifacts.getArtifactPath( Artifact.frontendServerSnapshotForEngineDartSdk ); FlutterProject flutterProject; if (fs.file('pubspec.yaml').existsSync()) &#123; flutterProject = await FlutterProject.current(); &#125; // TODO(cbracken): eliminate pathFilter. // Currently the compiler emits buildbot paths for the core libs in the // depfile. None of these are available on the local host. Fingerprinter fingerprinter; // 如果snapshot_blob.bin.d文件不为空，则说明有编译缓存 if (depFilePath != null) &#123; // 判断与上次编译对比，是否有文件的md5改变 fingerprinter = Fingerprinter( // snapshot_blob.bin.d.fingerprint文件 fingerprintPath: '$depFilePath.fingerprint', paths: &lt;String&gt;[mainPath], properties: &lt;String, String&gt;&#123; 'entryPoint': mainPath, 'trackWidgetCreation': trackWidgetCreation.toString(), 'linkPlatformKernelIn': linkPlatformKernelIn.toString(), 'engineHash': Cache.instance.engineRevision, 'buildersUsed': '$&#123;flutterProject != null ? flutterProject.hasBuilders : false&#125;', &#125;, depfilePaths: &lt;String&gt;[depFilePath], pathFilter: (String path) =&gt; !path.startsWith('/b/build/slave/'), ); // 判断是否有文件改动，如果没有，则直接返回。 if (await fingerprinter.doesFingerprintMatch()) &#123; printTrace('Skipping kernel compilation. Fingerprint match.'); return CompilerOutput(outputFilePath, 0, /* sources */ null); &#125; &#125; ... // 如果没有上次编译缓存，或者文件有改变，Fingerprinter不匹配，则使用dart重新编译 final List&lt;String&gt; command = &lt;String&gt;[ engineDartPath, frontendServer, '--sdk-root', sdkRoot, '--strong', '--target=$targetModel', ]; ... //参数拼接 final Process server = await processManager .start(command) .catchError((dynamic error, StackTrace stack) &#123; printError('Failed to start frontend server $error, $stack'); &#125;); final StdoutHandler _stdoutHandler = StdoutHandler(); server.stderr .transform&lt;String&gt;(utf8.decoder) .listen(printError); server.stdout .transform&lt;String&gt;(utf8.decoder) .transform&lt;String&gt;(const LineSplitter()) .listen(_stdoutHandler.handler); final int exitCode = await server.exitCode; if (exitCode == 0) &#123; if (fingerprinter != null) &#123; await fingerprinter.writeFingerprint(); &#125; return _stdoutHandler.compilerOutput.future; &#125; return null; &#125;&#125; Fingerprint对比1234567891011121314151617181920212223242526Future&lt;bool&gt; doesFingerprintMatch() async &#123; try &#123; // 获取到当前的 snapshot_blob.bin.d.fingerprint文件 final File fingerprintFile = fs.file(fingerprintPath); if (!fingerprintFile.existsSync()) return false; if (!_depfilePaths.every(fs.isFileSync)) return false; final List&lt;String&gt; paths = await _getPaths(); if (!paths.every(fs.isFileSync)) return false; // 读取缓存的的snapshot_blob.bin.d.fingerprint文件，构建一个老的Fingerprint对象 final Fingerprint oldFingerprint = Fingerprint.fromJson(await fingerprintFile.readAsString()); // 构建一个新的Fingerprint对象 final Fingerprint newFingerprint = await buildFingerprint(); // 对比两次的文件集合中的每个文件的md5是否一样 return oldFingerprint == newFingerprint; &#125; catch (e) &#123; // Log exception and continue, fingerprinting is only a performance improvement. printTrace('Fingerprint check error: $e'); &#125; return false; &#125; 重点 看看 newFingerprint 12345678910111213Future&lt;Fingerprint&gt; buildFingerprint() async &#123; final List&lt;String&gt; paths = await _getPaths(); return Fingerprint.fromBuildInputs(_properties, paths);&#125;Future&lt;List&lt;String&gt;&gt; _getPaths() async &#123; final Set&lt;String&gt; paths = _paths.toSet(); // 使用缓存的snapshot_blob.bin.d文件中的文件集合 for (String depfilePath in _depfilePaths) paths.addAll(await readDepfile(depfilePath)); final FingerprintPathFilter filter = _pathFilter ?? (String path) =&gt; true; return paths.where(filter).toList()..sort();&#125; 可以看到newFingerprint 路径依旧是使用缓存的路径，依次计算路径对应文件的md5，所以问题就在这里了 执行flutter packages upgrade更新pub依赖的时候，build目录下的缓存产物并不会有任何变动，路径依然是老的路径。有一种情况就是module工程 lib 目录下的dat文件有改动，newFingerprint就会跟old不一样，这会重新编译dart，这里又有一个问题，就是如果lib目录下是新增dart文件 则不会被编译进去。 最后综上，执行flutter clean命令，清空build目录缓存文件，build ios 就会重新编译整个dart文件，包括pub依赖中的。 参考链接Flutter深入之flutter-build-bundle命令如何编译Dart?]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
</search>
