<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ffmpeg源码之xx学习笔记]]></title>
    <url>%2F2023%2F01%2F16%2Fffmpeg%E6%BA%90%E7%A0%81%E4%B9%8Bxx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[AVClass12345678910111213AVFormatContext *s;s-&gt;av_class = &amp;av_format_context_class;static const AVClass av_format_context_class = &#123; .class_name = "AVFormatContext", .item_name = format_to_name, .option = avformat_options, .version = LIBAVUTIL_VERSION_INT, .child_next = format_child_next, .child_class_iterate = format_child_class_iterate, .category = AV_CLASS_CATEGORY_MUXER, .get_category = get_category,&#125;; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#define OFFSET(x) offsetof(AVFormatContext,x)static const AVOption avformat_options[] = &#123;&#123;"avioflags", NULL, OFFSET(avio_flags), AV_OPT_TYPE_FLAGS, &#123;.i64 = DEFAULT &#125;, INT_MIN, INT_MAX, D|E, "avioflags"&#125;,&#123;"direct", "reduce buffering", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVIO_FLAG_DIRECT &#125;, INT_MIN, INT_MAX, D|E, "avioflags"&#125;,&#123;"probesize", "set probing size", OFFSET(probesize), AV_OPT_TYPE_INT64, &#123;.i64 = 5000000 &#125;, 32, INT64_MAX, D&#125;,&#123;"formatprobesize", "number of bytes to probe file format", OFFSET(format_probesize), AV_OPT_TYPE_INT, &#123;.i64 = PROBE_BUF_MAX&#125;, 0, INT_MAX-1, D&#125;,&#123;"packetsize", "set packet size", OFFSET(packet_size), AV_OPT_TYPE_INT, &#123;.i64 = DEFAULT &#125;, 0, INT_MAX, E&#125;,&#123;"fflags", NULL, OFFSET(flags), AV_OPT_TYPE_FLAGS, &#123;.i64 = AVFMT_FLAG_AUTO_BSF &#125;, INT_MIN, INT_MAX, D|E, "fflags"&#125;,&#123;"flush_packets", "reduce the latency by flushing out packets immediately", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_FLUSH_PACKETS &#125;, INT_MIN, INT_MAX, E, "fflags"&#125;,&#123;"ignidx", "ignore index", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_IGNIDX &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"genpts", "generate pts", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_GENPTS &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"nofillin", "do not fill in missing values that can be exactly calculated", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_NOFILLIN &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"noparse", "disable AVParsers, this needs nofillin too", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_NOPARSE &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"igndts", "ignore dts", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_IGNDTS &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"discardcorrupt", "discard corrupted frames", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_DISCARD_CORRUPT &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"sortdts", "try to interleave outputted packets by dts", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_SORT_DTS &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"fastseek", "fast but inaccurate seeks", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_FAST_SEEK &#125;, INT_MIN, INT_MAX, D, "fflags"&#125;,&#123;"nobuffer", "reduce the latency introduced by optional buffering", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_FLAG_NOBUFFER &#125;, 0, INT_MAX, D, "fflags"&#125;,&#123;"bitexact", "do not write random/volatile data", 0, AV_OPT_TYPE_CONST, &#123; .i64 = AVFMT_FLAG_BITEXACT &#125;, 0, 0, E, "fflags" &#125;,&#123;"shortest", "stop muxing with the shortest stream", 0, AV_OPT_TYPE_CONST, &#123; .i64 = AVFMT_FLAG_SHORTEST &#125;, 0, 0, E, "fflags" &#125;,&#123;"autobsf", "add needed bsfs automatically", 0, AV_OPT_TYPE_CONST, &#123; .i64 = AVFMT_FLAG_AUTO_BSF &#125;, 0, 0, E, "fflags" &#125;,&#123;"seek2any", "allow seeking to non-keyframes on demuxer level when supported", OFFSET(seek2any), AV_OPT_TYPE_BOOL, &#123;.i64 = 0 &#125;, 0, 1, D&#125;,&#123;"analyzeduration", "specify how many microseconds are analyzed to probe the input", OFFSET(max_analyze_duration), AV_OPT_TYPE_INT64, &#123;.i64 = 0 &#125;, 0, INT64_MAX, D&#125;,&#123;"cryptokey", "decryption key", OFFSET(key), AV_OPT_TYPE_BINARY, &#123;.dbl = 0&#125;, 0, 0, D&#125;,&#123;"indexmem", "max memory used for timestamp index (per stream)", OFFSET(max_index_size), AV_OPT_TYPE_INT, &#123;.i64 = 1&lt;&lt;20 &#125;, 0, INT_MAX, D&#125;,&#123;"rtbufsize", "max memory used for buffering real-time frames", OFFSET(max_picture_buffer), AV_OPT_TYPE_INT, &#123;.i64 = 3041280 &#125;, 0, INT_MAX, D&#125;, /* defaults to 1s of 15fps 352x288 YUYV422 video */&#123;"fdebug", "print specific debug info", OFFSET(debug), AV_OPT_TYPE_FLAGS, &#123;.i64 = DEFAULT &#125;, 0, INT_MAX, E|D, "fdebug"&#125;,&#123;"ts", NULL, 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_FDEBUG_TS &#125;, INT_MIN, INT_MAX, E|D, "fdebug"&#125;,&#123;"max_delay", "maximum muxing or demuxing delay in microseconds", OFFSET(max_delay), AV_OPT_TYPE_INT, &#123;.i64 = -1 &#125;, -1, INT_MAX, E|D&#125;,&#123;"start_time_realtime", "wall-clock time when stream begins (PTS==0)", OFFSET(start_time_realtime), AV_OPT_TYPE_INT64, &#123;.i64 = AV_NOPTS_VALUE&#125;, INT64_MIN, INT64_MAX, E&#125;,&#123;"fpsprobesize", "number of frames used to probe fps", OFFSET(fps_probe_size), AV_OPT_TYPE_INT, &#123;.i64 = -1&#125;, -1, INT_MAX-1, D&#125;,&#123;"audio_preload", "microseconds by which audio packets should be interleaved earlier", OFFSET(audio_preload), AV_OPT_TYPE_INT, &#123;.i64 = 0&#125;, 0, INT_MAX-1, E&#125;,&#123;"chunk_duration", "microseconds for each chunk", OFFSET(max_chunk_duration), AV_OPT_TYPE_INT, &#123;.i64 = 0&#125;, 0, INT_MAX-1, E&#125;,&#123;"chunk_size", "size in bytes for each chunk", OFFSET(max_chunk_size), AV_OPT_TYPE_INT, &#123;.i64 = 0&#125;, 0, INT_MAX-1, E&#125;,/* this is a crutch for avconv, since it cannot deal with identically named options in different contexts. * to be removed when avconv is fixed */&#123;"f_err_detect", "set error detection flags (deprecated; use err_detect, save via avconv)", OFFSET(error_recognition), AV_OPT_TYPE_FLAGS, &#123;.i64 = AV_EF_CRCCHECK &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"err_detect", "set error detection flags", OFFSET(error_recognition), AV_OPT_TYPE_FLAGS, &#123;.i64 = AV_EF_CRCCHECK &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"crccheck", "verify embedded CRCs", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_CRCCHECK &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"bitstream", "detect bitstream specification deviations", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_BITSTREAM &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"buffer", "detect improper bitstream length", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_BUFFER &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"explode", "abort decoding on minor error detection", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_EXPLODE &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"ignore_err", "ignore errors", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_IGNORE_ERR &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"careful", "consider things that violate the spec, are fast to check and have not been seen in the wild as errors", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_CAREFUL &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"compliant", "consider all spec non compliancies as errors", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_COMPLIANT | AV_EF_CAREFUL &#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"aggressive", "consider things that a sane encoder shouldn't do as an error", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AV_EF_AGGRESSIVE | AV_EF_COMPLIANT | AV_EF_CAREFUL&#125;, INT_MIN, INT_MAX, D, "err_detect"&#125;,&#123;"use_wallclock_as_timestamps", "use wallclock as timestamps", OFFSET(use_wallclock_as_timestamps), AV_OPT_TYPE_BOOL, &#123;.i64 = 0&#125;, 0, 1, D&#125;,&#123;"skip_initial_bytes", "set number of bytes to skip before reading header and frames", OFFSET(skip_initial_bytes), AV_OPT_TYPE_INT64, &#123;.i64 = 0&#125;, 0, INT64_MAX-1, D&#125;,&#123;"correct_ts_overflow", "correct single timestamp overflows", OFFSET(correct_ts_overflow), AV_OPT_TYPE_BOOL, &#123;.i64 = 1&#125;, 0, 1, D&#125;,&#123;"flush_packets", "enable flushing of the I/O context after each packet", OFFSET(flush_packets), AV_OPT_TYPE_INT, &#123;.i64 = -1&#125;, -1, 1, E&#125;,&#123;"metadata_header_padding", "set number of bytes to be written as padding in a metadata header", OFFSET(metadata_header_padding), AV_OPT_TYPE_INT, &#123;.i64 = -1&#125;, -1, INT_MAX, E&#125;,&#123;"output_ts_offset", "set output timestamp offset", OFFSET(output_ts_offset), AV_OPT_TYPE_DURATION, &#123;.i64 = 0&#125;, -INT64_MAX, INT64_MAX, E&#125;,&#123;"max_interleave_delta", "maximum buffering duration for interleaving", OFFSET(max_interleave_delta), AV_OPT_TYPE_INT64, &#123; .i64 = 10000000 &#125;, 0, INT64_MAX, E &#125;,&#123;"f_strict", "how strictly to follow the standards (deprecated; use strict, save via avconv)", OFFSET(strict_std_compliance), AV_OPT_TYPE_INT, &#123;.i64 = DEFAULT &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"strict", "how strictly to follow the standards", OFFSET(strict_std_compliance), AV_OPT_TYPE_INT, &#123;.i64 = DEFAULT &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"very", "strictly conform to a older more strict version of the spec or reference software", 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_VERY_STRICT &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"strict", "strictly conform to all the things in the spec no matter what the consequences", 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_STRICT &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"normal", NULL, 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_NORMAL &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"unofficial", "allow unofficial extensions", 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_UNOFFICIAL &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"experimental", "allow non-standardized experimental variants", 0, AV_OPT_TYPE_CONST, &#123;.i64 = FF_COMPLIANCE_EXPERIMENTAL &#125;, INT_MIN, INT_MAX, D|E, "strict"&#125;,&#123;"max_ts_probe", "maximum number of packets to read while waiting for the first timestamp", OFFSET(max_ts_probe), AV_OPT_TYPE_INT, &#123; .i64 = 50 &#125;, 0, INT_MAX, D &#125;,&#123;"avoid_negative_ts", "shift timestamps so they start at 0", OFFSET(avoid_negative_ts), AV_OPT_TYPE_INT, &#123;.i64 = -1&#125;, -1, 2, E, "avoid_negative_ts"&#125;,&#123;"auto", "enabled when required by target format", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_AVOID_NEG_TS_AUTO &#125;, INT_MIN, INT_MAX, E, "avoid_negative_ts"&#125;,&#123;"disabled", "do not change timestamps", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_AVOID_NEG_TS_DISABLED &#125;, INT_MIN, INT_MAX, E, "avoid_negative_ts"&#125;,&#123;"make_non_negative", "shift timestamps so they are non negative", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_AVOID_NEG_TS_MAKE_NON_NEGATIVE &#125;, INT_MIN, INT_MAX, E, "avoid_negative_ts"&#125;,&#123;"make_zero", "shift timestamps so they start at 0", 0, AV_OPT_TYPE_CONST, &#123;.i64 = AVFMT_AVOID_NEG_TS_MAKE_ZERO &#125;, INT_MIN, INT_MAX, E, "avoid_negative_ts"&#125;,&#123;"dump_separator", "set information dump field separator", OFFSET(dump_separator), AV_OPT_TYPE_STRING, &#123;.str = ", "&#125;, 0, 0, D|E&#125;,&#123;"codec_whitelist", "List of decoders that are allowed to be used", OFFSET(codec_whitelist), AV_OPT_TYPE_STRING, &#123; .str = NULL &#125;, 0, 0, D &#125;,&#123;"format_whitelist", "List of demuxers that are allowed to be used", OFFSET(format_whitelist), AV_OPT_TYPE_STRING, &#123; .str = NULL &#125;, 0, 0, D &#125;,&#123;"protocol_whitelist", "List of protocols that are allowed to be used", OFFSET(protocol_whitelist), AV_OPT_TYPE_STRING, &#123; .str = NULL &#125;, 0, 0, D &#125;,&#123;"protocol_blacklist", "List of protocols that are not allowed to be used", OFFSET(protocol_blacklist), AV_OPT_TYPE_STRING, &#123; .str = NULL &#125;, 0, 0, D &#125;,&#123;"max_streams", "maximum number of streams", OFFSET(max_streams), AV_OPT_TYPE_INT, &#123; .i64 = 1000 &#125;, 0, INT_MAX, D &#125;,&#123;"skip_estimate_duration_from_pts", "skip duration calculation in estimate_timings_from_pts", OFFSET(skip_estimate_duration_from_pts), AV_OPT_TYPE_BOOL, &#123;.i64 = 0&#125;, 0, 1, D&#125;,&#123;"max_probe_packets", "Maximum number of packets to probe a codec", OFFSET(max_probe_packets), AV_OPT_TYPE_INT, &#123; .i64 = 2500 &#125;, 0, INT_MAX, D &#125;,&#123;NULL&#125;,&#125;; 12int avformat_open_input(AVFormatContext **ps, const char *filename, const AVInputFormat *fmt, AVDictionary **options) 以AVFormatContext来分析，av_format_context_class.option 是一个列表，基本对应着 AVFormatContext 内部成员。option 通过offset宏 来找到对应的成员的偏移位置。在avformat_open_input方法中，会遍历外部传入的options参数，对AVFormatContext实例进行属性成员赋值。 类似的这种用法 在ffmpeg中很常见，比如 AVIOContext、URLContext、AVCodecContext … priv_data123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139typedef struct URLContext &#123; const AVClass *av_class; /**&lt; information for av_log(). Set by url_open(). */ const struct URLProtocol *prot; void *priv_data; char *filename; /**&lt; specified URL */ int flags; int max_packet_size; /**&lt; if non zero, the stream is packetized with this max packet size */ int is_streamed; /**&lt; true if streamed (no seek possible), default = false */ int is_connected; AVIOInterruptCB interrupt_callback; int64_t rw_timeout; /**&lt; maximum time to wait for (network) read/write operation completion, in mcs */ const char *protocol_whitelist; const char *protocol_blacklist; int min_packet_size; /**&lt; if non zero, the stream is packetized with this min packet size */&#125; URLContext;const URLProtocol ff_https_protocol = &#123; .name = "https", .url_open2 = http_open, .url_read = http_read, .url_write = http_write, .url_seek = http_seek, .url_close = http_close, .url_get_file_handle = http_get_file_handle, .url_get_short_seek = http_get_short_seek, .url_shutdown = http_shutdown, .priv_data_size = sizeof(HTTPContext), .priv_data_class = &amp;https_context_class, .flags = URL_PROTOCOL_FLAG_NETWORK, .default_whitelist = "http,https,tls,rtp,tcp,udp,crypto,httpproxy"&#125;;#define HTTP_CLASS(flavor) \static const AVClass flavor ## _context_class = &#123; \ .class_name = # flavor, \ .item_name = av_default_item_name, \ .option = options, \ .version = LIBAVUTIL_VERSION_INT, \&#125;#if CONFIG_HTTP_PROTOCOLHTTP_CLASS(http);typedef struct HTTPContext &#123; const AVClass *class; URLContext *hd; unsigned char buffer[BUFFER_SIZE], *buf_ptr, *buf_end; int line_count; int http_code; /* Used if "Transfer-Encoding: chunked" otherwise -1. */ uint64_t chunksize; int chunkend; uint64_t off, end_off, filesize; char *uri; char *location; HTTPAuthState auth_state; HTTPAuthState proxy_auth_state; char *http_proxy; char *headers; char *mime_type; char *http_version; char *user_agent; char *referer; char *content_type; /* Set if the server correctly handles Connection: close and will close * the connection after feeding us the content. */ int willclose; int seekable; /**&lt; Control seekability, 0 = disable, 1 = enable, -1 = probe. */ int chunked_post; /* A flag which indicates if the end of chunked encoding has been sent. */ int end_chunked_post; /* A flag which indicates we have finished to read POST reply. */ int end_header; /* A flag which indicates if we use persistent connections. */ int multiple_requests; uint8_t *post_data; int post_datalen; int is_akamai; int is_mediagateway; char *cookies; ///&lt; holds newline (\n) delimited Set-Cookie header field values (without the "Set-Cookie: " field name) /* A dictionary containing cookies keyed by cookie name */ AVDictionary *cookie_dict; int icy; /* how much data was read since the last ICY metadata packet */ uint64_t icy_data_read; /* after how many bytes of read data a new metadata packet will be found */ uint64_t icy_metaint; char *icy_metadata_headers; char *icy_metadata_packet; AVDictionary *metadata;#if CONFIG_ZLIB int compressed; z_stream inflate_stream; uint8_t *inflate_buffer;#endif /* CONFIG_ZLIB */ AVDictionary *chained_options; /* -1 = try to send if applicable, 0 = always disabled, 1 = always enabled */ int send_expect_100; char *method; int reconnect; int reconnect_at_eof; int reconnect_on_network_error; int reconnect_streamed; int reconnect_delay_max; char *reconnect_on_http_error; int listen; char *resource; int reply_code; int is_multi_client; HandshakeState handshake_step; int is_connected_server; int short_seek_size; int64_t expires; char *new_location; AVDictionary *redirect_cache; uint64_t filesize_from_content_range;&#125; HTTPContext;static int url_alloc_for_protocol(URLContext **puc, const URLProtocol *up, const char *filename, int flags, const AVIOInterruptCB *int_cb) &#123;...if (up-&gt;priv_data_size) &#123; uc-&gt;priv_data = av_mallocz(up-&gt;priv_data_size); if (!uc-&gt;priv_data) &#123; err = AVERROR(ENOMEM); goto fail; &#125; if (up-&gt;priv_data_class) &#123; char *start; *(const AVClass **)uc-&gt;priv_data = up-&gt;priv_data_class; av_opt_set_defaults(uc-&gt;priv_data);...&#125; 以 URLContext 来分析，在 url_alloc_for_protocol 方法中，URLContext.priv_data 赋值的其实就是 对用的 URLProtocol的 priv_data_class实例，以http-protocol来举例，URLContext.priv_data 就是 httpcontext， 而 httpcontext 也符合上面avclass那一套，通过option对 httpcontext实例的成员变量赋值。 这里有个问题？URLContext 持有对应的 prot，为什么还要 弄一个 priv_data呢？ protocol里面有priv_data_class 、priv_data_size，也可以搞一个 priv_data，URLContext用过 prot来访问priv_data就好了。 类似的用法在ffmpeg中 也有很多，比如AVFormatContext，AVFormatContext.priv_data是处理 封装解封装的上下文，mp4格式对应的就是MOVContext. 比如 mp4格式的AVStream,AVStream.priv_data是 MOVStreamContext 123456789101112typedef struct AVFormatContext &#123; ... /** * Format private data. This is an AVOptions-enabled struct * if and only if iformat/oformat.priv_class is not NULL. * * - muxing: set by avformat_write_header() * - demuxing: set by avformat_open_input() */ void *priv_data; ...&#125; internal在 libavformat/internal.h 文件中， FFFormatContext &amp; FFStream 定义如下： 12345678910111213141516171819typedef struct FFFormatContext &#123; /** * The public context. */ AVFormatContext pub; ...&#125;typedef struct FFStream &#123; /** * The public context. */ AVStream pub; ...&#125; AVFormatContext &amp; AVStream 都是作为第一个成员存在于FFFormatContext &amp; FFStream 结构体中。 123456789101112131415161718192021222324252627AVFormatContext *avformat_alloc_context(void)&#123; FFFormatContext *const si = av_mallocz(sizeof(*si)); AVFormatContext *s; if (!si) return NULL; s = &amp;si-&gt;pub; ...&#125;AVStream *avformat_new_stream(AVFormatContext *s, const AVCodec *c)&#123; FFStream *sti; sti = av_mallocz(sizeof(*sti)); AVStream *st; st = &amp;sti-&gt;pub; ...&#125; 初始化逻辑都是通过创建 FFFormatContext &amp; FFStream 实例，然后取成员pub 1234567891011121314static av_always_inline FFFormatContext *ffformatcontext(AVFormatContext *s)&#123; return (FFFormatContext*)s;&#125;static av_always_inline FFStream *ffstream(AVStream *st)&#123; return (FFStream*)st;&#125;static av_always_inline const FFStream *cffstream(const AVStream *st)&#123; return (FFStream*)st;&#125; av_always_inline 是编译器优化，强制内敛 由于 AVFormatContext &amp; AVStream 作为第一个成员，所以二者可以跟FFFormatContext &amp; FFStream 做强制类型转换从定义上看， 好像AVFormatContext &amp; AVStream 是应该作为internal，但是实际用法上，FFFormatContext &amp; FFStream才是作为internal 来辅助 AVFormatContext &amp; AVStream存储相关信息 FFIOContext -&gt; AVIOContext 也是这种形式 ffmepg中也不是 所有的internal 都是这种用法， AVCodecInternal、AVFilterInternal 就是 常规的 作为 AVCodecContext &amp; AVFilterContext 的 internal 成员存在的 个人目前使用的ffmepg 是 5.x的version，早期4.x的时候，avformat &amp; avstream 也是类似 codec &amp; filter 这样常规的用法]]></content>
      <categories>
        <category>ffmepg</category>
      </categories>
      <tags>
        <tag>ffmepg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ffmepg源码之http学习笔记]]></title>
    <url>%2F2022%2F12%2F22%2Fffmepg%E6%BA%90%E7%A0%81%E4%B9%8Bhttp%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[start从 AVFormatContext （解封装结构体） 入口开始吧 … 123456789101112131415161718192021222324252627282930libavformat/options.cAVFormatContext *avformat_alloc_context(void)&#123; FFFormatContext *const si = av_mallocz(sizeof(*si)); AVFormatContext *s; if (!si) return NULL; s = &amp;si-&gt;pub; s-&gt;av_class = &amp;av_format_context_class; /// io入口 s-&gt;io_open = io_open_default; s-&gt;io_close = ff_format_io_close_default; s-&gt;io_close2= io_close2_default; av_opt_set_defaults(s); si-&gt;pkt = av_packet_alloc(); si-&gt;parse_pkt = av_packet_alloc(); if (!si-&gt;pkt || !si-&gt;parse_pkt) &#123; avformat_free_context(s); return NULL; &#125; si-&gt;shortest_end = AV_NOPTS_VALUE; return s;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150libavformat/demux.cint avformat_open_input(AVFormatContext **ps, const char *filename, const AVInputFormat *fmt, AVDictionary **options)&#123; AVFormatContext *s = *ps; FFFormatContext *si; AVDictionary *tmp = NULL; ID3v2ExtraMeta *id3v2_extra_meta = NULL; int ret = 0; if (!s &amp;&amp; !(s = avformat_alloc_context())) return AVERROR(ENOMEM); si = ffformatcontext(s); if (!s-&gt;av_class) &#123; av_log(NULL, AV_LOG_ERROR, "Input context has not been properly allocated by avformat_alloc_context() and is not NULL either\n"); return AVERROR(EINVAL); &#125; if (fmt) s-&gt;iformat = fmt; if (options) av_dict_copy(&amp;tmp, *options, 0); if (s-&gt;pb) // must be before any goto fail s-&gt;flags |= AVFMT_FLAG_CUSTOM_IO; if ((ret = av_opt_set_dict(s, &amp;tmp)) &lt; 0) goto fail; if (!(s-&gt;url = av_strdup(filename ? filename : ""))) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; /// 初始化输入流的信息。这里会初始化AVInputFormat if ((ret = init_input(s, filename, &amp;tmp)) &lt; 0) goto fail; s-&gt;probe_score = ret; if (!s-&gt;protocol_whitelist &amp;&amp; s-&gt;pb &amp;&amp; s-&gt;pb-&gt;protocol_whitelist) &#123; s-&gt;protocol_whitelist = av_strdup(s-&gt;pb-&gt;protocol_whitelist); if (!s-&gt;protocol_whitelist) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; &#125; if (!s-&gt;protocol_blacklist &amp;&amp; s-&gt;pb &amp;&amp; s-&gt;pb-&gt;protocol_blacklist) &#123; s-&gt;protocol_blacklist = av_strdup(s-&gt;pb-&gt;protocol_blacklist); if (!s-&gt;protocol_blacklist) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; &#125; if (s-&gt;format_whitelist &amp;&amp; av_match_list(s-&gt;iformat-&gt;name, s-&gt;format_whitelist, ',') &lt;= 0) &#123; av_log(s, AV_LOG_ERROR, "Format not on whitelist \'%s\'\n", s-&gt;format_whitelist); ret = AVERROR(EINVAL); goto fail; &#125; avio_skip(s-&gt;pb, s-&gt;skip_initial_bytes); /* Check filename in case an image number is expected. */ if (s-&gt;iformat-&gt;flags &amp; AVFMT_NEEDNUMBER) &#123; if (!av_filename_number_test(filename)) &#123; ret = AVERROR(EINVAL); goto fail; &#125; &#125; s-&gt;duration = s-&gt;start_time = AV_NOPTS_VALUE; /* Allocate private data. */ if (s-&gt;iformat-&gt;priv_data_size &gt; 0) &#123; if (!(s-&gt;priv_data = av_mallocz(s-&gt;iformat-&gt;priv_data_size))) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; if (s-&gt;iformat-&gt;priv_class) &#123; *(const AVClass **) s-&gt;priv_data = s-&gt;iformat-&gt;priv_class; av_opt_set_defaults(s-&gt;priv_data); if ((ret = av_opt_set_dict(s-&gt;priv_data, &amp;tmp)) &lt; 0) goto fail; &#125; &#125; /* e.g. AVFMT_NOFILE formats will not have an AVIOContext */ if (s-&gt;pb) ff_id3v2_read_dict(s-&gt;pb, &amp;si-&gt;id3v2_meta, ID3v2_DEFAULT_MAGIC, &amp;id3v2_extra_meta); if (s-&gt;iformat-&gt;read_header) if ((ret = s-&gt;iformat-&gt;read_header(s)) &lt; 0) &#123; if (s-&gt;iformat-&gt;flags_internal &amp; FF_FMT_INIT_CLEANUP) goto close; goto fail; &#125; if (!s-&gt;metadata) &#123; s-&gt;metadata = si-&gt;id3v2_meta; si-&gt;id3v2_meta = NULL; &#125; else if (si-&gt;id3v2_meta) &#123; av_log(s, AV_LOG_WARNING, "Discarding ID3 tags because more suitable tags were found.\n"); av_dict_free(&amp;si-&gt;id3v2_meta); &#125; if (id3v2_extra_meta) &#123; if (!strcmp(s-&gt;iformat-&gt;name, "mp3") || !strcmp(s-&gt;iformat-&gt;name, "aac") || !strcmp(s-&gt;iformat-&gt;name, "tta") || !strcmp(s-&gt;iformat-&gt;name, "wav")) &#123; if ((ret = ff_id3v2_parse_apic(s, id3v2_extra_meta)) &lt; 0) goto close; if ((ret = ff_id3v2_parse_chapters(s, id3v2_extra_meta)) &lt; 0) goto close; if ((ret = ff_id3v2_parse_priv(s, id3v2_extra_meta)) &lt; 0) goto close; &#125; else av_log(s, AV_LOG_DEBUG, "demuxer does not support additional id3 data, skipping\n"); ff_id3v2_free_extra_meta(&amp;id3v2_extra_meta); &#125; if ((ret = avformat_queue_attached_pictures(s)) &lt; 0) goto close; if (s-&gt;pb &amp;&amp; !si-&gt;data_offset) si-&gt;data_offset = avio_tell(s-&gt;pb); si-&gt;raw_packet_buffer_size = 0; update_stream_avctx(s); if (options) &#123; av_dict_free(options); *options = tmp; &#125; *ps = s; return 0;close: if (s-&gt;iformat-&gt;read_close) s-&gt;iformat-&gt;read_close(s);fail: ff_id3v2_free_extra_meta(&amp;id3v2_extra_meta); av_dict_free(&amp;tmp); if (s-&gt;pb &amp;&amp; !(s-&gt;flags &amp; AVFMT_FLAG_CUSTOM_IO)) avio_closep(&amp;s-&gt;pb); avformat_free_context(s); *ps = NULL; return ret;&#125; 1234567891011121314151617181920212223242526272829303132333435363738libavformat/demux.cstatic int init_input(AVFormatContext *s, const char *filename, AVDictionary **options)&#123; int ret; AVProbeData pd = &#123; filename, NULL, 0 &#125;; int score = AVPROBE_SCORE_RETRY; ///初始化的过程 s-&gt;pb 还没创建好 if (s-&gt;pb) &#123; s-&gt;flags |= AVFMT_FLAG_CUSTOM_IO; if (!s-&gt;iformat) return av_probe_input_buffer2(s-&gt;pb, &amp;s-&gt;iformat, filename, s, 0, s-&gt;format_probesize); else if (s-&gt;iformat-&gt;flags &amp; AVFMT_NOFILE) av_log(s, AV_LOG_WARNING, "Custom AVIOContext makes no sense and " "will be ignored with AVFMT_NOFILE format.\n"); return 0; &#125; /* Guess file format. */ // av_probe_input_format2 首次判断inputformat格式 iformat，这一步通常判断不出来 if ((s-&gt;iformat &amp;&amp; s-&gt;iformat-&gt;flags &amp; AVFMT_NOFILE) || (!s-&gt;iformat &amp;&amp; (s-&gt;iformat = av_probe_input_format2(&amp;pd, 0, &amp;score)))) return score; /// io_open 就是前面赋值的入口函数 io_open_default if ((ret = s-&gt;io_open(s, &amp;s-&gt;pb, filename, AVIO_FLAG_READ | s-&gt;avio_flags, options)) &lt; 0) return ret; if (s-&gt;iformat) return 0; // av_probe_input_buffer2再次判断inputformat格式 iformat，通过buf媒体流数据判断。 return av_probe_input_buffer2(s-&gt;pb, &amp;s-&gt;iformat, filename, s, 0, s-&gt;format_probesize);&#125; 1234567891011121314151617181920libavformat/options.cstatic int io_open_default(AVFormatContext *s, AVIOContext **pb, const char *url, int flags, AVDictionary **options)&#123; int loglevel; if (!strcmp(url, s-&gt;url) || s-&gt;iformat &amp;&amp; !strcmp(s-&gt;iformat-&gt;name, "image2") || s-&gt;oformat &amp;&amp; !strcmp(s-&gt;oformat-&gt;name, "image2") ) &#123; loglevel = AV_LOG_DEBUG; &#125; else loglevel = AV_LOG_INFO; av_log(s, loglevel, "Opening \'%s\' for %s\n", url, flags &amp; AVIO_FLAG_WRITE ? "writing" : "reading"); /// io-open return ffio_open_whitelist(pb, url, flags, &amp;s-&gt;interrupt_callback, options, s-&gt;protocol_whitelist, s-&gt;protocol_blacklist);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188libavformat/aviobuf.cint ffio_open_whitelist(AVIOContext **s, const char *filename, int flags, const AVIOInterruptCB *int_cb, AVDictionary **options, const char *whitelist, const char *blacklist )&#123; URLContext *h; int err; *s = NULL; /// 创建 URLContext err = ffurl_open_whitelist(&amp;h, filename, flags, int_cb, options, whitelist, blacklist, NULL); if (err &lt; 0) return err; /// 创建AVIOContext 赋值 urlcontext err = ffio_fdopen(s, h); if (err &lt; 0) &#123; ffurl_close(h); return err; &#125; return 0;&#125;```clibavformat/avio.cint ffurl_alloc(URLContext **puc, const char *filename, int flags, const AVIOInterruptCB *int_cb)&#123; const URLProtocol *p = NULL; /// 根据filename 找到 http p = url_find_protocol(filename); if (p) /// 创建URLContext 并赋值 URLProtocol return url_alloc_for_protocol(puc, p, filename, flags, int_cb); *puc = NULL; return AVERROR_PROTOCOL_NOT_FOUND;&#125;static int url_alloc_for_protocol(URLContext **puc, const URLProtocol *up, const char *filename, int flags, const AVIOInterruptCB *int_cb)&#123; URLContext *uc; int err;#if CONFIG_NETWORK if (up-&gt;flags &amp; URL_PROTOCOL_FLAG_NETWORK &amp;&amp; !ff_network_init()) return AVERROR(EIO);#endif if ((flags &amp; AVIO_FLAG_READ) &amp;&amp; !up-&gt;url_read) &#123; av_log(NULL, AV_LOG_ERROR, "Impossible to open the '%s' protocol for reading\n", up-&gt;name); return AVERROR(EIO); &#125; if ((flags &amp; AVIO_FLAG_WRITE) &amp;&amp; !up-&gt;url_write) &#123; av_log(NULL, AV_LOG_ERROR, "Impossible to open the '%s' protocol for writing\n", up-&gt;name); return AVERROR(EIO); &#125; /// 创建 URLContext uc = av_mallocz(sizeof(URLContext) + strlen(filename) + 1); if (!uc) &#123; err = AVERROR(ENOMEM); goto fail; &#125; uc-&gt;av_class = &amp;ffurl_context_class; uc-&gt;filename = (char *)&amp;uc[1]; strcpy(uc-&gt;filename, filename); /// 赋值 urlprotocal uc-&gt;prot = up; uc-&gt;flags = flags; uc-&gt;is_streamed = 0; /* default = not streamed */ uc-&gt;max_packet_size = 0; /* default: stream file */ /// httpprotocal 设置过了 priv_data 为 HTTPContext if (up-&gt;priv_data_size) &#123; uc-&gt;priv_data = av_mallocz(up-&gt;priv_data_size); if (!uc-&gt;priv_data) &#123; err = AVERROR(ENOMEM); goto fail; &#125; if (up-&gt;priv_data_class) &#123; char *start; *(const AVClass **)uc-&gt;priv_data = up-&gt;priv_data_class; av_opt_set_defaults(uc-&gt;priv_data); if (av_strstart(uc-&gt;filename, up-&gt;name, (const char**)&amp;start) &amp;&amp; *start == ',') &#123; int ret= 0; char *p= start; char sep= *++p; char *key, *val; p++; if (strcmp(up-&gt;name, "subfile")) ret = AVERROR(EINVAL); while(ret &gt;= 0 &amp;&amp; (key= strchr(p, sep)) &amp;&amp; p&lt;key &amp;&amp; (val = strchr(key+1, sep)))&#123; *val= *key= 0; if (strcmp(p, "start") &amp;&amp; strcmp(p, "end")) &#123; ret = AVERROR_OPTION_NOT_FOUND; &#125; else ret= av_opt_set(uc-&gt;priv_data, p, key+1, 0); if (ret == AVERROR_OPTION_NOT_FOUND) av_log(uc, AV_LOG_ERROR, "Key '%s' not found.\n", p); *val= *key= sep; p= val+1; &#125; if(ret&lt;0 || p!=key)&#123; av_log(uc, AV_LOG_ERROR, "Error parsing options string %s\n", start); av_freep(&amp;uc-&gt;priv_data); av_freep(&amp;uc); err = AVERROR(EINVAL); goto fail; &#125; memmove(start, key+1, strlen(key)); &#125; &#125; &#125; if (int_cb) uc-&gt;interrupt_callback = *int_cb; *puc = uc; return 0;fail: *puc = NULL; if (uc) av_freep(&amp;uc-&gt;priv_data); av_freep(&amp;uc);#if CONFIG_NETWORK if (up-&gt;flags &amp; URL_PROTOCOL_FLAG_NETWORK) ff_network_close();#endif return err;&#125;int ffurl_open_whitelist(URLContext **puc, const char *filename, int flags, const AVIOInterruptCB *int_cb, AVDictionary **options, const char *whitelist, const char* blacklist, URLContext *parent)&#123; AVDictionary *tmp_opts = NULL; AVDictionaryEntry *e; /// 创建 URLContext int ret = ffurl_alloc(puc, filename, flags, int_cb); if (ret &lt; 0) return ret; if (parent) &#123; ret = av_opt_copy(*puc, parent); if (ret &lt; 0) goto fail; &#125; if (options &amp;&amp; (ret = av_opt_set_dict(*puc, options)) &lt; 0) goto fail; if (options &amp;&amp; (*puc)-&gt;prot-&gt;priv_data_class &amp;&amp; (ret = av_opt_set_dict((*puc)-&gt;priv_data, options)) &lt; 0) goto fail; if (!options) options = &amp;tmp_opts; av_assert0(!whitelist || !(e=av_dict_get(*options, "protocol_whitelist", NULL, 0)) || !strcmp(whitelist, e-&gt;value)); av_assert0(!blacklist || !(e=av_dict_get(*options, "protocol_blacklist", NULL, 0)) || !strcmp(blacklist, e-&gt;value)); if ((ret = av_dict_set(options, "protocol_whitelist", whitelist, 0)) &lt; 0) goto fail; if ((ret = av_dict_set(options, "protocol_blacklist", blacklist, 0)) &lt; 0) goto fail; if ((ret = av_opt_set_dict(*puc, options)) &lt; 0) goto fail; /// 发起链接 ret = ffurl_connect(*puc, options); if (!ret) return 0;fail: ffurl_closep(puc); return ret;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162libavformat/avio.cint ffurl_connect(URLContext *uc, AVDictionary **options)&#123; int err; AVDictionary *tmp_opts = NULL; AVDictionaryEntry *e; if (!options) options = &amp;tmp_opts; // Check that URLContext was initialized correctly and lists are matching if set av_assert0(!(e=av_dict_get(*options, "protocol_whitelist", NULL, 0)) || (uc-&gt;protocol_whitelist &amp;&amp; !strcmp(uc-&gt;protocol_whitelist, e-&gt;value))); av_assert0(!(e=av_dict_get(*options, "protocol_blacklist", NULL, 0)) || (uc-&gt;protocol_blacklist &amp;&amp; !strcmp(uc-&gt;protocol_blacklist, e-&gt;value))); if (uc-&gt;protocol_whitelist &amp;&amp; av_match_list(uc-&gt;prot-&gt;name, uc-&gt;protocol_whitelist, ',') &lt;= 0) &#123; av_log(uc, AV_LOG_ERROR, "Protocol '%s' not on whitelist '%s'!\n", uc-&gt;prot-&gt;name, uc-&gt;protocol_whitelist); return AVERROR(EINVAL); &#125; if (uc-&gt;protocol_blacklist &amp;&amp; av_match_list(uc-&gt;prot-&gt;name, uc-&gt;protocol_blacklist, ',') &gt; 0) &#123; av_log(uc, AV_LOG_ERROR, "Protocol '%s' on blacklist '%s'!\n", uc-&gt;prot-&gt;name, uc-&gt;protocol_blacklist); return AVERROR(EINVAL); &#125; if (!uc-&gt;protocol_whitelist &amp;&amp; uc-&gt;prot-&gt;default_whitelist) &#123; av_log(uc, AV_LOG_DEBUG, "Setting default whitelist '%s'\n", uc-&gt;prot-&gt;default_whitelist); uc-&gt;protocol_whitelist = av_strdup(uc-&gt;prot-&gt;default_whitelist); if (!uc-&gt;protocol_whitelist) &#123; return AVERROR(ENOMEM); &#125; &#125; else if (!uc-&gt;protocol_whitelist) av_log(uc, AV_LOG_DEBUG, "No default whitelist set\n"); // This should be an error once all declare a default whitelist if ((err = av_dict_set(options, "protocol_whitelist", uc-&gt;protocol_whitelist, 0)) &lt; 0) return err; if ((err = av_dict_set(options, "protocol_blacklist", uc-&gt;protocol_blacklist, 0)) &lt; 0) return err; err = /// uc.prot 是 http url_open2 对应的是 http_open uc-&gt;prot-&gt;url_open2 ? uc-&gt;prot-&gt;url_open2(uc, uc-&gt;filename, uc-&gt;flags, options) : uc-&gt;prot-&gt;url_open(uc, uc-&gt;filename, uc-&gt;flags); av_dict_set(options, "protocol_whitelist", NULL, 0); av_dict_set(options, "protocol_blacklist", NULL, 0); if (err) return err; uc-&gt;is_connected = 1; /* We must be careful here as ffurl_seek() could be slow, * for example for http */ if ((uc-&gt;flags &amp; AVIO_FLAG_WRITE) || !strcmp(uc-&gt;prot-&gt;name, "file")) if (!uc-&gt;is_streamed &amp;&amp; ffurl_seek(uc, 0, SEEK_SET) &lt; 0) uc-&gt;is_streamed = 1; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859libavformat/aviobuf.cint ffio_fdopen(AVIOContext **s, URLContext *h)&#123; uint8_t *buffer = NULL; int buffer_size, max_packet_size; max_packet_size = h-&gt;max_packet_size; if (max_packet_size) &#123; buffer_size = max_packet_size; /* no need to bufferize more than one packet */ &#125; else &#123; buffer_size = IO_BUFFER_SIZE; &#125; if (!(h-&gt;flags &amp; AVIO_FLAG_WRITE) &amp;&amp; h-&gt;is_streamed) &#123; if (buffer_size &gt; INT_MAX/2) return AVERROR(EINVAL); buffer_size *= 2; &#125; buffer = av_malloc(buffer_size); if (!buffer) return AVERROR(ENOMEM); /// 创建 AVIOContext /// s-&gt;opaque = h; *s = avio_alloc_context(buffer, buffer_size, h-&gt;flags &amp; AVIO_FLAG_WRITE, h, (int (*)(void *, uint8_t *, int)) ffurl_read, (int (*)(void *, uint8_t *, int)) ffurl_write, (int64_t (*)(void *, int64_t, int))ffurl_seek); if (!*s) &#123; av_freep(&amp;buffer); return AVERROR(ENOMEM); &#125; (*s)-&gt;protocol_whitelist = av_strdup(h-&gt;protocol_whitelist); if (!(*s)-&gt;protocol_whitelist &amp;&amp; h-&gt;protocol_whitelist) &#123; avio_closep(s); return AVERROR(ENOMEM); &#125; (*s)-&gt;protocol_blacklist = av_strdup(h-&gt;protocol_blacklist); if (!(*s)-&gt;protocol_blacklist &amp;&amp; h-&gt;protocol_blacklist) &#123; avio_closep(s); return AVERROR(ENOMEM); &#125; (*s)-&gt;direct = h-&gt;flags &amp; AVIO_FLAG_DIRECT; (*s)-&gt;seekable = h-&gt;is_streamed ? 0 : AVIO_SEEKABLE_NORMAL; (*s)-&gt;max_packet_size = max_packet_size; (*s)-&gt;min_packet_size = h-&gt;min_packet_size; if(h-&gt;prot) &#123; (*s)-&gt;read_pause = (int (*)(void *, int))h-&gt;prot-&gt;url_read_pause; (*s)-&gt;read_seek = (int64_t (*)(void *, int, int64_t, int))h-&gt;prot-&gt;url_read_seek; if (h-&gt;prot-&gt;url_read_seek) (*s)-&gt;seekable |= AVIO_SEEKABLE_TIME; &#125; ((FFIOContext*)(*s))-&gt;short_seek_get = (int (*)(void *))ffurl_get_short_seek; (*s)-&gt;av_class = &amp;ff_avio_class; return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427libavformat/http.c/// 这个就是对应前面的 protocolconst URLProtocol ff_http_protocol = &#123; .name = "http", /// * .url_open2 = http_open, .url_accept = http_accept, .url_handshake = http_handshake, .url_read = http_read, .url_write = http_write, .url_seek = http_seek, .url_close = http_close, .url_get_file_handle = http_get_file_handle, .url_get_short_seek = http_get_short_seek, .url_shutdown = http_shutdown, /// * .priv_data_size = sizeof(HTTPContext), .priv_data_class = &amp;http_context_class, .flags = URL_PROTOCOL_FLAG_NETWORK, .default_whitelist = "http,https,tls,rtp,tcp,udp,crypto,httpproxy,data"&#125;;static int http_open(URLContext *h, const char *uri, int flags, AVDictionary **options)&#123; HTTPContext *s = h-&gt;priv_data; int ret; if( s-&gt;seekable == 1 ) h-&gt;is_streamed = 0; else h-&gt;is_streamed = 1; s-&gt;filesize = UINT64_MAX; s-&gt;location = av_strdup(uri); if (!s-&gt;location) return AVERROR(ENOMEM); s-&gt;uri = av_strdup(uri); if (!s-&gt;uri) return AVERROR(ENOMEM); if (options) av_dict_copy(&amp;s-&gt;chained_options, *options, 0); if (s-&gt;headers) &#123; int len = strlen(s-&gt;headers); if (len &lt; 2 || strcmp("\r\n", s-&gt;headers + len - 2)) &#123; av_log(h, AV_LOG_WARNING, "No trailing CRLF found in HTTP header. Adding it.\n"); ret = av_reallocp(&amp;s-&gt;headers, len + 3); if (ret &lt; 0) goto bail_out; s-&gt;headers[len] = '\r'; s-&gt;headers[len + 1] = '\n'; s-&gt;headers[len + 2] = '\0'; &#125; &#125; if (s-&gt;listen) &#123; return http_listen(h, uri, flags, options); &#125; /// 进这里 ret = http_open_cnx(h, options);bail_out: if (ret &lt; 0) &#123; av_dict_free(&amp;s-&gt;chained_options); av_dict_free(&amp;s-&gt;cookie_dict); av_dict_free(&amp;s-&gt;redirect_cache); av_freep(&amp;s-&gt;new_location); av_freep(&amp;s-&gt;uri); &#125; return ret;&#125;static int http_open_cnx(URLContext *h, AVDictionary **options)&#123; HTTPAuthType cur_auth_type, cur_proxy_auth_type; HTTPContext *s = h-&gt;priv_data; int ret, attempts = 0, redirects = 0; int reconnect_delay = 0; uint64_t off; char *cached;redo: cached = redirect_cache_get(s); if (cached) &#123; av_free(s-&gt;location); s-&gt;location = av_strdup(cached); if (!s-&gt;location) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; goto redo; &#125; av_dict_copy(options, s-&gt;chained_options, 0); cur_auth_type = s-&gt;auth_state.auth_type; cur_proxy_auth_type = s-&gt;auth_state.auth_type; off = s-&gt;off; /// 进这里 关键方法 ret = http_open_cnx_internal(h, options); if (ret &lt; 0) &#123; if (!http_should_reconnect(s, ret) || reconnect_delay &gt; s-&gt;reconnect_delay_max) goto fail; av_log(h, AV_LOG_WARNING, "Will reconnect at %"PRIu64" in %d second(s).\n", off, reconnect_delay); ret = ff_network_sleep_interruptible(1000U * 1000 * reconnect_delay, &amp;h-&gt;interrupt_callback); if (ret != AVERROR(ETIMEDOUT)) goto fail; reconnect_delay = 1 + 2 * reconnect_delay; /* restore the offset (http_connect resets it) */ s-&gt;off = off; ffurl_closep(&amp;s-&gt;hd); goto redo; &#125; attempts++; if (s-&gt;http_code == 401) &#123; if ((cur_auth_type == HTTP_AUTH_NONE || s-&gt;auth_state.stale) &amp;&amp; s-&gt;auth_state.auth_type != HTTP_AUTH_NONE &amp;&amp; attempts &lt; 4) &#123; ffurl_closep(&amp;s-&gt;hd); goto redo; &#125; else goto fail; &#125; if (s-&gt;http_code == 407) &#123; if ((cur_proxy_auth_type == HTTP_AUTH_NONE || s-&gt;proxy_auth_state.stale) &amp;&amp; s-&gt;proxy_auth_state.auth_type != HTTP_AUTH_NONE &amp;&amp; attempts &lt; 4) &#123; ffurl_closep(&amp;s-&gt;hd); goto redo; &#125; else goto fail; &#125; if ((s-&gt;http_code == 301 || s-&gt;http_code == 302 || s-&gt;http_code == 303 || s-&gt;http_code == 307 || s-&gt;http_code == 308) &amp;&amp; s-&gt;new_location) &#123; /* url moved, get next */ ffurl_closep(&amp;s-&gt;hd); if (redirects++ &gt;= MAX_REDIRECTS) return AVERROR(EIO); if (!s-&gt;expires) &#123; s-&gt;expires = (s-&gt;http_code == 301 || s-&gt;http_code == 308) ? INT64_MAX : -1; &#125; if (s-&gt;expires &gt; time(NULL) &amp;&amp; av_dict_count(s-&gt;redirect_cache) &lt; MAX_CACHED_REDIRECTS) &#123; redirect_cache_set(s, s-&gt;location, s-&gt;new_location, s-&gt;expires); &#125; av_free(s-&gt;location); s-&gt;location = s-&gt;new_location; s-&gt;new_location = NULL; /* Restart the authentication process with the new target, which * might use a different auth mechanism. */ memset(&amp;s-&gt;auth_state, 0, sizeof(s-&gt;auth_state)); attempts = 0; goto redo; &#125; return 0;fail: if (s-&gt;hd) ffurl_closep(&amp;s-&gt;hd); if (ret &lt; 0) return ret; return ff_http_averror(s-&gt;http_code, AVERROR(EIO));&#125;static int http_open_cnx_internal(URLContext *h, AVDictionary **options)&#123; /// lower_proto 关键 const char *path, *proxy_path, *lower_proto = "tcp", *local_path; char *env_http_proxy, *env_no_proxy; char *hashmark; char hostname[1024], hoststr[1024], proto[10]; char auth[1024], proxyauth[1024] = ""; char path1[MAX_URL_SIZE], sanitized_path[MAX_URL_SIZE + 1]; char buf[1024], urlbuf[MAX_URL_SIZE]; int port, use_proxy, err = 0; /// 前面定义httpprotocal的时候设置过了 HTTPContext *s = h-&gt;priv_data; av_url_split(proto, sizeof(proto), auth, sizeof(auth), hostname, sizeof(hostname), &amp;port, path1, sizeof(path1), s-&gt;location); ff_url_join(hoststr, sizeof(hoststr), NULL, NULL, hostname, port, NULL); env_http_proxy = getenv_utf8("http_proxy"); proxy_path = s-&gt;http_proxy ? s-&gt;http_proxy : env_http_proxy; env_no_proxy = getenv_utf8("no_proxy"); use_proxy = !ff_http_match_no_proxy(env_no_proxy, hostname) &amp;&amp; proxy_path &amp;&amp; av_strstart(proxy_path, "http://", NULL); freeenv_utf8(env_no_proxy); /// 如果是https 会走到 tls (tls_securetransport.c &amp; tls.c) /// tls 内部 最后也会走到tcp的 先略过 先直接看tcp if (!strcmp(proto, "https")) &#123; lower_proto = "tls"; use_proxy = 0; if (port &lt; 0) port = 443; /* pass http_proxy to underlying protocol */ if (s-&gt;http_proxy) &#123; err = av_dict_set(options, "http_proxy", s-&gt;http_proxy, 0); if (err &lt; 0) goto end; &#125; &#125; if (port &lt; 0) port = 80; hashmark = strchr(path1, '#'); if (hashmark) *hashmark = '\0'; if (path1[0] == '\0') &#123; path = "/"; &#125; else if (path1[0] == '?') &#123; snprintf(sanitized_path, sizeof(sanitized_path), "/%s", path1); path = sanitized_path; &#125; else &#123; path = path1; &#125; local_path = path; if (use_proxy) &#123; /* Reassemble the request URL without auth string - we don't * want to leak the auth to the proxy. */ ff_url_join(urlbuf, sizeof(urlbuf), proto, NULL, hostname, port, "%s", path1); path = urlbuf; av_url_split(NULL, 0, proxyauth, sizeof(proxyauth), hostname, sizeof(hostname), &amp;port, NULL, 0, proxy_path); &#125; /// 这里先以tcp来分析 ff_url_join(buf, sizeof(buf), lower_proto, NULL, hostname, port, NULL); if (!s-&gt;hd) &#123; /// s-&gt;hd 也是 URLContext，发现这里又进入了ffurl_open_whitelist 循环了 /// buf 是 tcp了 不是http 了， 否则就跳不出来了 /// 所以 s-&gt;hd 的 protocal 是 TCPProtocol err = ffurl_open_whitelist(&amp;s-&gt;hd, buf, AVIO_FLAG_READ_WRITE, &amp;h-&gt;interrupt_callback, options, h-&gt;protocol_whitelist, h-&gt;protocol_blacklist, h); &#125;end: freeenv_utf8(env_http_proxy); return err &lt; 0 ? err : http_connect( h, path, local_path, hoststr, auth, proxyauth);&#125;static int http_connect(URLContext *h, const char *path, const char *local_path, const char *hoststr, const char *auth, const char *proxyauth)&#123; HTTPContext *s = h-&gt;priv_data; int post, err; AVBPrint request; char *authstr = NULL, *proxyauthstr = NULL; uint64_t off = s-&gt;off; const char *method; int send_expect_100 = 0; av_bprint_init_for_buffer(&amp;request, s-&gt;buffer, sizeof(s-&gt;buffer)); /* send http header */ post = h-&gt;flags &amp; AVIO_FLAG_WRITE; if (s-&gt;post_data) &#123; /* force POST method and disable chunked encoding when * custom HTTP post data is set */ post = 1; s-&gt;chunked_post = 0; &#125; if (s-&gt;method) method = s-&gt;method; else method = post ? "POST" : "GET"; authstr = ff_http_auth_create_response(&amp;s-&gt;auth_state, auth, local_path, method); proxyauthstr = ff_http_auth_create_response(&amp;s-&gt;proxy_auth_state, proxyauth, local_path, method); if (post &amp;&amp; !s-&gt;post_data) &#123; if (s-&gt;send_expect_100 != -1) &#123; send_expect_100 = s-&gt;send_expect_100; &#125; else &#123; send_expect_100 = 0; /* The user has supplied authentication but we don't know the auth type, * send Expect: 100-continue to get the 401 response including the * WWW-Authenticate header, or an 100 continue if no auth actually * is needed. */ if (auth &amp;&amp; *auth &amp;&amp; s-&gt;auth_state.auth_type == HTTP_AUTH_NONE &amp;&amp; s-&gt;http_code != 401) send_expect_100 = 1; &#125; &#125; av_bprintf(&amp;request, "%s ", method); bprint_escaped_path(&amp;request, path); av_bprintf(&amp;request, " HTTP/1.1\r\n"); if (post &amp;&amp; s-&gt;chunked_post) av_bprintf(&amp;request, "Transfer-Encoding: chunked\r\n"); /* set default headers if needed */ if (!has_header(s-&gt;headers, "\r\nUser-Agent: ")) av_bprintf(&amp;request, "User-Agent: %s\r\n", s-&gt;user_agent); if (s-&gt;referer) &#123; /* set default headers if needed */ if (!has_header(s-&gt;headers, "\r\nReferer: ")) av_bprintf(&amp;request, "Referer: %s\r\n", s-&gt;referer); &#125; if (!has_header(s-&gt;headers, "\r\nAccept: ")) av_bprintf(&amp;request, "Accept: */*\r\n"); // Note: we send the Range header on purpose, even when we're probing, // since it allows us to detect more reliably if a (non-conforming) // server supports seeking by analysing the reply headers. if (!has_header(s-&gt;headers, "\r\nRange: ") &amp;&amp; !post &amp;&amp; (s-&gt;off &gt; 0 || s-&gt;end_off || s-&gt;seekable != 0)) &#123; av_bprintf(&amp;request, "Range: bytes=%"PRIu64"-", s-&gt;off); if (s-&gt;end_off) av_bprintf(&amp;request, "%"PRId64, s-&gt;end_off - 1); av_bprintf(&amp;request, "\r\n"); &#125; if (send_expect_100 &amp;&amp; !has_header(s-&gt;headers, "\r\nExpect: ")) av_bprintf(&amp;request, "Expect: 100-continue\r\n"); if (!has_header(s-&gt;headers, "\r\nConnection: ")) av_bprintf(&amp;request, "Connection: %s\r\n", s-&gt;multiple_requests ? "keep-alive" : "close"); if (!has_header(s-&gt;headers, "\r\nHost: ")) av_bprintf(&amp;request, "Host: %s\r\n", hoststr); if (!has_header(s-&gt;headers, "\r\nContent-Length: ") &amp;&amp; s-&gt;post_data) av_bprintf(&amp;request, "Content-Length: %d\r\n", s-&gt;post_datalen); if (!has_header(s-&gt;headers, "\r\nContent-Type: ") &amp;&amp; s-&gt;content_type) av_bprintf(&amp;request, "Content-Type: %s\r\n", s-&gt;content_type); if (!has_header(s-&gt;headers, "\r\nCookie: ") &amp;&amp; s-&gt;cookies) &#123; char *cookies = NULL; if (!get_cookies(s, &amp;cookies, path, hoststr) &amp;&amp; cookies) &#123; av_bprintf(&amp;request, "Cookie: %s\r\n", cookies); av_free(cookies); &#125; &#125; if (!has_header(s-&gt;headers, "\r\nIcy-MetaData: ") &amp;&amp; s-&gt;icy) av_bprintf(&amp;request, "Icy-MetaData: 1\r\n"); /* now add in custom headers */ if (s-&gt;headers) av_bprintf(&amp;request, "%s", s-&gt;headers); if (authstr) av_bprintf(&amp;request, "%s", authstr); if (proxyauthstr) av_bprintf(&amp;request, "Proxy-%s", proxyauthstr); av_bprintf(&amp;request, "\r\n"); av_log(h, AV_LOG_DEBUG, "request: %s\n", request.str); if (!av_bprint_is_complete(&amp;request)) &#123; av_log(h, AV_LOG_ERROR, "overlong headers\n"); err = AVERROR(EINVAL); goto done; &#125; /// 写入数据 发起请求 if ((err = ffurl_write(s-&gt;hd, request.str, request.len)) &lt; 0) goto done; if (s-&gt;post_data) if ((err = ffurl_write(s-&gt;hd, s-&gt;post_data, s-&gt;post_datalen)) &lt; 0) goto done; /* init input buffer */ s-&gt;buf_ptr = s-&gt;buffer; s-&gt;buf_end = s-&gt;buffer; s-&gt;line_count = 0; s-&gt;off = 0; s-&gt;icy_data_read = 0; s-&gt;filesize = UINT64_MAX; s-&gt;willclose = 0; s-&gt;end_chunked_post = 0; s-&gt;end_header = 0;#if CONFIG_ZLIB s-&gt;compressed = 0;#endif if (post &amp;&amp; !s-&gt;post_data &amp;&amp; !send_expect_100) &#123; /* Pretend that it did work. We didn't read any header yet, since * we've still to send the POST data, but the code calling this * function will check http_code after we return. */ s-&gt;http_code = 200; err = 0; goto done; &#125; /* wait for header */ err = http_read_header(h); if (err &lt; 0) goto done; if (s-&gt;new_location) s-&gt;off = off; err = (off == s-&gt;off) ? 0 : -1;done: av_freep(&amp;authstr); av_freep(&amp;proxyauthstr); return err;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162libavformat/avio.cint ffurl_write(URLContext *h, const unsigned char *buf, int size)&#123; if (!(h-&gt;flags &amp; AVIO_FLAG_WRITE)) return AVERROR(EIO); /* avoid sending too big packets */ if (h-&gt;max_packet_size &amp;&amp; size &gt; h-&gt;max_packet_size) return AVERROR(EIO); /// h-&gt;prot 通过前面的分析 就是TCPProtocol return retry_transfer_wrapper(h, (unsigned char *)buf, size, size, (int (*)(struct URLContext *, uint8_t *, int)) h-&gt;prot-&gt;url_write);&#125;static inline int retry_transfer_wrapper(URLContext *h, uint8_t *buf, int size, int size_min, int (*transfer_func)(URLContext *h, uint8_t *buf, int size))&#123; int ret, len; int fast_retries = 5; int64_t wait_since = 0; len = 0; while (len &lt; size_min) &#123; if (ff_check_interrupt(&amp;h-&gt;interrupt_callback)) return AVERROR_EXIT; /// 回调 ret = transfer_func(h, buf + len, size - len); if (ret == AVERROR(EINTR)) continue; if (h-&gt;flags &amp; AVIO_FLAG_NONBLOCK) return ret; if (ret == AVERROR(EAGAIN)) &#123; ret = 0; if (fast_retries) &#123; fast_retries--; &#125; else &#123; if (h-&gt;rw_timeout) &#123; if (!wait_since) wait_since = av_gettime_relative(); else if (av_gettime_relative() &gt; wait_since + h-&gt;rw_timeout) return AVERROR(EIO); &#125; av_usleep(1000); &#125; &#125; else if (ret == AVERROR_EOF) return (len &gt; 0) ? len : AVERROR_EOF; else if (ret &lt; 0) return ret; if (ret) &#123; fast_retries = FFMAX(fast_retries, 2); wait_since = 0; &#125; len += ret; &#125; return len;&#125; 12345678910111213141516171819202122232425262728293031libavformat/tcp.cconst URLProtocol ff_tcp_protocol = &#123; .name = "tcp", .url_open = tcp_open, .url_accept = tcp_accept, .url_read = tcp_read, .url_write = tcp_write, .url_close = tcp_close, .url_get_file_handle = tcp_get_file_handle, .url_get_short_seek = tcp_get_window_size, .url_shutdown = tcp_shutdown, .priv_data_size = sizeof(TCPContext), .flags = URL_PROTOCOL_FLAG_NETWORK, .priv_data_class = &amp;tcp_class,&#125;;static int tcp_write(URLContext *h, const uint8_t *buf, int size)&#123; TCPContext *s = h-&gt;priv_data; int ret; if (!(h-&gt;flags &amp; AVIO_FLAG_NONBLOCK)) &#123; ret = ff_network_wait_fd_timeout(s-&gt;fd, 1, h-&gt;rw_timeout, &amp;h-&gt;interrupt_callback); if (ret) return ret; &#125; /// socket send ret = send(s-&gt;fd, buf, size, MSG_NOSIGNAL); return ret &lt; 0 ? ff_neterrno() : ret;&#125; 整体的代码流程如上，关键入口都在对应的地方加了注释 class relationavformat 初始化 根据 url协议找到 urlprotocol， 创建urlcontext &amp; aviocontext 三者的关系如下 123avformatcontext.pb = aviocontextaviocontext.opaque = urlcontexturlcontext.prot = urlprotocol urlprotocol privdata 指定为 httpcontexthttpcontext 内部持有的urlcontext 根据 tcp 找到tcpprotocolhttpconext.hd = urlcontext 后续进入到 tcp 发送数据的流程 SSL/TLS上面为了快速弄清楚调用过程，当做http处理，简化了tls这一块，https早已普及，所以https 跟 tcp 中间有一层 ssl/tls ffmpeg 内部处理tls协议的有好几个 gnutls openssl schannel securetransport libtls mbedtls根据不用的平台保留对应的哪一个，我的设备是mac，对应的是 securetransport 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208libavformat/tls_securetransport.cconst URLProtocol ff_tls_protocol = &#123; .name = "tls", .url_open2 = tls_open, .url_read = tls_read, .url_write = tls_write, .url_close = tls_close, .url_get_file_handle = tls_get_file_handle, .url_get_short_seek = tls_get_short_seek, .priv_data_size = sizeof(TLSContext), .flags = URL_PROTOCOL_FLAG_NETWORK, .priv_data_class = &amp;tls_class,&#125;;typedef struct TLSContext &#123; const AVClass *class; TLSShared tls_shared; SSLContextRef ssl_context; CFArrayRef ca_array; int lastErr;&#125; TLSContext;libavformat/tls.htypedef struct TLSShared &#123; char *ca_file; int verify; char *cert_file; char *key_file; int listen; char *host; char *http_proxy; char underlying_host[200]; int numerichost; /// tls内部就是tcp URLContext *tcp;&#125; TLSShared;static int tls_open(URLContext *h, const char *uri, int flags, AVDictionary **options)&#123; TLSContext *c = h-&gt;priv_data; TLSShared *s = &amp;c-&gt;tls_shared; int ret; /// tcp open if ((ret = ff_tls_open_underlying(s, h, uri, options)) &lt; 0) goto fail; /// sslcontext 创建过程 见底部引用的苹果文档 c-&gt;ssl_context = SSLCreateContext(NULL, s-&gt;listen ? kSSLServerSide : kSSLClientSide, kSSLStreamType); if (!c-&gt;ssl_context) &#123; av_log(h, AV_LOG_ERROR, "Unable to create SSL context\n"); ret = AVERROR(ENOMEM); goto fail; &#125; if (s-&gt;ca_file) &#123; if ((ret = load_ca(h)) &lt; 0) goto fail; &#125; if (s-&gt;ca_file || !s-&gt;verify) CHECK_ERROR(SSLSetSessionOption, c-&gt;ssl_context, kSSLSessionOptionBreakOnServerAuth, true); if (s-&gt;cert_file) if ((ret = load_cert(h)) &lt; 0) goto fail; CHECK_ERROR(SSLSetPeerDomainName, c-&gt;ssl_context, s-&gt;host, strlen(s-&gt;host)); /// io 回调 CHECK_ERROR(SSLSetIOFuncs, c-&gt;ssl_context, tls_read_cb, tls_write_cb); CHECK_ERROR(SSLSetConnection, c-&gt;ssl_context, h); while (1) &#123; OSStatus status = SSLHandshake(c-&gt;ssl_context); if (status == errSSLServerAuthCompleted) &#123; SecTrustRef peerTrust; SecTrustResultType trustResult; if (!s-&gt;verify) continue; if (SSLCopyPeerTrust(c-&gt;ssl_context, &amp;peerTrust) != noErr) &#123; ret = AVERROR(ENOMEM); goto fail; &#125; if (SecTrustSetAnchorCertificates(peerTrust, c-&gt;ca_array) != noErr) &#123; ret = AVERROR_UNKNOWN; goto fail; &#125; if (SecTrustEvaluate(peerTrust, &amp;trustResult) != noErr) &#123; ret = AVERROR_UNKNOWN; goto fail; &#125; if (trustResult == kSecTrustResultProceed || trustResult == kSecTrustResultUnspecified) &#123; // certificate is trusted status = errSSLWouldBlock; // so we call SSLHandshake again &#125; else if (trustResult == kSecTrustResultRecoverableTrustFailure) &#123; // not trusted, for some reason other than being expired status = errSSLXCertChainInvalid; &#125; else &#123; // cannot use this certificate (fatal) status = errSSLBadCert; &#125; if (peerTrust) CFRelease(peerTrust); &#125; if (status == noErr) &#123; break; &#125; else if (status != errSSLWouldBlock) &#123; av_log(h, AV_LOG_ERROR, "Unable to negotiate TLS/SSL session: %i\n", (int)status); ret = AVERROR(EIO); goto fail; &#125; &#125; return 0;fail: tls_close(h); return ret;&#125;static OSStatus tls_read_cb(SSLConnectionRef connection, void *data, size_t *dataLength)&#123; URLContext *h = (URLContext*)connection; TLSContext *c = h-&gt;priv_data; size_t requested = *dataLength; /// 交给下一层的tcp去处理 read /// 这里读到的数据 还没有经过tls解码，不可读 int read = ffurl_read(c-&gt;tls_shared.tcp, data, requested); if (read &lt;= 0) &#123; *dataLength = 0; switch(AVUNERROR(read)) &#123; case ENOENT: case 0: return errSSLClosedGraceful; case ECONNRESET: return errSSLClosedAbort; case EAGAIN: return errSSLWouldBlock; default: c-&gt;lastErr = read; return ioErr; &#125; &#125; else &#123; *dataLength = read; if (read &lt; requested) return errSSLWouldBlock; else return noErr; &#125;&#125;static OSStatus tls_write_cb(SSLConnectionRef connection, const void *data, size_t *dataLength)&#123; URLContext *h = (URLContext*)connection; TLSContext *c = h-&gt;priv_data; /// 交给下一层的tcp去处理 write /// data 已经经过的tls的加密处理了。不可读 int written = ffurl_write(c-&gt;tls_shared.tcp, data, *dataLength); if (written &lt;= 0) &#123; *dataLength = 0; switch(AVUNERROR(written)) &#123; case EAGAIN: return errSSLWouldBlock; default: c-&gt;lastErr = written; return ioErr; &#125; &#125; else &#123; *dataLength = written; return noErr; &#125;&#125;static int tls_read(URLContext *h, uint8_t *buf, int size)&#123; TLSContext *c = h-&gt;priv_data; size_t available = 0, processed = 0; int ret; SSLGetBufferedReadSize(c-&gt;ssl_context, &amp;available); if (available) size = FFMIN(available, size); /// 读数据，这里读到的数据已经经过的tls的解密，是明文了 断点可以查看数据 ret = SSLRead(c-&gt;ssl_context, buf, size, &amp;processed); ret = map_ssl_error(ret, processed); if (ret &gt; 0) return ret; if (ret == 0) return AVERROR_EOF; return print_tls_error(h, ret);&#125;static int tls_write(URLContext *h, const uint8_t *buf, int size)&#123; TLSContext *c = h-&gt;priv_data; size_t processed = 0; /// 写数据，应用层的数据还没有经过tls的加密，断点可读 int ret = SSLWrite(c-&gt;ssl_context, buf, size, &amp;processed); ret = map_ssl_error(ret, processed); if (ret &gt; 0) return ret; if (ret == 0) return AVERROR_EOF; return print_tls_error(h, ret);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465libavformat/tls.cint ff_tls_open_underlying(TLSShared *c, URLContext *parent, const char *uri, AVDictionary **options)&#123; int port; const char *p; char buf[200], opts[50] = ""; struct addrinfo hints = &#123; 0 &#125;, *ai = NULL; const char *proxy_path; char *env_http_proxy, *env_no_proxy; int use_proxy; set_options(c, uri); if (c-&gt;listen) snprintf(opts, sizeof(opts), "?listen=1"); av_url_split(NULL, 0, NULL, 0, c-&gt;underlying_host, sizeof(c-&gt;underlying_host), &amp;port, NULL, 0, uri); p = strchr(uri, '?'); if (!p) &#123; p = opts; &#125; else &#123; if (av_find_info_tag(opts, sizeof(opts), "listen", p)) c-&gt;listen = 1; &#125; ff_url_join(buf, sizeof(buf), "tcp", NULL, c-&gt;underlying_host, port, "%s", p); hints.ai_flags = AI_NUMERICHOST; if (!getaddrinfo(c-&gt;underlying_host, NULL, &amp;hints, &amp;ai)) &#123; c-&gt;numerichost = 1; freeaddrinfo(ai); &#125; if (!c-&gt;host &amp;&amp; !(c-&gt;host = av_strdup(c-&gt;underlying_host))) return AVERROR(ENOMEM); env_http_proxy = getenv_utf8("http_proxy"); proxy_path = c-&gt;http_proxy ? c-&gt;http_proxy : env_http_proxy; env_no_proxy = getenv_utf8("no_proxy"); use_proxy = !ff_http_match_no_proxy(env_no_proxy, c-&gt;underlying_host) &amp;&amp; proxy_path &amp;&amp; av_strstart(proxy_path, "http://", NULL); freeenv_utf8(env_no_proxy); if (use_proxy) &#123; char proxy_host[200], proxy_auth[200], dest[200]; int proxy_port; av_url_split(NULL, 0, proxy_auth, sizeof(proxy_auth), proxy_host, sizeof(proxy_host), &amp;proxy_port, NULL, 0, proxy_path); ff_url_join(dest, sizeof(dest), NULL, NULL, c-&gt;underlying_host, port, NULL); ff_url_join(buf, sizeof(buf), "httpproxy", proxy_auth, proxy_host, proxy_port, "/%s", dest); &#125; freeenv_utf8(env_http_proxy); ///熟悉的方法，根据上面拼接的tcp:// 打开tcp return ffurl_open_whitelist(&amp;c-&gt;tcp, buf, AVIO_FLAG_READ_WRITE, &amp;parent-&gt;interrupt_callback, options, parent-&gt;protocol_whitelist, parent-&gt;protocol_blacklist, parent);&#125; tls这块的处理逻辑 作为http 跟 tcp的中间层，都是遵循的URLProtocol协议，所以流程基本跟http、tcp一样。关键点也同样加了对应的注释辅助理解，关于sslcontext 见下面的苹果文档， 如果熟悉tls的四次握手，基本还是可以理解。 summary整个代码过程顺下来，基本就是网络协议的调用过程，http/https -&gt; ssl/tls -&gt; tcp(socket) 最后来一张wireshark的抓包，很明显看到从tcp的三次握手开始， 紧接着tls的四次捂手，然后就是真正的请求数据的传输了… reference#macos security]]></content>
      <categories>
        <category>ffmepg</category>
      </categories>
      <tags>
        <tag>ffmepg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ios获取udid]]></title>
    <url>%2F2022%2F11%2F04%2Fios%E8%8E%B7%E5%8F%96udid%2F</url>
    <content type="text"><![CDATA[背景苹果企业账号打包分发app，方便内部安装测试App。最近公司企业账号续费失败，申请新的企业账号没成功。（国内近2年基本没有企业申请成功）这就蛋疼了，了解到其他app是通过多开发者账号，adhoc证书白名单的方式来做分发，adhoc证书无法解决设备数量的问题，最多100台设备，而且还要提前注册好，一年更新一次，注册设备就是要拿到设备的udid。 获取udid了解到苹果可以通过safari以OTA(over the air)的方式拿到udid。 整体流程分三步 下发udid.mobileconfig文件 授权安装配置文件 回调解析xml拿到udid udid.mobileconfig一个xml文件，文件内容如下 1234567891011121314151617181920212223242526272829303132333435&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;&lt;plist version="1.0"&gt; &lt;dict&gt; &lt;key&gt;PayloadContent&lt;/key&gt; &lt;dict&gt; &lt;key&gt;URL&lt;/key&gt; &lt;string&gt;https://xxx.net/udid/action&lt;/string&gt; &lt;!--接收数据的接口地址--&gt; &lt;key&gt;DeviceAttributes&lt;/key&gt; &lt;array&gt; &lt;string&gt;SERIAL&lt;/string&gt; &lt;string&gt;MAC_ADDRESS_EN0&lt;/string&gt; &lt;string&gt;UDID&lt;/string&gt; &lt;string&gt;IMEI&lt;/string&gt; &lt;string&gt;ICCID&lt;/string&gt; &lt;string&gt;VERSION&lt;/string&gt; &lt;string&gt;PRODUCT&lt;/string&gt; &lt;/array&gt; &lt;/dict&gt; &lt;key&gt;PayloadOrganization&lt;/key&gt; &lt;string&gt;dev.xxx.com&lt;/string&gt; &lt;!--组织名称--&gt; &lt;key&gt;PayloadDisplayName&lt;/key&gt; &lt;string&gt;查询设备UDID&lt;/string&gt; &lt;!--安装时显示的标题--&gt; &lt;key&gt;PayloadVersion&lt;/key&gt; &lt;integer&gt;1&lt;/integer&gt; &lt;key&gt;PayloadUUID&lt;/key&gt; &lt;string&gt;CD0BD3C5-7164-B9DA-FCBB-D81AA343C7A0&lt;/string&gt; &lt;!--自己随机填写的唯一字符串--&gt; &lt;key&gt;PayloadIdentifier&lt;/key&gt; &lt;string&gt;dev.xxx.profile-service&lt;/string&gt; &lt;key&gt;PayloadDescription&lt;/key&gt; &lt;string&gt;本文件仅用来获取设备ID&lt;/string&gt; &lt;!--描述--&gt; &lt;key&gt;PayloadType&lt;/key&gt; &lt;string&gt;Profile Service&lt;/string&gt; &lt;/dict&gt;&lt;/plist&gt; 需要注意就是回调接口 需要支持https，否者会报ATS错误 the resource could not be loaded because the app transport security policy requires the use of a secure connection 起service下发 xml 文件，我这里以java spring mvc来实现的 123456789101112131415161718@RequestMapping("/udid.mobileconfig") @ResponseBody public ResponseEntity&lt;byte[]&gt; downloadsEntity(HttpServletRequest request) throws Exception &#123; String path = Objects.requireNonNull(this.getClass().getClassLoader().getResource("udid.mobileconfig")).toURI().getPath(); File file = new File(path); if (!file.isFile()) &#123; return null; &#125; byte[] bytes = FileUtils.readFileToByteArray(file); HttpHeaders headers = new HttpHeaders(); headers.add("Content-type","application/x-apple-aspen-config; charset=utf-8"); // 按苹果的规范 headers.add("Content-Disposition", "attachment;filename=" + "udid.mobileconfig"); HttpStatus status = HttpStatus.OK; return new ResponseEntity&lt;&gt;(bytes, headers, status); &#125; 授权安装下载udid.mobileconfig之后，系统会自动在设置里面登记该配置文件，需要自行去设置里面 授权安装。安装成功之后 会回调 mobileconfig里面提供的回调地址，udid等信息会以参数的形式一并带上，格式也是xml格式。 如果下载的过程提示无效文件，可能就是mobileconfig文件内容有问题，可以通过pc浏览器检查下文件内容，我这里就遇到了文件乱码的情况 上面提到文件内容是以字节流的方式下发的，配置好ByteArrayHttpMessageConverter的顺序就ok了 123456789101112131415161718192021222324252627&lt;!--自动注册基于注解风格的处理器 (数据转换 格式化 数据)--&gt; &lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;ref bean="stringHttpMessageConverter"/&gt; &lt;!--配置ByteArrayHttpMessageConverter--&gt; &lt;bean class="org.springframework.http.converter.ByteArrayHttpMessageConverter"/&gt; &lt;bean class="com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter"&gt; &lt;property name="features"&gt; &lt;array&gt; &lt;!-- List字段如果为null,输出为[],而非nul --&gt; &lt;value&gt;WriteNullListAsEmpty&lt;/value&gt; &lt;!-- 字符类型字段如果为null,输出为”“,而非null --&gt; &lt;value&gt;WriteNullStringAsEmpty&lt;/value&gt; &lt;!-- 进制循环引用 --&gt; &lt;value&gt;DisableCircularReferenceDetect&lt;/value&gt; &lt;!--Boolean字段如果为null,输出为false,而非null --&gt; &lt;value&gt;WriteNullBooleanAsFalse&lt;/value&gt; &lt;!--时间 yyyy.MM.dd hh:mm:ss --&gt; &lt;value type="com.alibaba.fastjson.serializer.SerializerFeature"&gt;WriteDateUseDateFormat&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt; &lt;/mvc:annotation-driven&gt; 解析udid还是以java实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@RequestMapping(value = "/action", method = RequestMethod.POST) @ResponseBody public ResponseEntity&lt;String&gt; action(HttpServletRequest request) throws IOException &#123; //获取HTTP请求的输入流 InputStream is = request.getInputStream(); //已HTTP请求输入流建立一个BufferedReader对象 BufferedReader br = new BufferedReader(new InputStreamReader(is,"UTF-8")); StringBuilder sb = new StringBuilder(); //读取HTTP请求内容 String buffer = null; while ((buffer = br.readLine()) != null) &#123; sb.append(buffer); &#125; String content = sb.toString().substring(sb.toString().indexOf("&lt;?xml"), sb.toString().indexOf("&lt;/plist&gt;")+8); System.out.println(content); // 创建xml解析对象 SAXReader reader = new SAXReader(); // 定义一个文档 Document document = null; //将字符串转换为 try &#123; document = reader.read(new ByteArrayInputStream(content.getBytes("GBK"))); &#125; catch (DocumentException e) &#123; e.printStackTrace(); &#125; String udid = "ERROR"; if(document != null) &#123; Element element = (Element) document.selectNodes("/plist/dict").get(0); for(Iterator iter = element.content().iterator(); iter.hasNext();) &#123; DefaultElement next = (DefaultElement) iter.next(); String name = next.getName(); String text = next.getText(); if(StringUtils.isEquals(name,"key") &amp;&amp; StringUtils.isEquals(text,"UDID") &amp;&amp; iter.hasNext()) &#123; next = (DefaultElement) iter.next(); name = next.getName(); text = next.getText(); udid = text; &#125; &#125; &#125; HttpHeaders headers = new HttpHeaders(); headers.add("Content-type","text/html; charset=utf-8"); headers.setLocation(URI.create("https://xxx.com/udid/udid?udid="+udid)); /// 跳转到指定的页面 展示udid信息 HttpStatus status = HttpStatus.MOVED_PERMANENTLY; return new ResponseEntity&lt;&gt;("", headers, status); &#125; 解析xml就可以拿到udid字段，注意的是 重定向一定要使用301重定向（MOVED_PERMANENTLY）,有些重定向默认是302重定向,这样就会导致安装失败,设备安装会提示”无效的描述文件]]></content>
      <categories>
        <category>ios</category>
      </categories>
      <tags>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MP4笔记]]></title>
    <url>%2F2022%2F10%2F28%2FMP4%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MP4 BOX第一层Ftyp、Moov、Mdat、Free等: ftyp Boxftyp是MP4文件的第一个Box，包含了视频文件使用的编码格式、标准等，这个Box作用基本就是MP4这种封装格式的标识，同时在一份MP4文件中只有一个这样的Box。ftyp box通常放在文件的开始，通过对该box解析可以让我们的软件（播放器、demux、解析器）知道应该使用哪种协议对这该文件解析，是后续解读文件基础 Moov BoxMoov Box这个Box也是MP4文件中必须有但是只存在一个的Box,这个Box里面一般存的是媒体文件的元数据，这个Box本身是很简单的，是一种Container Box，里面的数据是子Box,自己更像是一个分界标识。 所谓的媒体元数据主要包含类似SPS PPS的编解码参数信息，还有音视频的时间戳等信息。对于MP4还有一个重要的采样表stbl信息，这里面定义了采样Sample、Chunk、Track的映射关系，是MP4能够进行随机拖动和播放的关键，也是需要好好理解的部分，对于实现一些音视频特殊操作很有帮助。 Mdat BoxMdat Box这个Box是存储音视频数据的Box，要从这个Box解封装出真实的媒体数据。当然这个Box一般都会存在，但是不是必须的。 在前面的文章《音视频压缩：H264码流层次结构和NALU详解》中已经讲解了H264的基本结构是由一系列的NALU组成。原始的NALU单元组成： Start code + NALU header + NALU payload 但是在MP4格式文件中，H264 slice并不是以00 00 00 01 Start Code来进行分割，而是存储在Mdat Box的Data中。 Mdat Box的格式： Box header + Box Data Box length + Box Type + NALU length + NALU header + Nalu Data……. NALU length + NALU header + Nalu Data Free BoxFree Box中的内容是无关紧要的，可以被忽略即该box被删除后，不会对播放产生任何影响。这种类型的Box也不是必须的，可有可无，类似的Box还有Sikp Box.虽然在解析是可以忽略，但是需要注意该Box的删除对其它Box的偏移量影响，特别是当Moov Box放到Mdat Box后面的情况。 MOOVMvhd Box这个Box也是全文件唯一的一个Box,一般处于Moov Box的第一个子Box,这个Box对整个媒体文件所包含的媒体数据（包含Video track和Audio Track等）进行全面的描述。其中包含了媒体的创建和修改时间，默认音量、色域、时长等信息。 Trak BoxTrak Box定义了媒体文件中媒体中一个Track的信息，视频有Video Track,音频有Audio Track，媒体文件中可以有多个Track，每个Track具有自己独立的时间和空间的信息，可以进行独立操作。 每个Track Box都需要有一个Tkhd Box和Mdia Box，其它的Box都是可选择的 TrakTkhd Box该Box描述了该Track的媒体整体信息包括时长、图像的宽度和高度等，实际比较重要 Mdia Box这个Box也是Container Box，里面包含子Box，一般必须有Mdhd Box、Hdlr Box、Minf Box。基本就是当前Track媒体头信息和媒体句柄以及媒体信息。 Box自身非常简单，就是一个标识而已，最复杂的还是里面包含的子Box. MdiaMdhd Box这个Box是Full Box，意味着Box Header有Version和Flag字段，该Box里面主要定义了该Track的媒体头信息，其中我们最关心的两个字段是Time scale和Duration，分别表示了该Track的时间戳和时长信息，这个时间戳信息也是PTS和DTS的单位。 Hdlr Box这个Box是Full Box，意味着Box Header有Version和Flag字段，该Box解释了媒体的播放过程信息，用来设置不同Track的处理方式，标识了该Track的类型。 Minf Box这个Box是我认为Moov Box里面最重要最复杂的Box，内部还有子Box，我们还是从上到下从外到内的分析各个Box。该Box建立了时间到真实音视频Sample的映射关系，对于音视频数据操作时很有帮助的。 同时该Box是Container Box，下面一般含有三大必须的子Box: 媒体信息头Box: Vmhd Box或者Smhd Box; 数据信息Box：Dinf Box 采样表Box：Stbl Box MinfVmhd BoxDinf BoxStbl Box怎样把媒体数据的Sample和时间进行映射的，看下Sample-Trunk-Track的三者关系。 我们知道Sample是媒体数据的存储单元，其中存储在Media的chunk中，在分析Mdat Box的H264 NALU打包时已经体现出来了 其中每个Sample的时间和位置、编解码信息、和chunk关系等都是由Stbl Box来描述的，该Box又称为采样参数列表即Sample Table。 Stbl Box包含了Track中media媒体的所有时间和索引，利用这个容器的Sample信息，就可以定位Sample的媒体时间、类型、大小以及和其相邻的Sample。同时该Box是必须在Trak Box中存在的。 其一般要包含下列子Box： 采样描述容器： Sample Description即Stsd Box 采样时间容器： Time To Sample 即Stts Box 采样同步容器： Sync Sample即Stss Chunk采样容器: Sample To Chunk即Stsc 采样大小容器： Sample Size即Stsz Chunk偏移容器：Chunk Offest即Stco Stsd Box 该Box存储了编码类型和初始化解码器需要的信息，与特定的Track Type有关，根于不同的Track使用不一样的编码标准。 box header和version字段后会有一个entry count字段，根据entry的个数，每个entry会有type信息，如“vide”、“sund”等，根据type不同sample description会提供不同的信息，例如对于video track，会有“VisualSampleEntry”类型信息，对于audio track会有“AudioSampleEntry”类型信息。视频的编码类型、宽高、长度，音频的声道、采样等信息都会出现在这个box中。 对于Video Track里面而言，H264的SPS PPS就存在该Box里面，对于解码非常重要。 Stsd/Avc1 Box: 当为Video Track，所以VisualSampleEntry为Avc1 Box，内部含有SPS PPS 等音视频解码信息。 Stsd/Avc1/AVCC Box 该Box则包含了真实的SPS PPS等信息，包含着音视频编解码参数： 音频的STSD的Box Data主要由mp4a和esds Box嵌套组成，里面包含了通道个数，采样率、音频AAC编码级别等信息 Stsd/mp4a Box: Stsd/mp4a/Esds Box 里面主要是将音频的编码信息和音频码率信息放到该Box里面，所以解码音频时非常关键。 Stts Box 这个Box是sample number和解码时间DTS之间的映射表，通过这个表格，我们可以找到任何时间的sample。Stts Box这个表格有两项值，其中一项是连续的样点数目即sample count和样点相对时间差值即sample delta即表格中每个条目提供了在同一个时间偏移量里面连续的sample序号以及sample偏移量。当然这里的相对时间差单位是由该Track的Mdhd Box描述的时间单位 Ctts Box 该Box保存了每个sample的composition time和decode time之间的差值，这里通过Composition Time就可以计算出Sample的PTS。 不存在B帧情况下PTS是等于DTS的，则不含B帧的视频文件是没有Ctts Box的，同样音频也没有Ctts Box。 Stss Box I帧是播放的起始位置，只有编码器拿到第一个I帧才能渲染出第一幅画面。所以后续的一些特殊随机操作，高标清切换时都需要找I帧，只有随机找到I帧才能完成这些特殊操作。 其中该Box就是存储了那些Sample是I关键帧，很显然音频Track也是不存在这个Box的。 Stsc Box MP4中存在Track-Trunk-Sample概念，讲到现在Track和Sample已经都讲到了，但是还没有讲解Trunk和Sample的映射关系，这个Box就是说明那些Sample可以划分为一个Trunk。 媒体数据被分为若干个chunk, chunk可以有不同的大小，同一个chunk中的样点sample也允许有不同的大小；通过本表可以定位一个样点的chunk位置,同时该Box里面的Box Data里面有三个字段，分别是first chunk、sample per chunk、sample description index。 fist chunk: 具有相同采样点sample和sample_description_index的chunk中，第一个chunk的索引值,也就是说该chunk索引值一直到下一个索引值之间的所有chunk都具有相同的sample个数，同时这些sample的描述description也一样； samples per chunk: 上面所有chunk的sample个数 sample description index: 描述采样点的采样描述项的索引值，范围为1到样本描述表中的表项数目； 这 3 个字段实际上决定了一个 MP4 中有多少个 chunks，每个 chunks 有多少个 samples。 Stsz Box 前面分析了sample的PTS、DTS等，也分析了chunk里面sample的信息，但是没有分析sample的大小，这是我们在文件读取和解析Sample的关键。这里给出每个Sample的Size即包含的字节数。 包含了媒体中全部sample的数目和一张给出每个sample大小的表。这个box相对来说体积是比较大的。 Stco Box该Box存储了Chunk Offset，表示了每个Chunk在文件中的位置，这样我们就能找到了chunk在文件的偏移量，然后根据其它表的关联关系就可以读取每个Sample的大小。 H.264数据打包MP4文件步骤 构造Ftyp Box，这是MP4文件的基本标识，基本按照规范构造即可； 构造Mdat Box，这里面都是音视频媒体数据，将NALU数据封装成一个个Sample，从上到下排列起来即可； 构造Moov Box,这个Box，先构造除了stbl的其它Box,这样从子Box，一直按照层次构造上来，这些Box的基本信息是明确的，个别信息通过解析SPS、PPS是可以明确知道的，所以上文已经明确了这些Box的字段，所以构造起来并不难； 构造Moov Box的Stbl部分，这部分要封装： SPS、PPS的NALU数据 也要构造时间戳PTS和DTS信息 Sample的大小和数量 Chunk的数量和在文件偏移量 Sample和Chunk的映射关系等， 需要仔细处理，一旦构造错误都会播放失败； 其中Moov到底是放到Mdat Box前面还是后面，需要在封装打包前确定后，因为会牵一发动全身，里面的信息都依赖在具体文件的位置，至于前面后面区别见下篇文章； 再填充一些你感兴趣的非必要信息Box，至此组装成整个MP4文件即可； reference# MP4核心Box详解]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[H264笔记]]></title>
    <url>%2F2022%2F10%2F26%2FH264%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[H264H264是视频编码规范，在这个规范下H264码流有两种方式：AnnexB格式 &amp;&amp; AVCC格式 AnnexB AnnexB 的原理是在每个 NALU 前面写上一个特殊的起始码，通过这个起始码来当做 NALU 的分隔符，从而分割每个 NALU [start code] NALU [start code] NALU [start code] NALU AVCC avcC 则采用了另外一种方式。那就是在 NALU 前面写上几个字节，这几个字节组成一个整数（大端字节序）这个整数表示了整个 NALU 的长度 [extra data] [length] NALU [length] NALU [length] NALU NALUNALU (Network Abstraction Layer Unit) 翻译过来就是网络抽象层单元。在 H.264/AVC 视频编码标准中，最外面一层叫做 NAL(Network Abstract Layer) 网络抽象层。所有的码流数据，最终都被封装成了一个一个的 NALU（Network Abstract Layer Unit）就是网络抽象层单元。 对应的NAL 还有一个视频编码层VCL(video code layer)，视频编码后的原始数据 SODB （String Of Data Bits） 视频编码层（VCL），是对视频编码核心算法过程、子宏块、宏块、片等概念的定义。这层主要是为了尽可能的独立于网络来高效的对视频内容进行编码。编码完成后，输出的数据是 SODB（String Of Data Bits）。 网络适配层（NAL），是对图像序列、图像等片级别以上的概念的定义。这层负责将 VCL 产生的比特字符串适配到各种各样的网络和多元环境中。该层将 VCL 层输出的 SODB 数据打包成 RBSP（Raw Byte Sequence Payload）。SODB 是编码后的原始数据，RBSP 是在原始编码数据后面添加了结尾比特，一个比特 1 和若干个比特 0，用于字节对齐。然后再在 RBSP 头部加上 NAL Header 来组成一个一个的 NAL 单元。 sps &amp; ppsSPS（Sequence Paramater Set） 序列参数集 PPS（Picture Paramater Set） 图像参数集 SPS 和 PPS 存放的的信息是码流中的参数信息，后续的解码流程是要依赖着里面的参数的，所以解码器在解码一路码流的时候，总是要首先读入 SPS 和 PPS AnnexB 格式的码流，sps 跟 pps 是作为nalu存在的，一般都最前面AVCC格式的码流，sps 跟 pps 是放在extra data中。 NALU header nalu 的第一个字节，其实就是nalu header forbidden_zero_bit nal_ref_idc nal_unit_type forbidden_zero_bit 占用 1 位，nal_ref_idc 占用 2 位，nal_unit_type 占用 5 位。三个元素一共占用 8 位，也就是一个字节 forbidden_zero_bit 禁止位，正常情况下为 0。在某些情况下，如果 NALU 发生丢失数据的情况，可以将这一位置为 1，以便接收方纠错或丢掉该单元 nal_ref_idc 该元素表示这个 NALU 的重要性。可能的值有 4 个，越重要的 NALU 越不能丢弃 nal_ref_idc 重要性 3 HIGHEST 2 HIGH 1 LOW 0 DISPOSABLE nal_unit_type nal_unit_type NALU 类型 0 未定义 1 非 IDR SLICE slice_layer_without_partitioning_rbsp( ) 2 非 IDR SLICE，采用 A 类数据划分片段 slice_data_partition_a_layer_rbsp( ) 3 非 IDR SLICE，采用 B 类数据划分片段 slice_data_partition_b_layer_rbsp( ) 4 非 IDR SLICE，采用 C 类数据划分片段 slice_data_partition_c_layer_rbsp( ) 5 IDR SLICE slice_layer_without_partitioning_rbsp( ) 6 补充增强信息 SEI sei_rbsp( ) 7 序列参数集 SPS seq_parameter_set_rbsp( ) 8 图像参数集 PPS pic_parameter_set_rbsp( ) 9 分隔符 access_unit_delimiter_rbsp( ) 10 序列结束符 end_of_seq_rbsp( ) 11 码流结束符 end_of_stream_rbsp( ) 12 填充数据 filler_data_rbsp( ) 13 序列参数扩展集 seq_parameter_set_extension_rbsp( ) 14~18 保留 19 未分割的辅助编码图像的编码条带 slice_layer_without_partitioning_rbsp( ) 20~23 保留 24~31 未指定 slice我们之前介绍过 SPS 和 PPS，SPS 和 PPS 中储存的信息是一些参数项，例如，图像的长宽，图像的 profile 信息等。那么在 Slice 中，存放的信息就是编码后的图像信息了，也就是说，解码 Slice，我们就能还原出来图像了。 一个 Slice 通常被分为两个部分，Slice Header 和 Slice Body Slice Header Slice Body slice_type 值 含义 0 P(P Slice) 1 B(B Slice) 2 I(I Slice) 3 SP(SP Slice) 4 SI(SI Slice) 5 P(P Slice) 6 B(B Slice) 7 I(I Slice) 8 SP(SP Slice) 9 SI(SI Slice) 可以看到 slice_type 规定了 slice 的类型，这也解释了之前的一些误区。有人说根据 NALU 的类型就能判读是 I Slice 还是 P Slice 还是 B Slice。其实是错误的，要判断这些必须要读到 slice_type 才行 pic_parameter_set_id slice_type 之后，是 pic_parameter_set_id，这个属性表明该 Slice 要依赖的 PPS 的 id。我们在解析 PPS 的时候，PPS 里面有一个 pic_parameter_set_id 的属性。当你解析出 Slice 中的 pic_parameter_set_id 之后，拿着这个 id 找到与之对应的 PPS 就可以了。然后 PPS 里还有个 seq_parameter_set_id，用这个 id 就可以找到依赖的 SPS 的 id。这样，你就可以为这个 Slice 查找到合适的 SPS 和 PPS 了。 MP4h264编码之后的码流，最终还是会封装到各种媒体文件中，这里就只了解下最常用的mp4. Mdat Box这个Box是存储音视频数据的Box，要从这个Box解封装出真实的媒体数据。当然这个Box一般都会存在，但是不是必须的。但是在MP4格式文件中，H264 slice并不是以00 00 00 01 Start Code来进行分割，而是存储在Mdat Box的Data中。 Mdat Box的格式： Box header + Box Data Box length + Box Type + NALU length + NALU header + Nalu Data……. NALU length + NALU header + Nalu Data Mdat里的NALU一般不再包含SPS PPS等数据，这些数据已经放到Moov Box里面了 mp4里的NALU其实就是AVCC格式的 references最详尽的 H.264 编码相关概念介绍自己动手写 H.264 解码器H.264 媒体流 AnnexB 和 AVCC 格式分析 及 FFmpeg 解析mp4的H.264码流方法]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[形状特效]]></title>
    <url>%2F2022%2F10%2F14%2F%E5%BD%A2%E7%8A%B6%E7%89%B9%E6%95%88%2F</url>
    <content type="text"><![CDATA[形状特效初次接触到形状特效是在剪映，剪映中叫蒙版，就是列举了一些常见形状的蒙层效果，除了常规的旋转、缩放、位移等操作外，还有个虚化的效果。可以去剪映感受下。 接触到这东西，觉得效果挺不错，打算研究下，正好手头的编辑工具项目也可以用上。线性、圆形、矩形，这几个规范的，通过平方根公式 跟 三角形余弦定理基本都可以推算出来(可以参考github shader)，重点来了，爱心 &amp; 五角星 怎么搞？ 特殊形状爱心 、五角星，一开始觉得可能是通过 三角函数、关键点、贝塞尔曲线 等数学计算实现的？ 想想觉得这样是不是过于复杂， 一翻google之后，还是没啥思路。放大招抓包，剪映的素材都是下发的，必然会有网络请求的，一顿操作之后，如愿拿到了资源包。 爱心 五角星 看到了这两个，很明显不是通过复杂数学计算得到的。 但是这一层一层的渐变红绿图 是什么鬼？资源包里还有shader文件的😄 float alpha = (col.r * 4.0 * 256.0 + col.g * 255.0) / 781.0; 重点的就是这行计算逻辑，还是懵的。 col.r col.g 对应图上的红绿分量可以理解*4 /781 是什么鬼？ 4 对应 图上的四层? 用颜色取色器取色看看，这一看就明白了。 1234567第1层c00d00 第2层800000~80ff00 第3层400000~40ff00 第4层000000~00ff00R * 4 + G 算下来 从 0 到 781 781 = 12(c0) * 16 * 4 + 13(0d) 每层红色分量固定(00/40/80/c0)，绿色分量渐变0到255(00-ff)，所以整个图层解析下来就是 从0 到 781. alpha 计算下来就有 256 *3 + 1 = 769 个值，最里面的一层最小的，固定为1，保障无虚化的时候也有最基本的形状效果。 搞明白之后，顿时觉得秒啊， 回头一想，为什么要搞的那么多复杂呢？ 反正是为了计算alpha，直接搞个灰度图 从黑到白，不就好了？仔细一想，灰度图一个分量最多256个，最终的效果割裂感会很明显，所以用到了r、g 两个分量，让效果更丝滑，实际上如果你想 也可以用rbg三个分量，重新设计下计算公式 让范围更大更丝滑。 举一反三学以致用，那必须举一反三下了。 花型 用五角星图去找设计师，参考一下做一个花型的。设计师也一下没掌握精髓，照着搞了一个但不是我想要的。所以有了人生第一次指导设计师画图了，一翻操作之后 搞定了 最后的shader 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#iUniform float iReverse = 0.#iUniform float _X = 0.5#iUniform float _Y = 0.5#iUniform float _S = 0.25#iUniform float _A = 0.#iUniform float _F = 0.00#iUniform float iPlatform_iOS = 1.0const float PI = 3.1415926;#iChannel0 &quot;file://followw813654.jpeg&quot;#iChannel1 &quot;file://mask1.png&quot;vec2 rotation(vec2 uv, float angle, float ratio)&#123; vec2 center = vec2(0.5, 0.5); mat2 zRotation = mat2(cos(angle), sin(angle), -sin(angle) * ratio, cos(angle) * ratio); vec2 centeredPoint = uv - center; vec2 newUv = zRotation * centeredPoint; return vec2(newUv.x, newUv.y / ratio) + center;&#125;vec2 scale(vec2 uv, vec2 scale)&#123; vec2 newPos = vec2(0.5) + (uv - vec2(0.5)) / scale; return newPos;&#125;vec2 offset(vec2 uv, vec2 offset)&#123; return uv + offset;&#125;void main()&#123; highp vec2 textureCoordinate = gl_FragCoord.xy/iResolution.xy; float radio = iResolution.x / iResolution.y; bool ls = (radio &gt; 1.0); vec4 base = texture(iChannel0, textureCoordinate); float _Radio = 1.0; float _Scale = _S * 5.; _Scale = ls ? _Scale / radio : _Scale; float _Angle = 360. * PI / 180. * _A; vec2 _Offset = vec2(_X * 2.0 - 1.0, (_Y * 2.0 - 1.0)); vec2 newUV = offset(textureCoordinate, vec2(-_Offset.x, _Offset.y)); newUV = scale(newUV, vec2(1. * _Scale, radio * _Scale)); newUV = rotation(newUV, _Angle, _Radio); newUV.y = (iPlatform_iOS == 1.) ? newUV.y : (1. - newUV.y); vec4 mask = texture(iChannel1, newUV) * step(newUV.x, 1.) * step(newUV.y, 1.) * step(0., newUV.x) * step(0., newUV.y); vec2 col = mask.rg; // 怎么理解 ？ 根据 mask图的 作图颜色搭配 动态调整 // 第1层c00d00 第2层800000~80ff00 第3层400000~40ff00 第4层000000~00ff00 // R * 4 + G 算下来 从 0 到 781 // 781 = 12 * 16 * 4 + 13 float alpha = (col.r * 4.0 * 256.0 + col.g * 255.0) / 781.0; alpha = smoothstep(0.49 - abs(sin(iTime/2.)), 0.51 + abs(sin(iTime/2.)), alpha); if (iReverse &gt; .5) &#123; alpha = 1.0 - alpha; &#125; gl_FragColor = mix(vec4(0, 0, 0, alpha), base, alpha);&#125; 完整效果]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态滤镜]]></title>
    <url>%2F2022%2F10%2F14%2F%E5%8A%A8%E6%80%81%E6%BB%A4%E9%95%9C%2F</url>
    <content type="text"><![CDATA[滤镜音视频编辑工具中，对图像或者视频加滤镜是一种常见的做法，普通滤镜很好理解，一张LUT图就搞定了，LUT图一般设计师会提供好，开发拿过来转成纹理，opengl做下颜色转换就好了，这里就不多说了。 但是今天想讲的是动态滤镜，也有叫特效的，动效等等。比如小红书/剪映 等等,感受一下就大概知道了。 动态滤镜先上一个效果感受下： 视频： 很明显动态滤镜跟普通滤镜根本上就不是一回事了，所以更像是动效 特效了，这里不纠结叫什么，重点是关注下怎么做的呢？直观感受就是静态图片上覆盖了一帧帧的透明图片，确实就是这么一回事。 用一帧帧的带alpha通道的图片集，通过opengl blend一帧帧融合也可以实现， 弊端很明显，虽然这种动态效果一般也只有几秒，按30fps，也需要百来张图，图片资源很大，gif格式也会存在同样的问题。如果用视频呢？ 但是视频没有alpha通道。。 方案参考腾讯动画方案 vap 我们用一种特殊的视频，视频的一半表示rgb 另一半表示alpha通道。 表示alpha通道的 rgb 三个分量值一样，所以是灰度图效果，知道这种特殊的视频构成 接下来就好处理了 123456789101112( precision mediump float; varying highp vec2 textureCoordinate; uniform sampler2D inputImageTexture; void main() &#123; vec4 textureColor = texture2D(inputImageTexture,textureCoordinate); vec4 leftColor = texture2D(inputImageTexture,vec2(textureCoordinate.x / 2.0,textureCoordinate.y)); vec4 rightColor = texture2D(inputImageTexture,vec2(textureCoordinate.x / 2.0 + 0.5,textureCoordinate.y)); gl_FragColor = vec4(leftColor.rgb * rightColor.b ,rightColor.b); &#125;); 实际应用中，借用GPUImageMoive来解码一半一半的视频，然后自定义filter，fragmentshader 就是上面的这段，最后渲染到GPUImageView上，就达到了在静态图片上播放透明视频的效果。 12345678910111213141516171819@interface GPUImageCustomFilter : GPUImageFilter@end@implementation GPUImageCustomFilter- (id)init &#123; if (self = [super initWithFragmentShaderFromString:kGPUImageCustomFragmentShaderString]) &#123; &#125; return self;&#125;/// size width / 2- (void)setInputSize:(CGSize)newSize atIndex:(NSInteger)textureIndex &#123; [super setInputSize:CGSizeMake(newSize.width / 2.0, newSize.height) atIndex:textureIndex];&#125;@end 使用customFilter 123456789101112131415161718NSURL *url = [NSURL fileURLWithPath:videoPath];CGRect frame = previewView.bounds;GPUImageView *gpuImageView = [[GPUImageView alloc] init];gpuImageView.backgroundColor = [UIColor clearColor];gpuImageView.fillMode = kGPUImageFillModePreserveAspectRatioAndFill;gpuImageView.frame = frame;GPUImageMovie *movie = [[GPUImageMovie alloc] initWithURL:url];movie.shouldRepeat = YES;movie.playAtActualSpeed = YES;movie.runBenchmark = YES;GPUImageCustomFilter *customFilter = [[GPUImageCustomFilter alloc] init];[movie addTarget:customFilter];[customFilter addTarget:gpuImageView];[movie startProcessing];]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像描边]]></title>
    <url>%2F2022%2F09%2F10%2F%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[image border最近项目中搞了个图像描边的需求，常见的美图工具App都有类似的功能，典型的如美图秀秀，一开始觉得应该不太复杂，正常评估时间，实际做的时候，发现问题比想象中的复杂多了，结果项目不得不延期 😭，所以有必要搞篇文章来总结下教训。 先说整体的流程： 1、原图 -&gt; 2、抠图 -&gt; 3、边缘检测 -&gt; 4、绘制边缘 -&gt; 5、结果导出 这个流程还是很容易想到，但是除了最后一步相对来说容易点，2、3、4都是一路坑 😞 image matting首先是抠图，就是这样的 跟我们这边的算法同学对接，爬虫收集图像、标注、模型训练 一套组合下来，效果不理想，生产环境不可用，第一步就卡住了 😞 为了赶项目周期，最后使用了阿里云的方案，这里就不多说了，算法同学持续优化模型，待成熟之后替换阿里云。 Edge detection这一步相对来说是最复杂的，这里遇到的问题也是最大，耗时最久 最初的方案大致是这样的：抠图结果-&gt;采样缩放图像-&gt;遍历图像bitmap取满足条件的点，条件简单的理解就是点周围3x3范围的点像素值取平均值。为什么要检测边缘，是因为要做虚线描边，获取连续的边缘点之后然后在画布上连接点画出来。 代码大概是这样的… 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647+ (NSArray *)imageFindContours:(UIImage *)image &#123; NSMutableArray *points = [[NSMutableArray array] init]; UIImage *newImage = [image mediumResolution:CGSizeMake(30, 30)]; CFDataRef imageData = CGDataProviderCopyData(CGImageGetDataProvider(newImage.CGImage)); const uint8_t *data = CFDataGetBytePtr(imageData); int w = newImage.size.width; int h = newImage.size.height; unsigned char *bitmap = malloc(w * h * 4); memcpy(bitmap, data, w * h * 4); CGFloat leftmost = 1; CGFloat rightmost = 0; for (int i = 1; i &lt; h - 1; i += 2) &#123; for (int j = 1; j &lt; w - 1; j += 2) &#123; unsigned int left = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, 0)]; unsigned int right = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, 0)]; unsigned int up = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, -1)]; unsigned int down = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, 1)]; unsigned int leftUp = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, -1)]; unsigned int rightUp = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, -1)]; unsigned int leftDown = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(-1, 1)]; unsigned int rightDown = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(1, 1)]; unsigned int center = [WDImageBorder valueForBitmap:data stride:w position:CGPointMake(i, j) offsets:CGSizeMake(0, 0)]; unsigned int avg = (left + right + up + down + leftUp + rightUp + leftDown + rightDown + center) / 9; int offset = i * w + j; if ((avg &gt;= (255. * 0.4) &amp;&amp; avg &lt;= (255. * 0.9)) &amp;&amp; center &gt; 65) &#123; bitmap[offset * 4] = 255; bitmap[offset * 4 + 1] = 0; bitmap[offset * 4 + 2] = 0; bitmap[offset * 4 + 3] = 255; CGFloat scale = 1.15; CGFloat x = (CGFloat)((((float) j / w) - 0.5) * scale + 0.5); CGFloat y = (CGFloat)((((float) i / h) - 0.5) * scale + 0.5); CGPoint point = CGPointMake(x, y); [points addObject:[NSValue valueWithCGPoint:point]]; if (x &lt;= leftmost) leftmost = x; if (x &gt;= rightmost) rightmost = x; &#125; &#125; &#125; ...&#125; 这个有个致命的问题，就是找到点之后，但是没办法有序的连起来，也试过一些方案，但是图像的边缘情况太复杂了，总是有问题，这里就不多说了。 接下来就需求其他的方案，大名鼎鼎的OpenCV出场了，参考官方文档，编译产物，接入app 调试下来就能获取正确的结果了，这里就不多说了，直接看下代码，对应的节点有注释说明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859+ (NSArray *)findContours:(UIImage *)image &#123; Mat src; Mat src_gray; src = [self cvMatFromUIImage:image]; cvtColor(src, src_gray, COLOR_BGR2GRAY); //UIImage *grayImg = [self UIImageFromCVMat:src_gray]; blur(src_gray, src_gray, cv::Size(3, 3)); //UIImage *blurgrayImg = [self UIImageFromCVMat:src_gray]; /// 利用阈值二值化 threshold(src_gray,src_gray,128,255,cv::THRESH_BINARY); /// 用Canny算子检测边缘 //Canny(src_gray, src_gray, 128, 255 , 3); //UIImage *canny_outputImg = [self UIImageFromCVMat:src_gray]; vector&lt;vector&lt;cv::Point&gt; &gt; contours; vector&lt;Vec4i&gt; hierarchy; /// 寻找轮廓 findContours(src_gray, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, cv::Point(0, 0)); /// 绘出轮廓 Mat drawing = Mat::zeros(src_gray.size(), CV_8UC3); for (int i = 0; i &lt; contours.size(); i++) &#123; Scalar color = Scalar(255, 255, 255); drawContours(drawing, contours, i, color, 1, 8, hierarchy, 0, cv::Point()); &#125; //UIImage *contoursImg = [self UIImageFromCVMat:drawing]; NSMutableArray *array = [NSMutableArray arrayWithCapacity:contours.size()]; for (int i = 0; i &lt; contours.size(); i++) &#123; if (hierarchy[i][3] &gt;= 0 || hierarchy[i][2] &gt;= 0) &#123; continue; &#125; vector&lt;cv::Point&gt; vect = contours[i]; std::vector&lt;cv::Point&gt;::const_iterator it; // declare a read-only iterator it = vect.cbegin(); // assign it to the start of the vector while (it != vect.cend()) &#123; // while it hasn't reach the end //std::cout &lt;&lt; it-&gt;x &lt;&lt;' '&lt;&lt; it-&gt;y &lt;&lt;' '; // print the value of the element it points to [array addObject:@(CGPointMake(it-&gt;x / image.size.width, it-&gt;y / image.size.height))]; ++it; // and iterate to the next element &#125; &#125; return @[array];&#125; 这个方案唯一的缺陷就是需要引入OpenCV静态库，增加包大小，也想过咱只用到了边缘检测，其他的牛逼功能暂时也用不到，裁剪下只保留需要的类是不是就可以，但是大致翻了下，牵扯的太多，最终放弃了, 最后也并没有使用OpenCV的方案。 因为发现了更轻量级的方案，Suzuki边缘检测算法，后面也了解该算法其实就是OpenCV内部的一种边缘检测方案。 恰好Android同学找到了一个开源库，java版本的Suzuki边缘检测算法。代码拉下来结合算法文档来回撸几遍，大致能理解了，android直接java拖进去用上，ios翻译成OC，也不复杂，因为算法核心方法也不过几十行代码，就算不理解，硬翻也能翻译过来。 贴一下翻译成OC的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212+ (NSArray *)findContours:(UIImage *)img threshold:(CGFloat)threshold &#123; img = [self blurImage:img blur:0.2]; /// 记录原图尺寸 int ow = (int) img.size.width; int w = ow; int h = (int) img.size.height; /// 考虑字节对齐 w 要重新计算 CFDataRef imageData = CGDataProviderCopyData(CGImageGetDataProvider(img.CGImage)); const uint8_t *data = CFDataGetBytePtr(imageData); w = (int) CFDataGetLength(imageData) / (h*4); char *F = malloc((size_t) CFDataGetLength(imageData)/4); /// 二值化处理 threshold *= 255.f; for (int i = 0; i &lt; h; i++) &#123; for (int j = 0; j &lt; w; j++) &#123; if (data[(i * w + j) * 4] &gt; threshold) &#123; F[i * w + j] = 1; &#125; else &#123; F[i * w + j] = 0; &#125; &#125; &#125; NSMutableArray&lt;Contour *&gt; *contours = [NSMutableArray array]; for (int i = 1; i &lt; h - 1; i++) &#123; F[i * w] = 0; F[i * w + w - 1] = 0; &#125; for (int i = 0; i &lt; w; i++) &#123; F[i] = 0; F[w * h - 1 - i] = 0; &#125; int nbd = 1; int lnbd = 1; for (int i = 1; i &lt; h - 1; i++) &#123; lnbd = 1; for (int j = 1; j &lt; w - 1; j++) &#123; int i2 = 0, j2 = 0; if (F[i * w + j] == 0) &#123; continue; &#125; //(a) If fij = 1 and fi, j-1 = 0, then decide that the pixel //(i, j) is the border following starting point of an outer //border, increment NBD, and (i2, j2) &lt;- (i, j - 1). if (F[i * w + j] == 1 &amp;&amp; F[i * w + (j - 1)] == 0) &#123; nbd++; i2 = i; j2 = j - 1; //(b) Else if fij &gt;= 1 and fi,j+1 = 0, then decide that the //pixel (i, j) is the border following starting point of a //hole border, increment NBD, (i2, j2) &lt;- (i, j + 1), and //LNBD + fij in case fij &gt; 1. &#125; else if (F[i * w + j] &gt;= 1 &amp;&amp; F[i * w + j + 1] == 0) &#123; nbd++; i2 = i; j2 = j + 1; if (F[i * w + j] &gt; 1) &#123; lnbd = F[i * w + j]; &#125; &#125; else &#123; //(c) Otherwise, go to (4). //(4) If fij != 1, then LNBD &lt;- |fij| and resume the raster //scan from pixel (i,j+1). The algorithm terminates when the //scan reaches the lower right corner of the picture if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; continue; &#125; //(2) Depending on the types of the newly found border //and the border with the sequential number LNBD //(i.e., the last border met on the current row), //decide the parent of the current border as shown in Table 1. // TABLE 1 // Decision Rule for the Parent Border of the Newly Found Border B // ---------------------------------------------------------------- // Type of border B' // \ with the sequential // \ number LNBD // Type of B \ Outer border Hole border // --------------------------------------------------------------- // Outer border The parent border The border B' // of the border B' // // Hole border The border B' The parent border // of the border B' // ---------------------------------------------------------------- Contour *B = [Contour new]; B.points = [NSMutableArray array]; [B.points addObject:[NSValue valueWithCGPoint:CGPointMake(j * 1.f / ow, i * 1.f / h)]]; B.isHole = (j2 == (j + 1)); B.idx = nbd; [contours addObject:B]; Contour *B0 = [Contour new]; for (int c = 0; c &lt; contours.count; c++) &#123; if (contours[c].idx == lnbd) &#123; B0 = contours[c]; break; &#125; &#125; if (B0.isHole) &#123; if (B.isHole) &#123; B.parentIdx = B0.parentIdx; &#125; else &#123; B.parentIdx = lnbd; &#125; &#125; else &#123; if (B.isHole) &#123; B.parentIdx = lnbd; &#125; else &#123; B.parentIdx = B0.parentIdx; &#125; &#125; //(3) From the starting point (i, j), follow the detected border: //this is done by the following substeps (3.1) through (3.5). //(3.1) Starting from (i2, j2), look around clockwise the pixels //in the neigh- borhood of (i, j) and tind a nonzero pixel. //Let (i1, j1) be the first found nonzero pixel. If no nonzero //pixel is found, assign -NBD to fij and go to (4). int i1j1[2] = &#123;-1, -1&#125;; cwNon0(F, w, h, i, j, i2, j2, 0, i1j1); if (i1j1[0] == -1 &amp;&amp; i1j1[1] == -1) &#123; F[i * w + j] = -nbd; //go to (4) if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; continue; &#125; int i1 = i1j1[0]; int j1 = i1j1[1]; // (3.2) (i2, j2) &lt;- (i1, j1) ad (i3,j3) &lt;- (i, j). i2 = i1; j2 = j1; int i3 = i; int j3 = j; while (true) &#123; //(3.3) Starting from the next elementof the pixel (i2, j2) //in the counterclock- wise order, examine counterclockwise //the pixels in the neighborhood of the current pixel (i3, j3) //to find a nonzero pixel and let the first one be (i4, j4). int i4j4[2] = &#123;-1, -1&#125;; ccwNon0(F, w, h, i3, j3, i2, j2, 1, i4j4); int i4 = i4j4[0]; int j4 = i4j4[1]; [contours[contours.count - 1].points addObject:[NSValue valueWithCGPoint:CGPointMake(j4 * 1.f / ow, i4 * 1.f / h)]]; //(a) If the pixel (i3, j3 + 1) is a O-pixel examined in the //substep (3.3) then fi3, j3 &lt;- -NBD. if (F[i3 * w + j3 + 1] == 0) &#123; F[i3 * w + j3] = (char) -nbd; //(b) If the pixel (i3, j3 + 1) is not a O-pixel examined //in the substep (3.3) and fi3,j3 = 1, then fi3,j3 &lt;- NBD. &#125; else if (F[i3 * w + j3] == 1) &#123; F[i3 * w + j3] = (char) nbd; &#125; else &#123; //(c) Otherwise, do not change fi3, j3. &#125; //(3.5) If (i4, j4) = (i, j) and (i3, j3) = (i1, j1) //(coming back to the starting point), then go to (4); if (i4 == i &amp;&amp; j4 == j &amp;&amp; i3 == i1 &amp;&amp; j3 == j1) &#123; if (F[i * w + j] != 1) &#123; lnbd = ABS(F[i * w + j]); &#125; break; //otherwise, (i2, j2) + (i3, j3),(i3, j3) + (i4, j4), //and go back to (3.3). &#125; else &#123; i2 = i3; j2 = j3; i3 = i4; j3 = j4; &#125; &#125; &#125; &#125; free(F); ...&#125; SDF上面提到边缘检测找连续的边缘点只是为了解决虚线描边，其他的描边情况其实是用不到这些点的，但是这里也遇到问题了。 一开始的想法跟上面通过3x3范围取平均值，通过条件过滤来做的，kernel code大概是这样 12345678910111213141516171819static NSString *KernelString = @&quot;\kernel vec4 borderDraw(sampler image, sampler mask, sampler source, vec4 rgba, float midpoint, float width) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 sumColor = vec4(0.0);\ float radiu = 2 * width;\ for (float m = -radiu; m &lt;= radiu; m += 2) &#123;\ for (float n = -radiu; n &lt;= radiu; n += 2) &#123;\ vec4 rgba = sample(image, samplerTransform(image, vec2(uv.x + m, uv.y + n)));\ sumColor += rgba;\ &#125;\ &#125;\ float avg = sumColor.a / float(radiu * radiu / 2.0);\ if (color.a &lt; 1.0 &amp;&amp; (avg &gt; .05 &amp;&amp; avg &lt; 1.)) &#123;\ return rgba;\ &#125;\ return color;\&#125;&quot;; 这套方案做demo的时候，感觉效果还行，一点点毛疵，以为是条件判断不严谨，以为后续调整下可以解决，还有个严重的问题，就是这个方案计算量太大，图片分辨率1080左右，表现就有点卡，尤其是拖动滑竿调整，描边粗细 、间距 ，实时渲染有明显的卡顿，但是我们又不能降低图片质量，所以这个方案最终也就是停留在demo阶段了。 因为计算量太大，所以想办法降低像素计算量，SDF出场了，通过距离场，可以生成一张图，这张图可以告知像素边界信息，直接通过边界信息，省去了极大的计算量。 SDF ：signed distance filed 有向距离场sdf有两种方式，一种是循环（横向x纵向） 一种是双线性（横向+纵向），很明显前一种计算量远远大于后一种，联调下来第二种方案实际效果也是相当不错了。 这里还要考虑一个问题，因为描边是有粗细跟间距的，所以可以通过调整距离参数生成图，很好的解决了描边粗细跟间距问题的，SDF方案在虚线描边的case也是有用的，通过把SDF生成图拿去做边缘检测找连续点。 来看下SDF生成图的效果 抠图横向SDF结果接纵向SDF结果 贴一下Metal版本的SDF计算逻辑 横向SDF 12345678910111213141516171819202122232425262728293031323334353637383940extern "C" &#123; namespace coreimage &#123; constant float threshold = 0.5; float source(sampler image,float2 uv) &#123; return image.sample(image.transform(uv)).a - threshold; &#125; float4 sdfhor(sampler image,float width,destination dest) &#123; float D2 = width * 2.0 + 1.0; // 获取当前点坐标 float2 uv = dest.coord(); float s = sign(source(image,uv)); float d = 0.; for(int i= 0; i &lt; width; i++) &#123; d ++; float sp = sign(source(image,float2(uv.x + d, uv.y))); if(s * sp &lt; 0.) &#123; break; &#125; sp = sign(source(image,float2(uv.x - d, uv.y))); if(s * sp &lt; 0.) &#123; break; &#125; &#125; float sd = -s * d / D2 ; return float4(float3(sd),1.0); &#125; &#125;&#125; 纵向SDF 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758extern "C" &#123; namespace coreimage &#123; float sd(sampler image,float2 uv,float width) &#123; float D2 = float(width * 2 + 1); float x = image.sample(image.transform(uv)).x; return x * D2; &#125; float4 sdf(sampler image,float width,destination dest) &#123; // 获取当前点坐标 float2 uv = dest.coord(); float dx = sd(image,uv,width); float dMin = abs(dx); float dy = 0.0; for(int i= 0; i &lt; width; i++)&#123; dy += 1.0; float2 offset = float2(0.0, dy); float dx1 = sd(image,uv+offset,width); //sign switch if(dx1 * dx &lt; 0.)&#123; dMin = dy; break; &#125; dMin = min(dMin, length (float2(dx1, dy))); float dx2 = sd(image,uv-offset,width); //sign switch if(dx2 * dx &lt; 0.)&#123; dMin = dy; break; &#125; dMin = min(dMin, length (float2(dx2, dy))); if(dy &gt; dMin)break; &#125; float D2 = float(width * 2 + 1); dMin *= sign(dx); float d = dMin/D2; d = 1.0 - d; d = smoothstep(0.5 ,1.0, d); return float4(float3(d),1.0); &#125; &#125;&#125; border有了距离场，描边的工作就一下子简单多了。 目前实现的五种描边效果就是这样式的 项目中使用coreimage自定义kernel做的，当然也可以metal搞定 sdfsourceKernelString 对应上面第三个效果 12345678910111213141516171819202122232425262728293031static NSString *sdfsourceKernelString = @"\kernel vec4 borderDraw(sampler image, sampler sdf, sampler source, float d1, float d2) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 sdfcolor = sample(sdf, samplerTransform(sdf, uv));\ vec4 sourcecolor = sample(source, samplerTransform(source, uv));\ if (sdfcolor.x &gt;= d1 &amp;&amp; sdfcolor.x &lt;= d2) &#123;\ return sourcecolor;\ &#125;\ return color;\&#125;";static NSString *sdfKernelString = @"\kernel vec4 borderDraw(sampler image, sampler sdf, sampler source, vec4 rgba, vec2 offset, float d1, float d2) \&#123;\ vec2 uv = destCoord();\ vec4 color = sample(image, samplerTransform(image, uv));\ vec4 offsetColor = sample(image, samplerTransform(image, uv-offset));\ vec4 sdfcolor = sample(sdf, samplerTransform(sdf, uv));\ vec4 sourcecolor = sample(source, samplerTransform(source, uv));\ if(offset.x != 0.0 || offset.y != 0.0) &#123;\ if(color.a &lt; 0.5 &amp;&amp; offsetColor.a &gt; 0.5 ) &#123;\ return mix(rgba,color,color.a);\ &#125;\ &#125; else if (sdfcolor.x &gt;= d1 &amp;&amp; sdfcolor.x &lt;= d2) &#123;\ return mix(rgba,sourcecolor,offsetColor.a);\ &#125;\ return color;\&#125;"; 最后一个虚线描边就是常规的连接边缘点安排画布绘制，然后跟抠图做一个合并导出，就不展开说了。 Othter最后还有一些注意点，比如抠图图像是在边缘，则需要考虑下预留描边空间，判断是否有落在边缘，如果有则补充点空间。还有一个就是最小包围盒，抠图很可能只占据原图的一部分预期，为了展示效果，需要把抠图的最小包围盒找到，找这个最小包围盒，不需要那么精确，找出一个差不多的最小矩形框就行，项目上用的就是粗暴的像素遍历，找四个角的位置就可以了，通过最小包围盒，也能判断抠图是否靠近边缘。 The Last综上，关键的几个步骤基本都尝试了多种方式，分析比较得出最合适项目需求的技术方案， 最终从性能、体验等维度拿到相对不错的结果，单纯从描边功能上来说，对比修复工具也不输 O(∩_∩)O哈哈~ referencePContourSDF双线性SDF]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[camera orientation]]></title>
    <url>%2F2022%2F09%2F09%2Fcamera-orientation%2F</url>
    <content type="text"><![CDATA[camera手机相机录视频或者拍照，需要考虑设备的方向，主要在两方面 采集过程中，无论手机什么方向采集，采集画面都是正着看，不用侧头 录制/或者拍照后，视频/图片在正常手持设备的情况下正常显示，不用横着手机 正常手持一般都是竖着拿手机，所以就算横着拍摄，竖着拿手机也需要正常显示，不理解可以参考下系统相机。 所以这里就存在一个方向修正的问题。 device orientation苹果手机，如果设置里锁定了方向，是没办法通过 [UIDevice currentDevice].orientation这个方法拿到的，同样UIDeviceOrientationDidChangeNotification这个通知也获取不到。同理 [[UIApplication sharedApplication] statusBarOrientation]; 跟 **UIApplicationDidChangeStatusBarOrientationNotification** 也是不可用的。 可以通过 CMMotionManager 监听设备的方向 12345678910111213141516171819202122232425262728293031323334_motionManager = [[CMMotionManager alloc] init];_motionManager.deviceMotionUpdateInterval = 1/15.0;if (!_motionManager.deviceMotionAvailable) &#123; _motionManager = nil; return self;&#125;OBJC_WEAK(self)[_motionManager startDeviceMotionUpdatesToQueue:[NSOperationQueue currentQueue] withHandler: ^(CMDeviceMotion*motion, NSError *error)&#123; [weak_self performSelectorOnMainThread:@selector(handleDeviceMotion:) withObject:motion waitUntilDone:YES];&#125;];- (void)handleDeviceMotion:(CMDeviceMotion *)deviceMotion &#123; double x = deviceMotion.gravity.x; double y = deviceMotion.gravity.y; if (fabs(y) &gt;= fabs(x)) &#123; if (y &gt;= 0) &#123; _deviceOrientation = UIDeviceOrientationPortraitUpsideDown; _videoOrientation = AVCaptureVideoOrientationPortraitUpsideDown; &#125; else &#123; _deviceOrientation = UIDeviceOrientationPortrait; _videoOrientation = AVCaptureVideoOrientationPortrait; &#125; &#125; else &#123; if (x &gt;= 0) &#123; _deviceOrientation = UIDeviceOrientationLandscapeRight; _videoOrientation = AVCaptureVideoOrientationLandscapeRight; &#125; else &#123; _deviceOrientation = UIDeviceOrientationLandscapeLeft; _videoOrientation = AVCaptureVideoOrientationLandscapeLeft; &#125; &#125;&#125; capture项目中需要支持设置滤镜、特效等等，所以显示采集的图像需要自己渲染，通过官方的AVCaptureSession配置好采集流程，不管是竖着还是横着采集，获取到的pixelBuffer都是横向的，因为默认就是home键在右边采集图像，但是显示的视图一般都是竖向，所以这里要做一个90°的旋转处理,OPENGL 就是一个旋转矩阵。 123456789101112void main()&#123; // z mat4 rotationMatrix = mat4(cos(angle) , sin(angle) , 0.0, 0.0, -sin(angle) , cos(angle) , 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0); vec4 outPosition = rotationMatrix * position; gl_Position = outPosition; textureCoordinate = inputTextureCoordinate.xy;&#125; 如果只是解决采集画面显示，也可以通过设置connection.videoOrientation = [self currentVideoOrientation]; 来解决项目中没有用这个，因为后面还需要录制视频导出视频文件。 12345678910111213141516171819202122// 当前设备取向- (AVCaptureVideoOrientation)currentVideoOrientation&#123; AVCaptureVideoOrientation orientation; switch (self.motionManager.deviceOrientation) &#123; case UIDeviceOrientationPortrait: orientation = AVCaptureVideoOrientationPortrait; break; case UIDeviceOrientationLandscapeLeft: orientation = AVCaptureVideoOrientationLandscapeRight; break; case UIDeviceOrientationLandscapeRight: orientation = AVCaptureVideoOrientationLandscapeLeft; break; case UIDeviceOrientationPortraitUpsideDown: orientation = AVCaptureVideoOrientationPortraitUpsideDown; break; default: orientation = AVCaptureVideoOrientationPortrait; break; &#125; return orientation;&#125; Record录制过程通过AVAssetWriterInput *writerInput = [AVAssetWriterInput assetWriterInputWithMediaType:obj outputSettings:options]; 和[self.mPixelBufferAdaptor appendPixelBuffer:pixelBuffer withPresentationTime:ts] 来积累pixelBuffer， 为了保障导出的视频可以正常显示，需要记录视频的方向，通过设置writerInput.transform[ = self transformFromCurrentVideoOrientationToOrientation:AVCaptureVideoOrientationPortrait],系统自动帮我们做转换 1234567891011121314151617181920212223242526272829303132// 旋转视频方向函数实现- (CGAffineTransform)transformFromCurrentVideoOrientationToOrientation:(AVCaptureVideoOrientation)orientation &#123; CGFloat orientationAngleOffset = [self angleOffsetFromPortraitOrientationToOrientation:orientation]; CGFloat videoOrientationAngleOffset = [self angleOffsetFromPortraitOrientationToOrientation:self.currentOrientation]; CGFloat angleOffset; if (self.position == AVCaptureDevicePositionBack) &#123; angleOffset = videoOrientationAngleOffset - orientationAngleOffset + M_PI_2; &#125; else &#123; angleOffset = orientationAngleOffset - videoOrientationAngleOffset + M_PI_2; &#125; CGAffineTransform transform = CGAffineTransformMakeRotation(angleOffset); return transform;&#125;- (CGFloat)angleOffsetFromPortraitOrientationToOrientation:(AVCaptureVideoOrientation)orientation &#123; CGFloat angle = 0.0; switch (orientation) &#123; case AVCaptureVideoOrientationPortrait: angle = 0.0; break; case AVCaptureVideoOrientationPortraitUpsideDown: angle = M_PI; break; case AVCaptureVideoOrientationLandscapeRight: angle = -M_PI_2; break; case AVCaptureVideoOrientationLandscapeLeft: angle = M_PI_2; break; &#125; return angle;&#125; 主要就是通过录制视频时候的设备方向跟显示方向计算一下transform Pic拍照生成图片是同样的道理，直接给出方法，主要就是图片方向的旋转 123456789101112131415161718- (UIImageOrientation)getImageRotationOrientationFromCaptureVideoOrientation:(AVCaptureVideoOrientation)orientation &#123; UIImageOrientation imageOrientation; switch (orientation) &#123; case AVCaptureVideoOrientationPortrait: imageOrientation = UIImageOrientationRight; break; case AVCaptureVideoOrientationPortraitUpsideDown: imageOrientation = UIImageOrientationLeft; break; case AVCaptureVideoOrientationLandscapeRight: imageOrientation = UIImageOrientationUp; break; case AVCaptureVideoOrientationLandscapeLeft: imageOrientation = UIImageOrientationDown; break; &#125; return imageOrientation;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455+ (UIImage *)image:(UIImage *)image rotation:(UIImageOrientation)orientation &#123; long double rotate = 0.0; CGRect rect; float translateX = 0; float translateY = 0; float scaleX = 1.0; float scaleY = 1.0; switch (orientation) &#123; case UIImageOrientationLeft: rotate = M_PI_2; rect = CGRectMake(0, 0, image.size.height, image.size.width); translateX = 0; translateY = -rect.size.width; scaleY = rect.size.width/rect.size.height; scaleX = rect.size.height/rect.size.width; break; case UIImageOrientationRight: rotate = -M_PI_2; rect = CGRectMake(0, 0, image.size.height, image.size.width); translateX = -rect.size.height; translateY = 0; scaleY = rect.size.width/rect.size.height; scaleX = rect.size.height/rect.size.width; break; case UIImageOrientationDown: rotate = M_PI; rect = CGRectMake(0, 0, image.size.width, image.size.height); translateX = -rect.size.width; translateY = -rect.size.height; break; default: rotate = 0.0; rect = CGRectMake(0, 0, image.size.width, image.size.height); translateX = 0; translateY = 0; break; &#125; UIGraphicsBeginImageContext(rect.size); CGContextRef context = UIGraphicsGetCurrentContext(); //做CTM变换 CGContextTranslateCTM(context, 0.0, rect.size.height); CGContextScaleCTM(context, 1.0, -1.0); CGContextRotateCTM(context, rotate); CGContextTranslateCTM(context, translateX, translateY); CGContextScaleCTM(context, scaleX, scaleY); //绘制图片 CGContextDrawImage(context, CGRectMake(0, 0, rect.size.width, rect.size.height), image.CGImage); UIImage *newPic = UIGraphicsGetImageFromCurrentImageContext(); return newPic;&#125;]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>camera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[__has_include]]></title>
    <url>%2F2022%2F09%2F06%2Fhas-include%2F</url>
    <content type="text"><![CDATA[VS在搞音视频的过程中，早期为了快速实现功能，会用到一些开源库，比如FFMpeg、OpenCV、LibYUV 等，这些大名鼎鼎的开源库能够解决音视频领域的很多问题，编解码、边缘检测、RGB-YUV格式转换等等。 这几个开源库还都是C/C++的，完美解决跨平台的问题，所以早期可以快速上线，但也会带来增加包大小的问题，这是最明显的，另外开源方案也并非没有BUG。对开源库做裁剪减少大小/提issue等待官方处理、自己处理BUG等这些成本相对也不小。 另一个选择就是使用其他的替代方案：比如原生的技术实现、解决指定问题独立模块算法、DIY \ 编解码 边缘检测 格式转换 开源方案 FFmpeg OpenCV LibYUV 替代方案 VideoToolBox/MediaCodec 边界跟踪算法Suzuki85 DIY 说明 软解-&gt; 硬解 边界跟踪算法Suzuki85 就是 OpenCV 内部的一种边缘检测方案 自己处理像素数据 后期通过替代方案可以解决上面提到的问题，主要是包大小问题。另外也可以自己动手加深了解开源的技术方案 __has_include通过替代方案可以移除开源库，但是接入开源方案实现功能的相关代码就没必要移除了。所以可以通过条件编译做一个区分。正好__has_include这个宏可以满足要求 描述此宏传入一个你想引入文件的名称作为参数，如果该文件能够被引入则返回1，否则返回0。 用法 12345#if __has_include(&lt;XXX/XXX.h&gt;)#import &lt;XXX/XXX.h&gt;#else#import "YYY.h"#endif 项目中的实际用法： 123456789#if __cplusplus &amp;&amp; __has_include(&lt;Libyuv/libyuv.h&gt;)#include &lt;Libyuv/libyuv.h&gt;#endif#if __cplusplus &amp;&amp; __has_include(&lt;Libyuv/libyuv.h&gt;)...#else...#endif 12345#if __has_include(&lt;opencv2/imgcodecs/ios.h&gt;) contourArr = [OpenCVWrapper findContours:image];#else contourArr = [PContour findContours:image threshold:threshold];#endif 12345678910111213141516#if __cplusplus &amp;&amp; __has_include(&lt;ffmpeg/avformat.h&gt;)extern "C" &#123;#include "ffmpeg/timestamp.h"#include "ffmpeg/avformat.h"#include "ffmpeg/bsf.h"#include "ffmpeg/swscale.h"#include "ffmpeg/swresample.h"#include "ffmpeg/avformat.h"#include "ffmpeg/imgutils.h"#include "ffmpeg/samplefmt.h"&#125;#endif ...#endif]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RGB2YUV]]></title>
    <url>%2F2022%2F09%2F05%2FRGB2YUV%2F</url>
    <content type="text"><![CDATA[RGB2YUV处理音视频，对YUV RGB肯定不陌生，这里记录下经过OpenGL处理后的 RGB 格式的pixelBuffer 转成 YUV 格式导出视频。 DIY自己处理矩阵的计算，YUV的存储格式，可以加深对YUV的理解， 类似的反过来处理，或者处理其他格式的YUV 422 444等，也是一样的。所以还是有必要了解一下，DO IT YOURSELF. 重点是注意内存对齐，YUV420采样存储方式就好了，其他没什么复杂的。 转换后 如果出现这种像素错位的情况，一般是内存对齐 导致的补位没有考虑到，参考下代码里面的 stride字段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109CFDictionaryRef CreateCFDictionary(CFTypeRef* keys, CFTypeRef* values, size_t size) &#123; return CFDictionaryCreate(kCFAllocatorDefault, keys, values, size, &amp;kCFTypeDictionaryKeyCallBacks, &amp;kCFTypeDictionaryValueCallBacks); &#125;static void bt709_rgb2yuv8bit_TV(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.183 * R + 0.614 * G + 0.062 * B + 16; U = -0.101 * R - 0.339 * G + 0.439 * B + 128; V = 0.439 * R - 0.399 * G - 0.040 * B + 128; &#125;CVPixelBufferRef RGB2YCbCr8Bit(CVPixelBufferRef pixelBuffer) &#123; CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); int stride = (int) CVPixelBufferGetBytesPerRow(pixelBuffer) / 4; OSType pixelFormat = kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 1; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey, &#125;; CFDictionaryRef io_surface_value = CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;; CFDictionaryRef attributes = CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 0); size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 1); int plane_h1 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 0); int plane_h2 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 1); uint8_t *y = (uint8_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 0); memset(y, 0x80, plane_h1 * y_stride); uint8_t *uv = (uint8_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 1); memset(uv, 0x80, plane_h2 * uv_stride); int y_bufferSize = w * h; int uv_bufferSize = w * h / 4; uint8_t *y_planeData = (uint8_t *) malloc(y_bufferSize * sizeof(uint8_t)); uint8_t *u_planeData = (uint8_t *) malloc(uv_bufferSize * sizeof(uint8_t)); uint8_t *v_planeData = (uint8_t *) malloc(uv_bufferSize * sizeof(uint8_t)); int u_offset = 0; int v_offset = 0; uint8_t R, G, B; uint8_t Y, U, V; for (int i = 0; i &lt; h; i ++) &#123; for (int j = 0; j &lt; w; j ++) &#123; int offset = i * stride + j; B = baseAddress[offset * 4]; G = baseAddress[offset * 4 + 1]; R = baseAddress[offset * 4 + 2]; bt709_rgb2yuv8bit_TV(R, G, B, Y, U, V); y_planeData[i * w + j] = Y; //隔行扫描 偶数行的偶数列取U 奇数行的偶数列取V if (j % 2 == 0) &#123; (i % 2 == 0) ? u_planeData[u_offset++] = U : v_planeData[v_offset++] = V; &#125; &#125; &#125; for (int i = 0; i &lt; plane_h1; i ++) &#123; memcpy(y + i * y_stride, y_planeData + i * w, w); if (i &lt; plane_h2) &#123; for (int j = 0 ; j &lt; w ; j+=2) &#123; //NV12 和 NV21 格式都属于 YUV420SP 类型。它也是先存储了 Y 分量，但接下来并不是再存储所有的 U 或者 V 分量，而是把 UV 分量交替连续存储。 //NV12 是 IOS 中有的模式，它的存储顺序是先存 Y 分量，再 UV 进行交替存储。 memcpy(uv + i * y_stride + j, u_planeData + i * w/2 + j/2, 1); memcpy(uv + i * y_stride + j + 1, v_planeData + i * w/2 + j/2, 1); &#125; &#125; &#125; free(y_planeData); free(u_planeData); free(v_planeData); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); return pixelBufferCopy; &#125; LibYUV看过了上一种方式，LibYUV就更好理解了，这里主要通过pod 依赖下 LibYUV-ios， 就不自己编译了。 pod &#39;Libyuv&#39;,&#39;1703&#39; LibYUV 不能直接RGB转成NV12 ,需要通过I420过度下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); size_t bgraStride = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer,0); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); OSType pixelFormat = kCVPixelFormatType_420YpCbCr8Planar; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 1; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey &#125;; CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;; CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); unsigned char* y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,0); unsigned char* u = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,1); unsigned char* v = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy,2); int32_t width = (int32_t)CVPixelBufferGetWidth(pixelBufferCopy); int32_t height = (int32_t)CVPixelBufferGetHeight(pixelBufferCopy); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,0); size_t u_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,1); size_t v_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy,2); libyuv::ARGBToI420(baseAddress, (int)bgraStride, y, (int)y_stride, u, (int)u_stride, v, (int)v_stride, width, height); CVPixelBufferRef pixelBufferNV12 = NULL; const size_t size = 1; CFTypeRef _keys[size] = &#123; kCVPixelBufferIOSurfacePropertiesKey &#125;; CFDictionaryRef _io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef _values[size] = &#123;_io_surface_value&#125;; CFDictionaryRef _attributes = vtc::CreateCFDictionary(_keys, _values, size); CVReturn _status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, _attributes, &amp;pixelBufferNV12); if (_status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (_attributes) &#123; CFRelease(_attributes); _attributes = nullptr; &#125; CVPixelBufferLockBaseAddress(pixelBufferNV12, 0); unsigned char* _y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,0); unsigned char* _uv = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,1); size_t _y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 0); size_t _uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 1); int32_t _width = (int32_t)CVPixelBufferGetWidth(pixelBufferNV12); int32_t _height = (int32_t)CVPixelBufferGetHeight(pixelBufferNV12); libyuv::I420ToNV12(y, (int)y_stride, u, (int)u_stride, v, (int)v_stride, _y, (int)_y_stride, _uv, (int)_uv_stride, _width, _height); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); CVPixelBufferUnlockBaseAddress(pixelBufferNV12, 0); CVPixelBufferRelease(pixelBufferCopy); RGB-&gt;NV21更新下:上次使用libyuv的时候 看漏了，其实有argb转nv21的 123456789101112131415161718192021222324252627282930313233343536373839404142CVPixelBufferLockBaseAddress(pixelBuffer, 0);uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer);size_t bgraStride = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer,0);int w = (int) CVPixelBufferGetWidth(pixelBuffer);int h = (int) CVPixelBufferGetHeight(pixelBuffer);CVPixelBufferRef pixelBufferNV12 = NULL;const size_t attributes_size = 1;CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey&#125;;CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0);CFTypeRef values[attributes_size] = &#123;io_surface_value&#125;;CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size);CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, attributes, &amp;pixelBufferNV12);if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr;&#125;if (attributes) &#123; CFRelease(attributes); attributes = nullptr;&#125;CVPixelBufferLockBaseAddress(pixelBufferNV12, 0);unsigned char* y = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,0);unsigned char* uv = (unsigned char*)CVPixelBufferGetBaseAddressOfPlane(pixelBufferNV12,1);size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 0);size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferNV12, 1);int32_t width = (int32_t)CVPixelBufferGetWidth(pixelBufferNV12);int32_t height = (int32_t)CVPixelBufferGetHeight(pixelBufferNV12);// ARGB-&gt;NV21libyuv::ARGBToNV12(baseAddress, (int)bgraStride, y, (int)y_stride, uv, (int)uv_stride, width, height)CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);CVPixelBufferUnlockBaseAddress(pixelBufferNV12, 0);return pixelBufferNV12;]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>media</tag>
        <tag>YUV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter Display P3]]></title>
    <url>%2F2022%2F08%2F31%2Fflutter-Display-P3%2F</url>
    <content type="text"><![CDATA[Display P3Display P3 是苹果手机相机使用的一种色域，查看图片信息可以看到， 然而在flutter中 渲染 Display P3格式的图片 bitmap颜色失真,不了解色域的参考下前面HDR相关的文章 失真效果： 修复效果： 原因就是Flutter 直接把Display P3社区的当做sRGB色域的图像处理了，而没有做色域转换 FlutterFlutter 跟 Native 图片打通，常见有两种方式：bitmap传递 &amp;&amp; 外接纹理，这边文章针对的是前者，外接纹理也会有这种问题的，无非就是pixelBuffer 转纹理，pixelBuffer一样也是有色域问题的，解决方案可以参考 Flutter HDR,原理是一样的。 针对图片的bitmap做色域转换，方案有很多，这里列出常见的两种： ImageIO123456789101112131415161718192021222324252627282930CGImageSourceRef src = CGImageSourceCreateWithData((__bridge CFDataRef) imageData, NULL); NSUInteger frameCount = CGImageSourceGetCount(src); if (frameCount &gt; 0) &#123; NSDictionary *options = @&#123;(__bridge NSString *)kCGImageSourceShouldCache : @YES, (__bridge NSString *)kCGImageSourceShouldCacheImmediately : @NO &#125;; NSDictionary *props = (NSDictionary *) CFBridgingRelease(CGImageSourceCopyPropertiesAtIndex(src, (size_t) 0, (__bridge CFDictionaryRef)options)); NSString *profileName = [props objectForKey:(NSString *) kCGImagePropertyProfileName]; if ([profileName isEqualToString:@"Display P3"]) &#123; NSMutableData *data = [NSMutableData data]; CGImageDestinationRef destRef = CGImageDestinationCreateWithData((__bridge CFMutableDataRef)data, kUTTypePNG, 1, NULL); NSMutableDictionary *properties = [NSMutableDictionary dictionary]; properties[(__bridge NSString *)kCGImageDestinationLossyCompressionQuality] = @(1); properties[(__bridge NSString *)kCGImageDestinationEmbedThumbnail] = @(0); properties[(__bridge NSString *)kCGImagePropertyNamedColorSpace] = (__bridge id _Nullable)(kCGColorSpaceSRGB); properties[(__bridge NSString *)kCGImageDestinationOptimizeColorForSharing] = @(YES); CGImageDestinationAddImageFromSource(destRef, src, 0, (__bridge CFDictionaryRef)properties); CGImageDestinationFinalize(destRef); CFRelease(destRef); return data; &#125; &#125; return imageData; 核心就是这个属性，很好理解吧 12345/* Create an image using a colorspace, that has is compatible with older devices * The value should be kCFBooleanTrue or kCFBooleanFalse * Defaults to kCFBooleanFalse = don&apos;t do any color conversion */IMAGEIO_EXTERN const CFStringRef kCGImageDestinationOptimizeColorForSharing IMAGEIO_AVAILABLE_STARTING(10.12, 9.3); 重新Render一张图1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980CGImageSourceRef src = CGImageSourceCreateWithData((__bridge CFDataRef) imageData, NULL); NSUInteger frameCount = CGImageSourceGetCount(src); if (frameCount &gt; 0) &#123; NSDictionary *frameProperties = (NSDictionary *) CFBridgingRelease(CGImageSourceCopyPropertiesAtIndex(src, (size_t) 0, NULL)); NSString *profileName = [frameProperties objectForKey:(NSString *) kCGImagePropertyProfileName]; if ([profileName isEqualToString:@"Display P3"]) &#123; CGImageRef imageRef = CGImageSourceCreateImageAtIndex(src, (size_t) 0, NULL); CIImage *image = [CIImage imageWithCGImage:imageRef]; CIContext *context = [[CIContext alloc] init]; float w = image.extent.size.width; float h = image.extent.size.height; unsigned char *bitmap = malloc(w * h * 4); CIRenderDestination *destination = [[CIRenderDestination alloc] initWithBitmapData:bitmap width:w height:h bytesPerRow:w * 4 format:kCIFormatBGRA8]; NSError *error = nil; [context startTaskToRender:image toDestination:destination error:&amp;error]; if (error) &#123; CFRelease(src); return imageData; &#125; CFRelease(src); UIImage *newImage = [FlutterImagePlugin imageFromBRGABytes:bitmap imageSize:image.extent.size]; free(bitmap); CGImageRelease(imageRef); if (newImage == nil) &#123; return imageData; &#125; return UIImagePNGRepresentation(newImage); &#125; &#125;+ (UIImage *)imageFromBRGABytes:(unsigned char *)imageBytes imageSize:(CGSize)imageSize &#123; CGImageRef imageRef = [self imageRefFromBGRABytes:imageBytes imageSize:imageSize]; if (imageRef == NULL) &#123; return nil; &#125; UIImage *image = [UIImage imageWithCGImage:imageRef]; CGImageRelease(imageRef); return image;&#125;+ (CGImageRef)imageRefFromBGRABytes:(unsigned char *)imageBytes imageSize:(CGSize)imageSize &#123; CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB(); CGContextRef context = CGBitmapContextCreate(imageBytes, imageSize.width, imageSize.height, 8, imageSize.width * 4, colorSpaceSDImage, kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst); if (context == NULL) &#123; return NULL; &#125; CGImageRef imageRef = CGBitmapContextCreateImage(context); if (imageRef == NULL) &#123; CGContextRelease(context); return NULL; &#125; CGContextRelease(context); return imageRef;&#125; 从代码量跟性能考量，无脑选前者 😝]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter-hdr]]></title>
    <url>%2F2022%2F08%2F24%2Fflutter-hdr%2F</url>
    <content type="text"><![CDATA[flutterflutter 播放 HDR 视频，色彩跟亮度都有问题 ，github 也有反馈这个issue Video Player HDR Problem 跟原生的对比可以很明显看到差距，官方看起来好像也不重视这个问题 😕 关于 HDR 格式，可以看下前面的文章 HDR笔记 这里记录下Flutter video player plugin 中处理视频色彩的方法，亮度提升需要通过硬件激活，Flutter中好像没法处理。 颜色的处理核心就是做了一个HDR-&gt;SDR的tonemap,恰好CIImage中提供了这样的Filter，处理就方便多了，看过HDR笔记 这篇文章的话，应该也可以自己通过 EOTF + 色域映射 来处理。暂时不知道 CIImage中是怎么处理的，猜测是差不多的。 12345678910111213141516171819202122232425262728293031323334353637383940- (CVPixelBufferRef)pixelBufferFormCIImage:(CIImage *)image &#123; NSDictionary *options = [NSDictionary dictionaryWithObjectsAndKeys: @&#123;&#125;, kCVPixelBufferIOSurfacePropertiesKey, @YES, kCVPixelBufferCGImageCompatibilityKey, @YES, kCVPixelBufferCGBitmapContextCompatibilityKey, nil]; CVPixelBufferRef pixelBufferCopy = NULL; CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, image.extent.size.width, image.extent.size.height, kCVPixelFormatType_32BGRA, (__bridge CFDictionaryRef) options, &amp;pixelBufferCopy); if (status == kCVReturnSuccess) &#123; CIRenderDestination *destination = [[CIRenderDestination alloc] initWithPixelBuffer:pixelBufferCopy]; [self.mContext startTaskToRender:image toDestination:destination error:nil]; &#125; return pixelBufferCopy;&#125;- (CVPixelBufferRef)copyPixelBuffer &#123; CMTime outputItemTime = [_videoOutput itemTimeForHostTime:CACurrentMediaTime()]; if ([_videoOutput hasNewPixelBufferForItemTime:outputItemTime]) &#123; CVPixelBufferRef p = [_videoOutput copyPixelBufferForItemTime:outputItemTime itemTimeForDisplay:NULL]; CFTypeRef colorPrimaries = CVBufferGetAttachment(p, kCVImageBufferTransferFunctionKey, NULL); if (colorPrimaries &amp;&amp; CFEqual(colorPrimaries, kCVImageBufferTransferFunction_ITU_R_2100_HLG)) &#123; if (@available(iOS 14.1, *)) &#123; CIImage *image = [CIImage imageWithCVPixelBuffer:p options:@&#123;kCIImageToneMapHDRtoSDR : @(YES)&#125;]; CVPixelBufferRef newP = [self pixelBufferFormCIImage:image]; CVPixelBufferRelease(p); return newP; &#125; &#125; return p; &#125; else &#123; return NULL; &#125;&#125;]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>HDR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDR笔记]]></title>
    <url>%2F2022%2F08%2F18%2FHDR%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Noun explanationITU：国际电信联盟 International Telecommunication UnionITU-R：国际电信联盟无线电通信部门 ITU Radiocommunication Sector CIE :国际照明协会 （英文：International Commission on Illumination ，法文：Commission internationale de l&#39;éclairage ，采用法文缩写：CIE ） SMPTE：电影电视工程师协会 Society of Motion Picture and Television Engineers Hue：色调 色彩 色相Chroma：色调饱和度 浓度Luminance：亮度 明度 HDR ConceptHDR:High Dynamic Range 字面上是动态范围，一般指亮度上可以表达更大的亮度范围，呈现更大的亮度对比度。但是实际实际上HDR的技术和标准涉及色彩相关的一组属性的改善，可以带来更多的颜色、更大的亮度对比度、更高精度的量化。 OETF/EOTF: Optical-Electro/Electro-Optical Transfer Function 光电/电光转换函数 人对亮度的感知是非线性的，对暗部细节敏感，对亮部细节不敏感，利用这个特点设计了非线性的光电转换和电光转换的函数。这样的处理不仅可以节省带宽，也可以基本满足用户体验需求。光电转换的时候做了特殊的非线性编码，为暗部细节分配更多的码率，亮部细节进行了压缩或者截断减少码率的分配。电光转换进行显示还原的时候，通过应用一个逆的非线性变化，还原出线性光。 Info通过mediainfo 查看一个HDR视频信息 Video ID 1 Format HEVC Format/Info High Efficiency Video Coding Format profile Main 10@L5@High Codec ID hvc1 Codec ID/Info High Efficiency Video Coding Duration 14 s 0 ms Bit rate 123 kb/s Width 3 840 pixels Height 2 160 pixels Display aspect ratio 16:9 Frame rate mode Constant Frame rate 30.000 FPS Color space YUV Chroma subsampling 4:2:0 Bit depth 10 bits Scan type Progressive Bits/(PixelFrame)* 0.000 Stream size 211 KiB (98%) Title Core Media Video Encoded date UTC 2020-06-14 21:45:40 Tagged date UTC 2020-06-14 21:47:33 Color range Limited Color primaries BT.2020 Transfer characteristics HLG Matrix coefficients BT.2020 non-constant Codec configuration box hvcC 下面几个参数属于元数据 ，可能没有 后文会讲到 Video Mastering display color primaries Display P3 / R: x=0.677980 y=0.321980, G: x=0.245000 y=0.703000, B: x=0.137980 y=0.052000, White point: x=0.312680 y=0.328980 Mastering display luminance min: 0.0001 cd/m2, max: 1000 cd/m2 Maximum Content Light Level 1000 cd/m2 Maximum Frame-Average Light Level 400 cd/m2 重点关注 ： Color range ：色彩范围 Color primaries ：色彩原色 Transfer characteristics ：传输特性 Matrix coefficients ：矩阵系数 元数据字段 Mastering display color primaries Mastering display luminance Maximum Content Light Level Maximum Frame-Average Light Level 部分参数可以通过ffprobe查看对应的选项 ffprobe -h &gt;&gt; ffprobe.txt Color range色彩范围主要是两个： Full range （PC range ） Video range（limited range，tv range） Full Range 就是我们所熟悉的 [0, 255]，而在 Limited Range 中，Y’ 的值被限制在 [16, 235]，Cb 和 Cr 的值被限制在 [16, 240] （针对8bit的）。 HDR一个重要属性就是量化精度。 SDR技术使用8bit进行颜色的表达，而HDR使用10bit/12bit进行颜色的表示，从而减少了8bit容易出现的人为条带效应。 ios 中的精度 &amp; 色彩范围 ： 123kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange = &apos;420v&apos;, /* Bi-Planar Component Y&apos;CbCr 8-bit 4:2:0, video-range (luma=[16,235] chroma=[16,240]). baseAddr points to a big-endian CVPlanarPixelBufferInfo_YCbCrBiPlanar struct */kCVPixelFormatType_420YpCbCr10BiPlanarVideoRange = &apos;x420&apos;, /* 2 plane YCbCr10 4:2:0, each 10 bits in the MSBs of 16bits, video-range (luma=[64,940] chroma=[64,960]) */ 123kCVPixelFormatType_420YpCbCr8BiPlanarFullRange = &apos;420f&apos;, /* Bi-Planar Component Y&apos;CbCr 8-bit 4:2:0, full-range (luma=[0,255] chroma=[1,255]). baseAddr points to a big-endian CVPlanarPixelBufferInfo_YCbCrBiPlanar struct */ kCVPixelFormatType_420YpCbCr10BiPlanarFullRange = &apos;xf20&apos;, /* 2 plane YCbCr10 4:2:0, each 10 bits in the MSBs of 16bits, full-range (Y range 0-1023) */ 关于为什么要将YUV量化为tv range 16-235 ？ 以下是维基百科摘抄的一段， 意思是tv range是为了解决滤波（模数转换）后的过冲现象， Y′ values are conventionally shifted and scaled to the range [16, 235] (referred to as studio swing or “TV levels”) rather than using the full range of [0, 255] (referred to as full swing or “PC levels”). This practice was standardized in SMPTE-125M in order to accommodate signal overshoots (“ringing”) due to filtering. The value 235 accommodates a maximal black-to-white overshoot of 255 − 235 = 20, or 20 / (235 − 16) = 9.1%, which is slightly larger than the theoretical maximal overshoot (Gibbs phenomenon) of about 8.9% of the maximal step. The toe-room is smaller, allowing only 16 / 219 = 7.3% overshoot, which is less than the theoretical maximal overshoot of 8.9%. This is why 16 is added to Y′ and why the Y′ coefficients in the basic transform sum to 220 instead of 255.^[9]^ U and V values, which may be positive or negative, are summed with 128 to make them always positive, giving a studio range of 16–240 for U and V. (These ranges are important in video editing and production, since using the wrong range will result either in an image with “clipped” blacks and whites, or a low-contrast image.) Color primaries一般理解为色域，色域指可以显示的所有颜色的范围，常见的有Rec.709（全高清广播标准）、Rec.2020（4K/8K广播标准BT.2020）、Adobe RGB、P3等。 bt709 unknown reserved bt470m bt470bg smpte170m smpte240m film bt2020 smpte428 smpte431 smpte432 下图显示了人眼能够感知的所有RGB值的范围。三角形表示色域：三角形越大，可以显示的颜色越多。 这张马蹄形的图上面可以看到HDR使用的色域是BT2020， SDR使用的是BT709，可以明显看到HDR的色域大于SDR。 理解颜色空间： 颜色空间 由 颜色模型 跟 色域 共同定义。颜色模型的概念为：一种抽象数学模型，通过一组数字来描述颜色（例如RGB使用三元组、CMYK使用四元组）例如Adobe RGB和sRGB都基于RGB颜色模型，但它们是两个不同的颜色空间，因为色域不一样 Color Transfer描述光电转换过程的视频属性也叫颜色传输函数Color Transfer 就是上面表格里面的 Transfer characteristics 常见的hdr转换曲线为HLG和PQ，其中，smpte2084为PQ曲线（感知量化），arib-std-b67为HLG曲线（混合对数伽玛） bt709 unknown reserved bt470m bt470bg smpte170m smpte240m linear log100 log316 iec61966-2-4 bt1361e iec61966-2-1 bt2020-10 bt2020-12 smpte2084 smpte428 arib-std-b67 传统的SDR视频使用的BT709的光电转换函数，对高亮部分进行了截断，可以表达的亮度动态范围有限，最大亮度只有100nit。而HDR视频，增加了高亮部分细节的表达，很大的扩展亮度的动态范围。 不同HDR的设计初衷不同，其中PQ的设计更接近人眼的特点，亮度表达更准确，可以表示高达10000nit的亮度。而HLG的设计考虑了老设备的兼容性，和传统bt709的传输函数有部分是重合的，天然的对老设备具有一定兼容性 MetadataHDR元数据分为两种，静态元数据 和动态元数据 ； 使用PQ曲线的HDR10是采用静态元数据的，但是杜比公司提出来的杜比视界和三星的HDR10+，尽管使用了PQ曲线，但是他们使用的是动态元数据，HLG没有元数据。 其中 DolbyVision 等价于SMPTE ST 2094-10, HDR10+ 等价于 SMPTE ST 2094-40 静态元数据规定了整个片子像素级别最大亮度上限，在ST 2086中有标准化的定义。静态元数据的缺点是必须做全局的色调映射，没有足够的调节空间，兼容性不好。 动态元数据可以很好地解决这个问题。动态元数据主要有两个方面的作用：与静态元数据相比，它可以在每一个场景或者每一帧画面，给调色师一个发挥的空间，以展现更丰富的细节；另一个方面，通过动态元数据，在目标显示亮度上做色调映射，可以最大程度在目标显示器上呈现作者的创作意图。 最大内容亮度（MaxCLL）：整个视频流中最亮像素的亮度。 最大帧平均亮度（MaxFALL）：整个视频流中最亮帧的平均亮度 另外解释一下多出现的几个参数:progressive,SAR,DAR. progressive,其实就是扫描方式,逐行扫描.另外的一种方式就是隔行扫描:interlaced.我们平时所谓的1080p,这个p就是progressive,表示的是1080尺寸的逐行扫描视频.DAR - display aspect ratio就是视频播放时，我们看到的图像宽高的比例，缩放视频也要按这个比例来，否则会使图像看起来被压扁或者拉长了似的。 SAR - storage aspect ratio就是对图像采集时，横向采集与纵向采集构成的点阵，横向点数与纵向点数的比值。比如VGA图像640/480 = 4:3，D-1 PAL图像720/576 = 5:4 PAR - pixel aspect ratio大多数情况为1:1,就是一个正方形像素，否则为长方形像素这三者的关系PAR x SAR = DAR或者PAR = DAR/SAR Tone Mapping色调映射的目的是使高动态范围HDR图像能够适应低动态范围LDR显示器。 色调映射算法的目的在于将HDR图像的亮度进行压缩，进而映射到LDR显示设备的显示范围之内，同时，在映射的过程中要尽量保持原HDR图像的细节与颜色等重要信息。 所以色调映射算法需要具有两方面的性质： 能够将图像亮度进行压缩。 能够保持图像细节与颜色。 EOTF/OETF HLG 1234567891011121314151617181920212223242526272829float ARIB_B67_A = 0.17883277;float ARIB_B67_B = 0.28466892;float ARIB_B67_C = 0.55991073;highp float arib_b67_inverse_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt;= (1.0/2.0)) x = (x * x) * (1.0 / 3.0); else x = (exp((x - ARIB_B67_C) / ARIB_B67_A) + ARIB_B67_B) / 12.0; return x;&#125;highp float arib_b67_ootf(highp float x)&#123; return x &lt; 0.0 ? x : pow(x, 1.2);&#125;highp float arib_b67_eotf(highp float x)&#123; return arib_b67_ootf(arib_b67_inverse_oetf(x));&#125;highp float arib_b67_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt;= (1.0 / 12.0)) x = sqrt(3.0 * x); else x = ARIB_B67_A * log(12.0 * x - ARIB_B67_B) + ARIB_B67_C; return x;&#125; PQ 12345678910111213highp float ST2084_M1 = 0.1593017578125;const float ST2084_M2 = 78.84375;const float ST2084_C1 = 0.8359375;const float ST2084_C2 = 18.8515625;const float ST2084_C3 = 18.6875;highp float FLT_MIN = 1.17549435082228750797e-38;highp float st_2084_eotf(highp float x)&#123; highp float xpow = pow(x, float(1.0 / ST2084_M2)); highp float num = max(xpow - ST2084_C1, 0.0); highp float den = max(ST2084_C2 - ST2084_C3 * xpow, FLT_MIN); return pow(num/den, 1.0 / ST2084_M1);&#125; BT.709 1234567891011const float REC709_ALPHA = 1.09929682680944;const float REC709_BETA = 0.018053968510807;highp float rec_709_oetf(highp float x)&#123; x = max(x, 0.0); if (x &lt; REC709_BETA ) x = x * 4.5; else x = REC709_ALPHA * pow(x, 0.45) - (REC709_ALPHA - 1.0); return x;&#125; bt2020 -&gt; bt709 1231.6605, -0.5876, -0.0728-0.1246, 1.1329, -0.0083-0.0182, -0.1006, 1.1187 bt709 -&gt; bt2020 1230.6274, 0.3293, 0.04330.0691, 0.9195, 0.01140.0164, 0.0880, 0.8956 Matrix coefficientsYCbCr-&gt;RGB Video Range 1234567891011121314151617181920// BT.601, which is the standard for SDTV.static const GLfloat kColorConversion601[] = &#123; 1.164, 1.164, 1.164, 0.0, -0.392, 2.017, 1.596, -0.813, 0.0,&#125;;// BT.709, which is the standard for HDTV.static const GLfloat kColorConversion709[] = &#123; 1.164, 1.164, 1.164, 0.0, -0.213, 2.112, 1.793, -0.533, 0.0,&#125;;// BT.2020 (which is the standard for UHDTV, ITU_R_2020 )static const GLfloat kColorConversion2020[] = &#123; 1.1644f, 1.1644f, 1.1644f, 0.0f, -0.1881, 2.1501, 1.6853, -0.6529, 0.0f,&#125;; Full Range 1234567891011121314151617181920// BT.601 full range static const GLfloat kColorConversion601FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.343, 1.765,1.4, -0.711, 0.0,&#125;;// BT.709 full range static const GLfloat kColorConversion709FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.187, 1.855,1.574, -0.468, 0.0,&#125;;// BT.2020 full range static const GLfloat kColorConversion2020FullRange[] = &#123;1.0, 1.0, 1.0,0.0, -0.1645, 1.8814,1.4746, -0.5713, 0.0,&#125;; OpenGL Video Range 1234567891011121314151617precision mediump float; varying mediump vec2 textureCoordinate; uniform sampler2D luminanceTexture; uniform sampler2D chrominanceTexture; uniform highp mat3 colorConversionMatrix; void main() &#123; mediump vec3 yuv; highp vec3 rgb; yuv.x = texture2D(luminanceTexture, textureCoordinate).r - (16.0/255.0); yuv.yz = texture2D(chrominanceTexture, textureCoordinate).ra - vec2(0.5, 0.5); rgb = colorConversionMatrix * yuv; gl_FragColor = vec4(rgb, 1.); &#125; Full Range 12345678910111213141516precision mediump float; varying mediump vec2 textureCoordinate; uniform sampler2D luminanceTexture; uniform sampler2D chrominanceTexture; uniform highp mat3 colorConversionMatrix; void main() &#123; mediump vec3 yuv; highp vec3 rgb; yuv.x = texture2D(luminanceTexture, textureCoordinate).r; yuv.yz = texture2D(chrominanceTexture, textureCoordinate).ra - vec2(0.5, 0.5); rgb = colorConversionMatrix * yuv; gl_FragColor = vec4(rgb, 1.); &#125; RGB-YUV123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687// FULL RANGE static void bt2020_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2627 * R + 0.6780 * G + 0.0593 * B; U = -0.1396 * R - 0.3604 * G + 0.5000 * B + 128; V = 0.5000 * R - 0.4598 * G - 0.0402 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt2020_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2256 * R + 0.5823 * G + 0.0509 * B + 16; U = -0.1222 * R - 0.3154 * G + 0.4375 * B + 128; V = 0.4375 * R - 0.4023 * G - 0.0352 * B + 128; /// 8 分解为2 + 6 /// &lt;&lt;2 ：对应 把 yuv 从 (luma=[16,235] chroma=[16,240]) 拉到 (luma=[64,940] chroma=[64,960]) /// &lt;&lt;6 ：10bit 按字节为单位需要两个字节，因为按照大端模式存储（低地址到高地址的顺序存放数据的高位字节到低位字节）后面6个bit是padding 补0 Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt709_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.2126 * R + 0.7152 * G + 0.0722 * B; U = -0.1146 * R - 0.3854 * G + 0.5000 * B + 128; V = 0.5000 * R - 0.4542 * G - 0.0458 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt709_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.183 * R + 0.614 * G + 0.062 * B + 16; U = -0.101 * R - 0.339 * G + 0.439 * B + 128; V = 0.439 * R - 0.399 * G - 0.040 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt601_rgb2yuv10bit_PC(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.169 * R - 0.331 * G + 0.500 * B + 128; V = 0.500 * R - 0.419 * G - 0.081 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // VIDEO RANGE static void bt601_rgb2yuv10bit_TV(uint8_t R, uint8_t G, uint8_t B, uint16_t &amp;Y, uint16_t &amp;U, uint16_t &amp;V) &#123; Y = 0.257 * R + 0.504 * G + 0.098 * B + 16; U = -0.148 * R - 0.291 * G + 0.439 * B + 128; V = 0.439 * R - 0.368 * G - 0.071 * B + 128; Y = Y &lt;&lt; 8; U = U &lt;&lt; 8; V = V &lt;&lt; 8; &#125; // FULL RANGE static void bt601_rgb2yuv_PC(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.169 * R - 0.331 * G + 0.500 * B + 128; V = 0.500 * R - 0.419 * G - 0.081 * B + 128; &#125; // VIDEO RANGE static void bt601_rgb2yuv_TV(uint8_t R, uint8_t G, uint8_t B, uint8_t &amp;Y, uint8_t &amp;U, uint8_t &amp;V) &#123; Y = 0.257 * R + 0.504 * G + 0.098 * B + 16; U = -0.148 * R - 0.291 * G + 0.439 * B + 128; V = 0.439 * R - 0.368 * G - 0.071 * B + 128; &#125; 12345678910111213141516171819202122inline CFDictionaryRef HDRAttachmentsOfMedium(void) &#123; const size_t attributes_size = 6; CFTypeRef keys[attributes_size] = &#123; kCVImageBufferFieldCountKey, kCVImageBufferChromaLocationBottomFieldKey, kCVImageBufferChromaLocationTopFieldKey, kCVImageBufferTransferFunctionKey, kCVImageBufferColorPrimariesKey, kCVImageBufferYCbCrMatrixKey &#125;; CFTypeRef values[attributes_size] = &#123; kCFBooleanTrue, kCVImageBufferChromaLocation_Left, kCVImageBufferChromaLocation_Left, kCVImageBufferTransferFunction_ITU_R_2100_HLG, kCVImageBufferColorPrimaries_ITU_R_2020, kCVImageBufferYCbCrMatrix_ITU_R_2020 &#125;; return CreateCFDictionary(keys, values, attributes_size); &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495CVPixelBufferRef RGB2YCbCr10Bit(CVPixelBufferRef pixelBuffer, CFDictionaryRef dic) &#123; CVPixelBufferLockBaseAddress(pixelBuffer, 0); uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(pixelBuffer); int w = (int) CVPixelBufferGetWidth(pixelBuffer); int h = (int) CVPixelBufferGetHeight(pixelBuffer); //int stride = (int) CVPixelBufferGetBytesPerRow(pixelBuffer) / 4; OSType pixelFormat = kCVPixelFormatType_420YpCbCr10BiPlanarVideoRange; CVPixelBufferRef pixelBufferCopy = NULL; const size_t attributes_size = 5; CFTypeRef keys[attributes_size] = &#123; kCVPixelBufferIOSurfacePropertiesKey, kCVPixelBufferExtendedPixelsBottomKey, kCVPixelBufferExtendedPixelsTopKey, kCVPixelBufferExtendedPixelsRightKey, kCVPixelBufferExtendedPixelsLeftKey &#125;; CFDictionaryRef io_surface_value = vtc::CreateCFDictionary(nullptr, nullptr, 0); CFTypeRef values[attributes_size] = &#123;io_surface_value, kCFBooleanFalse, kCFBooleanFalse, kCFBooleanFalse, kCFBooleanFalse&#125;; CFDictionaryRef attributes = vtc::CreateCFDictionary(keys, values, attributes_size); CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, w, h, pixelFormat, attributes, &amp;pixelBufferCopy); if (status != kCVReturnSuccess) &#123; std::cout &lt;&lt; "YUVBufferCopyWithPixelBuffer :: failed" &lt;&lt; std::endl; return nullptr; &#125; if (attributes) &#123; CFRelease(attributes); attributes = nullptr; &#125; int plane_h1 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 0); int plane_h2 = (int) CVPixelBufferGetHeightOfPlane(pixelBufferCopy, 1); CVPixelBufferLockBaseAddress(pixelBufferCopy, 0); if (dic == nullptr || CFDictionaryGetCount(dic) &lt;= 0) &#123; dic = HDRAttachmentsOfMedium(); &#125; CVBufferSetAttachments(pixelBufferCopy, dic, kCVAttachmentMode_ShouldPropagate); size_t y_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 0); //size_t uv_stride = CVPixelBufferGetBytesPerRowOfPlane(pixelBufferCopy, 1); unsigned long y_bufferSize = w * h; unsigned long uv_bufferSize = w * h / 4; uint16_t *y_planeData = (uint16_t *) malloc(y_bufferSize * sizeof(uint16_t)); uint16_t *u_planeData = (uint16_t *) malloc(uv_bufferSize * sizeof(uint16_t)); uint16_t *v_planeData = (uint16_t *) malloc(uv_bufferSize * sizeof(uint16_t)); uint16_t *y = (uint16_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 0); uint16_t *uv = (uint16_t *) CVPixelBufferGetBaseAddressOfPlane(pixelBufferCopy, 1); int u_offset = 0; int v_offset = 0; uint8_t R, G, B; uint16_t Y, U, V; for (int i = 0; i &lt; h; i ++) &#123; for (int j = 0; j &lt; w; j ++) &#123; int offset = i * w + j; B = baseAddress[offset * 4]; G = baseAddress[offset * 4 + 1]; R = baseAddress[offset * 4 + 2]; bt2020_rgb2yuv10bit_TV(R, G, B, Y, U, V); y_planeData[offset] = Y; //隔行扫描 偶数行的偶数列取U 奇数行的偶数列取V if (j % 2 == 0) &#123; (i % 2 == 0) ? u_planeData[++u_offset] = U : v_planeData[++v_offset] = V; &#125; &#125; &#125; for (int i = 0; i &lt; plane_h1; i ++) &#123; memcpy(y + i * y_stride / 2, y_planeData + i * w, 2 * w); if (i &lt; plane_h2) &#123; for (int j = 0 ; j &lt; w ; j ++) &#123; //NV12 和 NV21 格式都属于 YUV420SP 类型。它也是先存储了 Y 分量，但接下来并不是再存储所有的 U 或者 V 分量，而是把 UV 分量交替连续存储。 //NV12 是 IOS 中有的模式，它的存储顺序是先存 Y 分量，再 UV 进行交替存储。 memcpy(uv + i * y_stride / 2 + 2*j, u_planeData + i * w/2 + j, 2); memcpy(uv + i * y_stride / 2 + 2*j + 1, v_planeData + i * w/2 + j, 2); &#125; &#125; &#125; free(y_planeData); free(u_planeData); free(v_planeData); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); CVPixelBufferUnlockBaseAddress(pixelBufferCopy, 0); return pixelBufferCopy; &#125; Formula BT.601 BT.709 BT.2020 a 0.299 0.2126 0.2627 b 0.587 0.7152 0.6780 c 0.114 0.0722 0.0593 d 1.772 1.8556 1.8814 e 1.402 1.5748 1.4747 123Y = a * R + b * G + c * BCb = (B - Y) / dCr = (R - Y) / e 123R = Y + e * CrG = Y - (a * e / b) * Cr - (c * d / b) * CbB = Y + d * Cb 123a+b+c = 1e = 2 * (1 - a)d = 2* (a + b) https://www.itu.int/rec/R-REC-BT.601 https://www.itu.int/rec/R-REC-BT.709 https://www.itu.int/rec/R-REC-BT.2020 Range Deduce1234567891011121314[16/255, 16/255, 16/255, 1.0][235/255, 240/255, 240/255, 1.0]x[255/219, 0, 0, 0][0, 255/224, 0, 0][0, 0, 255/224, 0][-16/219, -128/224, -128/224, 1]=[0, -0.5, -0.5, 1.0][1, 0.5, 0.5, 1.0] 将 videorange 通过 齐次矩阵 转换为 fullrange Other大部分图像捕捉设备在保存图像时会自动加上伽马校正，也就是说图像中存储的是非线性空间中的颜色 非线性的RGB转换为YUV也是非线性 OpenGL 无法直接对 10bit YUV 进行处理，需要先转换为 8bit YUV tone mapping 需要在线性RGB空间进行 References搞清楚编程中YUV和RGB间的相互转换YUV - RGB colorconversion推导视频YUV转RGB矩阵齐次坐标# Gamma校正# 我理解的伽马校正Colour gamut conversion from Recommendation ITU-R BT.2020 to Recommendation ITU-R BT.709Colour conversion from Recommendation ITU-R BT.709 to Recommendation ITU-R BT.2020]]></content>
      <categories>
        <category>HDR</category>
      </categories>
      <tags>
        <tag>HDR</tag>
        <tag>YUV</tag>
        <tag>OPENGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coreimage with metal 笔记]]></title>
    <url>%2F2022%2F08%2F08%2Fcoreimage-with-metal-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[xcode近期有自定义CoreImage的CIFilter的需求，前期通过CIKL 定义 CIKernel完成了任务，后面了解到CoreImage新特性支持metal的方式直接自定义 CIKernel，提高效率。 CIKL的方式，存在两个问题： 编写 kernel 的时候，没有报错提示，哪怕是参数名错误都无法检查处理。效率极低。 翻译转换，编译，都是发生到运行时，导致第一次使用滤镜的时候，耗时较久。 Metal: 在build阶段 就可以编译 链接 .metal文件 参考苹果的官方文档 Metal Shading Language for CoreImage Kernels ,在xcode integration 部分提到在build setting 设置 Other Metal Compiler Flags, 文档已经很老了（2018年的），新版的xcode已经没有这个选项了，如果不做处理，会有报错 &quot;/air-lld:1:1: symbol(s) not found for target &#39;air64-apple-ios12.0.0&#39;&quot; and &quot;air-lld command failed with exit code 1 (use -v to see invocation)&quot; build rules新版xcode中可以通过配置build rules解决上面的报错 *.metal 1xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel &quot;$&#123;INPUT_FILE_PATH&#125;&quot; -o &quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;&quot; output files : $(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib *.air 1xcrun metallib -cikernel &quot;$&#123;INPUT_FILE_PATH&#125;&quot; -o &quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;&quot; output files : $(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air 如图： cocoapodsbuild rules 的方式存在一个问题，如果metal shader文件在pod库中，在主工程从配置build rules无法针对pod中的resouce 生效，虽然可以手动针对pod target 配置build rules解决问题，但是这样配置是一次性的，无法提交保存，下一次pod update就清空了，所以到了这里就很自然的能想到通过pod 的post hook 来解决问题，接下来就是怎么用ruby 来写 pod hook 脚本了 通过之前在主工程配置build rules, 可以看到project.pbxproj文件的变更情况 123456789101112131415161718192021222324252627282930/* Begin PBXBuildRule section */ BF25E98B28A0A91A00188AE3 /* PBXBuildRule */ = &#123; isa = PBXBuildRule; compilerSpec = com.apple.compilers.proxy.script; filePatterns = &quot;*.metal&quot;; fileType = pattern.proxy; inputFiles = ( ); isEditable = 1; outputFiles = ( &quot;$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air&quot;, ); runOncePerArchitecture = 0; script = &quot;# Type a script or drag a script file from your workspace to insert its path.\nxcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \&quot;$&#123;INPUT_FILE_PATH&#125;\&quot; -o \&quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;\&quot;\n&quot;; &#125;; BF25E98C28A0A92200188AE3 /* PBXBuildRule */ = &#123; isa = PBXBuildRule; compilerSpec = com.apple.compilers.proxy.script; filePatterns = &quot;*.air&quot;; fileType = pattern.proxy; inputFiles = ( ); isEditable = 1; outputFiles = ( &quot;$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib&quot;, ); runOncePerArchitecture = 0; script = &quot;# Type a script or drag a script file from your workspace to insert its path.\nxcrun metallib -cikernel \&quot;$&#123;INPUT_FILE_PATH&#125;\&quot; -o \&quot;$&#123;SCRIPT_OUTPUT_FILE_0&#125;\&quot;\n&quot;; &#125;;/* End PBXBuildRule section */ 哈哈 ，这正是我们需要的build rule的字段 最终的 MetalBuildRule.rb 文件如下 ： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/usr/bin/ruby# -*- coding: UTF-8 -*-def add_build_rule(target_name, project) project.targets.each do |target| if target.name == target_name# puts "#&#123;target.name&#125; has #&#123;target.build_rules.count&#125; build rule." if target.build_rules.count &gt;= 2 puts "#&#123;target.name&#125; already has 2 build rule." return end puts "Updating #&#123;target.name&#125; build rules" metal_rule = project.new(Xcodeproj::Project::Object::PBXBuildRule) metal_rule.name = 'Metal Build Rule' metal_rule.compiler_spec = 'com.apple.compilers.proxy.script' metal_rule.file_patterns = '*.metal' metal_rule.file_type = 'pattern.proxy' metal_rule.is_editable = '1' metal_rule.run_once_per_architecture = '0' metal_rule.output_files = ["$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air"] metal_rule.input_files = [] metal_rule.output_files_compiler_flags = [] metal_rule.script = "xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(metal_rule) air_rule = project.new(Xcodeproj::Project::Object::PBXBuildRule) air_rule.name = 'Air Build Rule' air_rule.compiler_spec = 'com.apple.compilers.proxy.script' air_rule.file_patterns = '*.air' air_rule.file_type = 'pattern.proxy' air_rule.is_editable = '1' air_rule.run_once_per_architecture = '0' air_rule.output_files = ["$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib"] air_rule.input_files = [] air_rule.output_files_compiler_flags = [] air_rule.script = "xcrun metallib -cikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(air_rule) project.objects_by_uuid[metal_rule.uuid] = metal_rule project.objects_by_uuid[air_rule.uuid] = air_rule project.save() end endend podfile 文件里加载 MetalBuildRule.rb, 配置hook 123post_install do |installer| add_build_rule("your-target-name", installer.pods_project)end framework如果pod库是通过cocoapods-packager插件 打.a 或者 .framework的方式提供给主工程使用的话，发现还是会遇到上文提到的错误 &quot;/air-lld:1:1: symbol(s) not found for target &#39;air64-apple-ios12.0.0&#39;&quot; and &quot;air-lld command failed with exit code 1 (use -v to see invocation)&quot; 这里需要简单了解下 cocoapods-packager 的原理，浅析 Cocoapods-Packager 实现.因为 cocoapods-packager 会重新生成一个podfile 来构造一个打包用的工程，所以这个错误的出现跟文章最开始提到的情况是一模一样的，解法是不是也可以通过配置build rule来解呢，不过打包工程我们看起来好像无法干预，怎么解呢？ 还是要回到cocoapods-packager插件来解决问题。 1https://github.com/CocoaPods/cocoapods-packager git 代码拉下来 通过ide(vscode/rubymine) 打开插件工程 配置好工程ruby环境 DEBUG 代码，找到干预点 设置build rule 生成packager gem，安装 这里涉及到ruby gem bundle等ruby环境的基本命令/用法，可以自行google一下。 通过刚刚提到的插件原理，很容易找到干预点 1234pod_utils.rbdef install_pod(platform_name, sandbox)...end 修改： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def install_pod(platform_name, sandbox) # 判断resource_bundle 是否有.metal文件 metal = false if @spec.attributes_hash["resource_bundle"] metal = @spec.attributes_hash["resource_bundle"][@spec.name].include?("metal") end if @spec.attributes_hash["resource_bundles"] if @spec.attributes_hash["resource_bundles"][@spec.name] @spec.attributes_hash["resource_bundles"][@spec.name].each &#123; |res| metal ||= res.include?("metal") &#125; end end ... unless static_installer.nil? static_installer.pods_project.targets.each do |target| # 如果有.metal文件 &amp;&amp; target匹配 -&gt; 设置 build rule if metal &amp;&amp; target.name.start_with?(@spec.name) UI.puts "#&#123;target.name&#125; has #&#123;target.build_rules.count&#125; build rule." if target.build_rules.count &gt;= 2 UI.puts "#&#123;target.name&#125; already has 2 build rule." return end metal_rule = static_installer.pods_project.new(Xcodeproj::Project::Object::PBXBuildRule) UI.puts "Updating #&#123;target.name&#125; rules" metal_rule.name = 'Metal Build Rule' metal_rule.compiler_spec = 'com.apple.compilers.proxy.script' metal_rule.file_patterns = '*.metal' metal_rule.file_type = 'pattern.proxy' metal_rule.is_editable = '1' metal_rule.run_once_per_architecture = '0' metal_rule.output_files = ["$(DERIVED_FILE_DIR)/$(INPUT_FILE_BASE).air"] metal_rule.input_files = [] metal_rule.output_files_compiler_flags = [] metal_rule.script = "xcrun metal -c $MTL_HEADER_SEARCH_PATHS -fcikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(metal_rule) air_rule = static_installer.pods_project.new(Xcodeproj::Project::Object::PBXBuildRule) UI.puts "Updating #&#123;target.name&#125; rules" air_rule.name = 'Air Build Rule' air_rule.compiler_spec = 'com.apple.compilers.proxy.script' air_rule.file_patterns = '*.air' air_rule.file_type = 'pattern.proxy' air_rule.is_editable = '1' air_rule.run_once_per_architecture = '0' air_rule.output_files = ["$(METAL_LIBRARY_OUTPUT_DIR)/$(INPUT_FILE_BASE).metallib"] air_rule.input_files = [] air_rule.output_files_compiler_flags = [] air_rule.script = "xcrun metallib -cikernel \"$&#123;INPUT_FILE_PATH&#125;\" -o \"$&#123;SCRIPT_OUTPUT_FILE_0&#125;\"" target.build_rules.append(air_rule) static_installer.pods_project.objects_by_uuid[metal_rule.uuid] = metal_rule static_installer.pods_project.objects_by_uuid[air_rule.uuid] = air_rule static_installer.pods_project.save end ... end ... end ... end 最后重新生成、安装gem 12345#!/bin/bashgem uninstall cocoapods-packagergem build cocoapods-packager.gemspecgem install cocoapods-packager podfile使用自定义的cocoapods-packager打出来的二方库 Framework包，包内容里面已经替换成xxx.metallib文件了,所以主工程的podfile pod post hook 要根据二方库的接入方式做下处理。 我这边主工程是用过cocoapod-binary插件管理二方库的加入，源码&amp;静态Framework，一般Release模式提升编译速度，都是以framework方式，Debug模式有时候需要在主工程Debug二方库，可以选择是源码方式接入。 最终的逻辑如下： 1234567891011121314151617post_install do |installer| installer.pods_project.targets.each do |target| ... target.build_configurations.each do |config| ... #Release model, no need execute if config.name != 'Release' &amp;&amp; target.name == 'your target name' puts "===================&gt; #&#123;config.name&#125;" eval(File.open('MetalBuildRule.rb').read) if File.exist? 'MetalBuildRule.rb' # metal shader build rule add_build_rule(target, installer.pods_project) end end endend DONE referencesMetalCIKLReferenceAdd custom build rule with Podfile post_install hookxcodeprojxcode工程文件解析CocoaPods源码与插件断点调试]]></content>
      <categories>
        <category>metal</category>
      </categories>
      <tags>
        <tag>metal</tag>
        <tag>coreimage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter 1.17共享engine]]></title>
    <url>%2F2020%2F06%2F03%2Fflutter-1-17%E5%85%B1%E4%BA%ABengine%2F</url>
    <content type="text"><![CDATA[前言flutter 升级到 1.17之后，app ios 线上遇到一个crash ,通过官方的 符号表文件 flutter.dsym 还原出堆栈如下 1234560 auto fml::internal::CopyableLambda&lt;flutter::Shell::OnPlatformViewCreated(std::__1::unique_ptr&lt;flutter::Surface, std::__1::default_delete&lt;flutter::Surface&gt; &gt;)::$_8&gt;::operator()&lt;&gt;() const (in Flutter) (make_copyable.h:24)1 auto fml::internal::CopyableLambda&lt;flutter::Shell::OnPlatformViewCreated(std::__1::unique_ptr&lt;flutter::Surface, std::__1::default_delete&lt;flutter::Surface&gt; &gt;)::$_8&gt;::operator()&lt;&gt;() const (in Flutter) (make_copyable.h:24)2 fml::MessageLoopImpl::FlushTasks(fml::FlushType) (in Flutter) (message_loop_impl.cc:129)3 fml::MessageLoopDarwin::OnTimerFire(__CFRunLoopTimer*, fml::MessageLoopDarwin*) (in Flutter) (message_loop_darwin.mm:76)9 fml::MessageLoopDarwin::Run() (in Flutter) (message_loop_darwin.mm:47)10 void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, fml::Thread::Thread(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)::$_0&gt; &gt;(void*) (in Flutter) (thread:352) 这里还只能看到crash在engine的c++代码中，具体原因未知 定位我们根据crash 用户的 埋点日志 分析crash前的 使用路径，基本都是打开push 落地到一个flutter页面app 的 第一个tab 也是个 flutter 页面，所以是push 唤起app，连续打开两个flutter页面。手动打开app，点击进到flutter页面是不会crash的（这么简单的路径，如果crash，那就该死了）很快我们就可以通过这个 路径 复现 crash ，能复现就好说。 debug engine源码，可以定位到更具体的地方 surface_ 为 null ，EXC_BAD_ACCESS 野指针 分析定位到了具体的代码位置，接下来分析下野指针的原因xcode中 crash 的时候，看到主线程的 堆栈记录 是从 application 的 didbecomeactive 的 notification发起的由于是 push 唤起app ，有这个通知是对的，crash 是在 共享engine 的 raster 线程。 看代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#pragma mark - Application lifecycle notifications// app 的 首页 flutter 页面会 执行 surfaceUpdated 方法- (void)applicationBecameActive:(NSNotification*)notification &#123; TRACE_EVENT0("flutter", "applicationBecameActive"); if (_viewportMetrics.physical_width) [self surfaceUpdated:YES]; [self goToApplicationLifecycle:@"AppLifecycleState.resumed"];&#125;#pragma mark - Surface creation and teardown updates- (void)surfaceUpdated:(BOOL)appeared &#123; // NotifyCreated/NotifyDestroyed are synchronous and require hops between the UI and raster // thread. if (appeared) &#123; [self installFirstFrameCallback]; [_engine.get() platformViewsController] -&gt; SetFlutterView(_flutterView.get()); [_engine.get() platformViewsController] -&gt; SetFlutterViewController(self); // 这里 [_engine.get() platformView] -&gt; NotifyCreated(); &#125; else &#123; self.displayingFlutterUI = NO; [_engine.get() platformView] -&gt; NotifyDestroyed(); [_engine.get() platformViewsController] -&gt; SetFlutterView(nullptr); [_engine.get() platformViewsController] -&gt; SetFlutterViewController(nullptr); &#125;&#125;void PlatformView::NotifyCreated() &#123; std::unique_ptr&lt;Surface&gt; surface; // Threading: We want to use the platform view on the non-platform thread. // Using the weak pointer is illegal. But, we are going to introduce a latch // so that the platform view is not collected till the surface is obtained. auto* platform_view = this; fml::ManualResetWaitableEvent latch; fml::TaskRunner::RunNowOrPostTask( task_runners_.GetRasterTaskRunner(), [platform_view, &amp;surface, &amp;latch]() &#123; surface = platform_view-&gt;CreateRenderingSurface(); latch.Signal(); &#125;); latch.Wait(); //这里 delegate_.OnPlatformViewCreated(std::move(surface));&#125;// |PlatformView::Delegate|void Shell::OnPlatformViewCreated(std::unique_ptr&lt;Surface&gt; surface) &#123; TRACE_EVENT0("flutter", "Shell::OnPlatformViewCreated"); FML_DCHECK(is_setup_); FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); // Note: // This is a synchronous operation because certain platforms depend on // setup/suspension of all activities that may be interacting with the GPU in // a synchronous fashion. fml::AutoResetWaitableEvent latch; auto raster_task = fml::MakeCopyable([&amp; waiting_for_first_frame = waiting_for_first_frame_, rasterizer = rasterizer_-&gt;GetWeakPtr(), // surface = std::move(surface), // &amp;latch]() mutable &#123; if (rasterizer) &#123; //这里 rasterizer-&gt;Setup(std::move(surface)); &#125; waiting_for_first_frame.store(true); // Step 3: All done. Signal the latch that the platform thread is // waiting on. latch.Signal(); &#125;); ... &#125;void Rasterizer::Setup(std::unique_ptr&lt;Surface&gt; surface) &#123; surface_ = std::move(surface); if (max_cache_bytes_.has_value()) &#123; SetResourceCacheMaxBytes(max_cache_bytes_.value(), user_override_resource_cache_bytes_); &#125; compositor_context_-&gt;OnGrContextCreated(); // surface_ null，BAD_ACCESS if (surface_-&gt;GetExternalViewEmbedder()) &#123; const auto platform_id = task_runners_.GetPlatformTaskRunner()-&gt;GetTaskQueueId(); const auto gpu_id = task_runners_.GetRasterTaskRunner()-&gt;GetTaskQueueId(); raster_thread_merger_ = fml::MakeRefCounted&lt;fml::RasterThreadMerger&gt;(platform_id, gpu_id); &#125;&#125; 这么一路看下来，surface_怎么会变成null呢？一般情况是，执行 [self surfaceUpdated:NO] 的时候会销毁surface，断点根本都没进去。继续看代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107// push 落地页 flutter 页面 init 的时候，会重新attach 到 engine，会执行setViewController方法- (void)setViewController:(FlutterViewController*)viewController &#123; FML_DCHECK(self.iosPlatformView); _viewController = viewController ? [viewController getWeakPtr] : fml::WeakPtr&lt;FlutterViewController&gt;(); //这里 self.iosPlatformView-&gt;SetOwnerViewController(_viewController); [self maybeSetupPlatformViewChannels]; if (viewController) &#123; __block FlutterEngine* blockSelf = self; self.flutterViewControllerWillDeallocObserver = [[NSNotificationCenter defaultCenter] addObserverForName:FlutterViewControllerWillDealloc object:viewController queue:[NSOperationQueue mainQueue] usingBlock:^(NSNotification* note) &#123; [blockSelf notifyViewControllerDeallocated]; &#125;]; &#125; else &#123; self.flutterViewControllerWillDeallocObserver = nil; &#125;&#125;void PlatformViewIOS::SetOwnerViewController(fml::WeakPtr&lt;FlutterViewController&gt; owner_controller) &#123; FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); std::lock_guard&lt;std::mutex&gt; guard(ios_surface_mutex_); // 重点是这里 if (ios_surface_ || !owner_controller) &#123; // 这里会销毁 NotifyDestroyed(); ios_surface_.reset(); accessibility_bridge_.reset(); &#125; owner_controller_ = owner_controller; // Add an observer that will clear out the owner_controller_ ivar and // the accessibility_bridge_ in case the view controller is deleted. dealloc_view_controller_observer_.reset( [[[NSNotificationCenter defaultCenter] addObserverForName:FlutterViewControllerWillDealloc object:owner_controller_.get() queue:[NSOperationQueue mainQueue] usingBlock:^(NSNotification* note) &#123; // Implicit copy of 'this' is fine. accessibility_bridge_.reset(); owner_controller_.reset(); &#125;] retain]); if (owner_controller_ &amp;&amp; [owner_controller_.get() isViewLoaded]) &#123; this-&gt;attachView(); &#125; // Do not call `NotifyCreated()` here - let FlutterViewController take care // of that when its Viewport is sized. If `NotifyCreated()` is called here, // it can occasionally get invoked before the viewport is sized resulting in // a framebuffer that will not be able to completely attach.&#125;void PlatformView::NotifyDestroyed() &#123; delegate_.OnPlatformViewDestroyed();&#125;// |PlatformView::Delegate|void Shell::OnPlatformViewDestroyed() &#123; TRACE_EVENT0("flutter", "Shell::OnPlatformViewDestroyed"); FML_DCHECK(is_setup_); FML_DCHECK(task_runners_.GetPlatformTaskRunner()-&gt;RunsTasksOnCurrentThread()); // Note: // This is a synchronous operation because certain platforms depend on // setup/suspension of all activities that may be interacting with the GPU in // a synchronous fashion. fml::AutoResetWaitableEvent latch; auto io_task = [io_manager = io_manager_.get(), &amp;latch]() &#123; // Execute any pending Skia object deletions while GPU access is still // allowed. io_manager-&gt;GetIsGpuDisabledSyncSwitch()-&gt;Execute( fml::SyncSwitch::Handlers().SetIfFalse( [&amp;] &#123; io_manager-&gt;GetSkiaUnrefQueue()-&gt;Drain(); &#125;)); // Step 3: All done. Signal the latch that the platform thread is waiting // on. latch.Signal(); &#125;; auto raster_task = [rasterizer = rasterizer_-&gt;GetWeakPtr(), io_task_runner = task_runners_.GetIOTaskRunner(), io_task]() &#123; if (rasterizer) &#123; // 这里 rasterizer-&gt;Teardown(); &#125; // Step 2: Next, tell the IO thread to complete its remaining work. fml::TaskRunner::RunNowOrPostTask(io_task_runner, io_task); &#125;; ...void Rasterizer::Teardown() &#123; compositor_context_-&gt;OnGrContextDestroyed(); // 这里 reset surface_.reset(); last_layer_tree_.reset();&#125; 所以原因 就是 落地页 init 的时候 重新attach 引擎，NotifyDestroyed 方法 最终会销毁 surface，这时候正好raster线程使用 surface_做方法调用。 修复定位到原因，修复就简单了，做下空判断就好了,如果为空 就直接return 123456789101112131415161718192021void Rasterizer::Setup(std::unique_ptr&lt;Surface&gt; surface) &#123; surface_ = std::move(surface); if (!surface_) &#123; FML_DLOG(INFO) &lt;&lt; "Rasterizer::Setup called with no surface."; return; &#125; if (max_cache_bytes_.has_value()) &#123; SetResourceCacheMaxBytes(max_cache_bytes_.value(), user_override_resource_cache_bytes_); &#125; compositor_context_-&gt;OnGrContextCreated(); if (surface_-&gt;GetExternalViewEmbedder()) &#123; const auto platform_id = task_runners_.GetPlatformTaskRunner()-&gt;GetTaskQueueId(); const auto gpu_id = task_runners_.GetRasterTaskRunner()-&gt;GetTaskQueueId(); raster_thread_merger_ = fml::MakeRefCounted&lt;fml::RasterThreadMerger&gt;(platform_id, gpu_id); &#125;&#125; 这里是直接修改了引擎的代码，所以需要重新编译engine 产物，替换掉就搞定了 其他1.17之前的版本 1.12.13 的时候，不确定engine存不存在这个问题，有空再看看。后面github提issue、PR，看看官方怎么看待这个问题，修复应该还有其他办法。]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter 1.17升级]]></title>
    <url>%2F2020%2F05%2F21%2Fflutter-1-17%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[升级最近官方发布了flutter 稳定版本1.17.0 ，记录下升级1.17 ios上碰到的的问题 App产物在 1.12.13 的时候，为了支持模拟器运行，会进行 debug 产物 跟 release 产物的merge （lipo create …）debug 产物 x86 、release 产物 arm64 arm7 升级到1.17.0 之后 ，merge报错 lipo 查看下 发现针对模拟器的debug产物 含有arm64 Debug Flutter tool源码， build 里面进行了两次createStubAppFramework（iphone &amp;&amp; simulator）然后做了merge，实际上environment参数里面 iosArchs只有 arch x86，所以问题出在这里 DebugUniveralFramework1234567891011121314151617181920212223242526272829303132333435363738394041@overrideFuture&lt;void&gt; build(Environment environment) async &#123; // Generate a trivial App.framework. final Set&lt;DarwinArch&gt; iosArchs = environment.defines[kIosArchs] ?.split(' ') ?.map(getIOSArchForName) ?.toSet() ?? &lt;DarwinArch&gt;&#123;DarwinArch.arm64&#125;; final File iphoneFile = environment.buildDir.childFile('iphone_framework'); final File simulatorFile = environment.buildDir.childFile('simulator_framework'); final File lipoOutputFile = environment.buildDir.childFile('App'); final RunResult iphoneResult = await createStubAppFramework( iphoneFile, SdkType.iPhone, // Only include 32bit if it is contained in the active architectures. include32Bit: iosArchs.contains(DarwinArch.armv7) ); final RunResult simulatorResult = await createStubAppFramework( simulatorFile, SdkType.iPhoneSimulator, ); if (iphoneResult.exitCode != 0 || simulatorResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125; final List&lt;String&gt; lipoCommand = &lt;String&gt;[ 'xcrun', 'lipo', '-create', iphoneFile.path, simulatorFile.path, '-output', lipoOutputFile.path ]; final RunResult lipoResult = await processUtils.run( lipoCommand, ); if (lipoResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125;&#125; 解决办法 可以通过archs 判断下具体执行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@override Future&lt;void&gt; build(Environment environment) async &#123; // Generate a trivial App.framework. final Set&lt;DarwinArch&gt; iosArchs = environment.defines[kIosArchs] ?.split(' ') ?.map(getIOSArchForName) ?.toSet() ?? &lt;DarwinArch&gt;&#123;DarwinArch.arm64&#125;; final File iphoneFile = environment.buildDir.childFile('iphone_framework'); final File simulatorFile = environment.buildDir.childFile('simulator_framework'); final File lipoOutputFile = environment.buildDir.childFile('App'); RunResult iphoneResult; if(iosArchs.contains(DarwinArch.arm64) || iosArchs.contains(DarwinArch.armv7)) &#123; iphoneResult = await createStubAppFramework( iphoneFile, SdkType.iPhone, // Only include 32bit if it is contained in the active architectures. include32Bit: iosArchs.contains(DarwinArch.armv7) ); if (iphoneResult.exitCode != 0) &#123; throw Exception('(iphoneResult)Failed to create App.framework.'); &#125; &#125; RunResult simulatorResult; if(iosArchs.contains(DarwinArch.x86_64)) &#123; simulatorResult = await createStubAppFramework( simulatorFile, SdkType.iPhoneSimulator, ); if (simulatorResult.exitCode != 0) &#123; throw Exception('(simulatorResult)Failed to create App.framework.'); &#125; &#125; if(simulatorResult == null) &#123; iphoneFile.copySync(lipoOutputFile.path); return; &#125; if(iphoneResult == null) &#123; simulatorFile.copySync(lipoOutputFile.path); return; &#125; final List&lt;String&gt; lipoCommand = &lt;String&gt;[ 'xcrun', 'lipo', '-create', iphoneFile.path, simulatorFile.path, '-output', lipoOutputFile.path ]; final RunResult lipoResult = await processUtils.run( lipoCommand, ); if (lipoResult.exitCode != 0) &#123; throw Exception('Failed to create App.framework.'); &#125; &#125; 突然想到 既然1.17 对debug 产物做了arm64的支持，那我们收集产物是不是可以不用自己做merge，发现是不可以的因为除了App.framework,还有plugin native代码生成的pod静态库 libxxx.a。静态库 是哪里生成的呢？ 这里 build_ios.dart -&gt; buildXcodeProject 123456789101112131415161718...final List&lt;String&gt; buildCommands = &lt;String&gt;[ &apos;/usr/bin/env&apos;, &apos;xcrun&apos;, &apos;xcodebuild&apos;, &apos;-configuration&apos;, configuration,]; ...if (buildForDevice) &#123; buildCommands.addAll(&lt;String&gt;[&apos;-sdk&apos;, &apos;iphoneos&apos;]);&#125; else &#123; buildCommands.addAll(&lt;String&gt;[&apos;-sdk&apos;, &apos;iphonesimulator&apos;, &apos;-arch&apos;, &apos;x86_64&apos;]);&#125;... 这里只针对x86做了xcode build，所以还是要自己merge的…]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter tool debug]]></title>
    <url>%2F2020%2F03%2F13%2Fflutter-tool-debug%2F</url>
    <content type="text"><![CDATA[Flutterflutter 开发过程中，少不了会运行一些 flutter 命令 ，比如 flutter build xxx 、 flutter run 等等看下 bin/flutter 脚本，背后都是 flutter_tool 在执行各种操作。 12345678FLUTTER_TOOLS_DIR="$FLUTTER_ROOT/packages/flutter_tools"SNAPSHOT_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.snapshot"STAMP_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.stamp"SCRIPT_PATH="$FLUTTER_TOOLS_DIR/bin/flutter_tools.dart"DART_SDK_PATH="$FLUTTER_ROOT/bin/cache/dart-sdk""$DART" --packages="$FLUTTER_TOOLS_DIR/.packages" $FLUTTER_TOOL_ARGS "$SNAPSHOT_PATH" "$@" 关于 debug flutter_tool,就不说的了，自行google一下。。 这里记录下 debug 过程中遇到的问题 Exception指定端口导出FLUTTER_TOOL_ARGS 环境变量 export FLUTTER_TOOL_ARGS=&quot;--pause_isolates_on_start --enable-vm-service:65432&quot; run 起来 会停在这里… Observatory listening on http://127.0.0.1:65432/ZbWg3veM6kY=/ IDE 中打开flutter tool 项目，配置 dart remote debug attach 出现以下错误信息 123Failed to connect to the VM observatory service: java.io.IOException: Failed to connect: ws://127.0.0.1:65432/wsCaused by: de.roderick.weberknecht.WebSocketException: error while creating socket to ws://127.0.0.1:65432/wsCaused by: java.net.ConnectException: Connection refused (Connection refused) 原因 ： http://127.0.0.1:65432/ZbWg3veM6kY=/ 后面多了个ZbWg3veM6kY，这是一种认证码，是为了安全原因，防止应用被远程调试。可以通过参数–disable-service-auth-codes进行关闭。 export FLUTTER_TOOL_ARGS=&quot;--pause_isolates_on_start --enable-vm-service:65432 --disable-service-auth-codes&quot;]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter共享engine]]></title>
    <url>%2F2020%2F03%2F10%2Fflutter%E5%85%B1%E4%BA%ABengine%2F</url>
    <content type="text"><![CDATA[flutter 共享引擎 问题记录共享引擎，就是只有一个 flutter engine，每个页面一个 flutterviewcontroller。flutter页面 切换，引擎会相应的 detach atach 最近 升级 flutter 到 v1.12.13 版本后，贡献引擎遇到的几个问题 记录下 present flutter 页面这个其实不是v1.12.13出现的问题 flutterPageA present flutterPageB 会 出现 pageA的 viewDidDisappear 比 pageB的 viewDidAppear 后执行 12345678910- (void)viewDidDisappear:(BOOL)animated &#123; [super viewDidDisappear:animated]; //处理下present 页面卡死的情况 if ([WD_FLUTTER_ENGINE flutterViewController] != self) &#123; [WD_FLUTTER_ENGINE resume]; [(WDFlutterViewContainer *)[WD_FLUTTER_ENGINE flutterViewController] surfaceUpdated:YES]; &#125; else &#123; [WD_FLUTTER_ENGINE detach]; &#125;&#125; 从flutter 返回到 native页面在 v.1.12.13之前 flutter popto native 无需处理v.1.12.13 出现crash 123456789101112131415161718192021222324[VERBOSE-2:FlutterObservatoryPublisher.mm(131)] Could not register as server for FlutterObservatoryPublisher. Check your network settings and relaunch the application.SCNetworkReachabilitySetDispatchQueue() failed: Invalid argumentSCNetworkReachabilitySetDispatchQueue() failed: Invalid argumentlocalConnectionInitializedStatus:2localConnectionInitializedStatus:2SCNetworkReachabilitySetDispatchQueue() failed: Invalid argument[Bugly] Fatal signal(11) raised.[Bugly] Trapped fatal signal &apos;SIGSEGV(11)&apos; ( &quot;0 Flutter 0x000000010516f92c _ZNK3fml8internal14CopyableLambdaIZN7flutter5Shell21OnPlatformViewCreatedENSt3__110unique_ptrINS2_7SurfaceENS4_14default_deleteIS6_EEEEE3$_8EclIJEEEDaDpOT_ + 236&quot;, &quot;1 Flutter 0x000000010516f928 _ZNK3fml8internal14CopyableLambdaIZN7flutter5Shell21OnPlatformViewCreatedENSt3__110unique_ptrINS2_7SurfaceENS4_14default_deleteIS6_EEEEE3$_8EclIJEEEDaDpOT_ + 232&quot;, &quot;2 Flutter 0x0000000105123cf4 _ZN3fml15MessageLoopImpl10FlushTasksENS_9FlushTypeE + 1700&quot;, &quot;3 Flutter 0x0000000105126000 _ZN3fml17MessageLoopDarwin11OnTimerFireEP16__CFRunLoopTimerPS0_ + 32&quot;, &quot;4 CoreFoundation 0x0000000184cd3aa8 0x0000000184be5000 + 977576&quot;, &quot;5 CoreFoundation 0x0000000184cd376c 0x0000000184be5000 + 976748&quot;, &quot;6 CoreFoundation 0x0000000184cd3010 0x0000000184be5000 + 974864&quot;, &quot;7 CoreFoundation 0x0000000184cd0b60 0x0000000184be5000 + 965472&quot;, &quot;8 CoreFoundation 0x0000000184bf0da8 CFRunLoopRunSpecific + 552&quot;, &quot;9 Flutter 0x0000000105125edc _ZN3fml17MessageLoopDarwin3RunEv + 88&quot;, &quot;10 Flutter 0x0000000105125684 _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN3fml6ThreadC1ERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEEE3$_0EEEEEPvSJ_ + 200&quot;, &quot;11 libsystem_pthread.dylib 0x0000000184951220 0x000000018494f000 + 8736&quot;, &quot;12 libsystem_pthread.dylib 0x0000000184951110 0x000000018494f000 + 8464&quot;)Application finished. 处理办法： flutterVc dealloc 或者 disappear 的时候 执行 flutterEngine detach]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter engine 定制]]></title>
    <url>%2F2020%2F03%2F05%2Fflutter-engine-%E5%AE%9A%E5%88%B6%2F</url>
    <content type="text"><![CDATA[…使用Flutter开发的时候最直接接触的并不是 Flutter Engine 而是 Flutter Framework(https://github.com/flutter/flutter)在flutter framework 的 目录里面 有编译好的engine 产物 简单说就是， 编译引擎 替换 产物文件就好了 路径 flutter_path/bin/cache/artifacts/engine/ios 参考Flutter Engine定制流程Flutter Engine 编译指北]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter eventChannel crash on ios]]></title>
    <url>%2F2019%2F10%2F21%2Fflutter-eventChannel-crash-on-ios%2F</url>
    <content type="text"><![CDATA[flutter issuecrash 记录一次困扰了很久的 flutter event channel crash EventChannel dart1234567891011121314151617181920212223242526272829303132333435363738394041Stream&lt;dynamic&gt; receiveBroadcastStream([ dynamic arguments ]) &#123; final MethodChannel methodChannel = MethodChannel(name, codec); StreamController&lt;dynamic&gt; controller; controller = StreamController&lt;dynamic&gt;.broadcast(onListen: () async &#123; defaultBinaryMessenger.setMessageHandler(name, (ByteData reply) async &#123; if (reply == null) &#123; controller.close(); &#125; else &#123; try &#123; controller.add(codec.decodeEnvelope(reply)); &#125; on PlatformException catch (e) &#123; controller.addError(e); &#125; &#125; return null; &#125;); try &#123; await methodChannel.invokeMethod&lt;void&gt;('listen', arguments); &#125; catch (exception, stack) &#123; FlutterError.reportError(FlutterErrorDetails( exception: exception, stack: stack, library: 'services library', context: ErrorDescription('while activating platform stream on channel $name'), )); &#125; &#125;, onCancel: () async &#123; defaultBinaryMessenger.setMessageHandler(name, null); try &#123; await methodChannel.invokeMethod&lt;void&gt;('cancel', arguments); &#125; catch (exception, stack) &#123; FlutterError.reportError(FlutterErrorDetails( exception: exception, stack: stack, library: 'services library', context: ErrorDescription('while de-activating platform stream on channel $name'), )); &#125; &#125;); return controller.stream; &#125; FlutterChannel.mm1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static void SetStreamHandlerMessageHandlerOnChannel(NSObject&lt;FlutterStreamHandler&gt;* handler, NSString* name, NSObject&lt;FlutterBinaryMessenger&gt;* messenger, NSObject&lt;FlutterMethodCodec&gt;* codec) &#123; __block FlutterEventSink currentSink = nil; FlutterBinaryMessageHandler messageHandler = ^(NSData* message, FlutterBinaryReply callback) &#123; FlutterMethodCall* call = [codec decodeMethodCall:message]; if ([call.method isEqual:@"listen"]) &#123; if (currentSink) &#123; FlutterError* error = [handler onCancelWithArguments:nil]; if (error) NSLog(@"Failed to cancel existing stream: %@. %@ (%@)", error.code, error.message, error.details); &#125; currentSink = ^(id event) &#123; if (event == FlutterEndOfEventStream) [messenger sendOnChannel:name message:nil]; else if ([event isKindOfClass:[FlutterError class]]) [messenger sendOnChannel:name message:[codec encodeErrorEnvelope:(FlutterError*)event]]; else [messenger sendOnChannel:name message:[codec encodeSuccessEnvelope:event]]; &#125;; FlutterError* error = [handler onListenWithArguments:call.arguments eventSink:currentSink]; if (error) callback([codec encodeErrorEnvelope:error]); else callback([codec encodeSuccessEnvelope:nil]); &#125; else if ([call.method isEqual:@"cancel"]) &#123; if (!currentSink) &#123; callback( [codec encodeErrorEnvelope:[FlutterError errorWithCode:@"error" message:@"No active stream to cancel" details:nil]]); return; &#125; currentSink = nil; FlutterError* error = [handler onCancelWithArguments:call.arguments]; if (error) callback([codec encodeErrorEnvelope:error]); else callback([codec encodeSuccessEnvelope:nil]); &#125; else &#123; callback(nil); &#125; &#125;; [messenger setMessageHandlerOnChannel:name binaryMessageHandler:messageHandler];&#125; EventSink正常结束stream流 eventSink(FlutterEndOfEventStream) ，异常结束stream流 eventSink(FlutterError) 都会回调执行 onCancel 参考Flutter 与 Native(iOS) 通信原理深入Flutter技术内幕:Platform Channel设计与实现]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter ios 13 dark mode]]></title>
    <url>%2F2019%2F10%2F14%2Fflutter-ios-13-dark-mode%2F</url>
    <content type="text"><![CDATA[前言ios 13 开启 dark model,flutter页面status bar文字一直是白色 flutter issues 1SystemChrome.setSystemUIOverlayStyle(SystemUiOverlayStyle.dark); 设置dark style 并没有用 SystemChrome12345678910111213141516171819202122232425static void setSystemUIOverlayStyle(SystemUiOverlayStyle style) &#123; assert(style != null); if (_pendingStyle != null) &#123; // The microtask has already been queued; just update the pending value. _pendingStyle = style; return; &#125; if (style == _latestStyle) &#123; // Trivial success: no microtask has been queued and the given style is // already in effect, so no need to queue a microtask. return; &#125; _pendingStyle = style; scheduleMicrotask(() &#123; assert(_pendingStyle != null); if (_pendingStyle != _latestStyle) &#123; SystemChannels.platform.invokeMethod&lt;void&gt;( 'SystemChrome.setSystemUIOverlayStyle', _pendingStyle._toMap(), ); _latestStyle = _pendingStyle; &#125; _pendingStyle = null; &#125;); &#125; FlutterPlatformPlugin1234567891011121314151617181920212223242526272829- (void)setSystemChromeSystemUIOverlayStyle:(NSDictionary*)message &#123; NSString* style = message[@"statusBarBrightness"]; if (style == (id)[NSNull null]) return; UIStatusBarStyle statusBarStyle; if ([style isEqualToString:@"Brightness.dark"]) statusBarStyle = UIStatusBarStyleLightContent; else if ([style isEqualToString:@"Brightness.light"]) statusBarStyle = UIStatusBarStyleDefault; else return; NSNumber* infoValue = [[NSBundle mainBundle] objectForInfoDictionaryKey:@"UIViewControllerBasedStatusBarAppearance"]; Boolean delegateToViewController = (infoValue == nil || [infoValue boolValue]); if (delegateToViewController) &#123; // This notification is respected by the iOS embedder [[NSNotificationCenter defaultCenter] postNotificationName:@(kOverlayStyleUpdateNotificationName) object:nil userInfo:@&#123;@(kOverlayStyleUpdateNotificationKey) : @(statusBarStyle)&#125;]; &#125; else &#123; // Note: -[UIApplication setStatusBarStyle] is deprecated in iOS9 // in favor of delegating to the view controller [[UIApplication sharedApplication] setStatusBarStyle:statusBarStyle]; &#125;&#125; engine 源码中 可以看到 没有 UIStatusBarStyleDarkContent 尝试 去掉 info.plist 中的 UIViewControllerBasedStatusBarAppearance 然后 监听 通知 1234567[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(appStatusBar:) name:@"io.flutter.plugin.platform.SystemChromeOverlayNotificationName" object:nil];- (void)appStatusBar:(id)notification &#123; if (@available(iOS 13.0, *)) &#123; [UIApplication sharedApplication].statusBarStyle = UIStatusBarStyleDarkContent; &#125;&#125;]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mustache之dart]]></title>
    <url>%2F2019%2F05%2F31%2Fmustache%E4%B9%8Bdart%2F</url>
    <content type="text"><![CDATA[前言Mustache 是一个 logic-less （轻逻辑）模板解析引擎，可以应用在 js、PHP、Python、Perl 等多种编程语言中。这里主要是看dart中的应用。 模板语法很简单 看这里1234567&#123;&#123;keyName&#125;&#125; &#123;&#123;#keyName&#125;&#125; &#123;&#123;/keyName&#125;&#125;&#123;&#123;^keyName&#125;&#125; &#123;&#123;/keyName&#125;&#125;&#123;&#123;.&#125;&#125;&#123;&#123;&gt;partials&#125;&#125;&#123;&#123;&#123;keyName&#125;&#125;&#125;&#123;&#123;!comments&#125;&#125; 使用在flutter项目中，使用annation router注解的方式生成路由表管理类 RouterManager ，以及业务相关的类文件（ios android）在使用mustache之前，是通过stringbuff 的方式拼接字符串，也可以完成，但是阅读性比较差。 之前123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/// 生成路由表类 StringBuffer sb = new StringBuffer(); sb..write(_createImport())..write(_createClazz(element.name)); return sb.toString();/// 生成路由表类的 import 信息 String _createImport() &#123; StringBuffer sb = new StringBuffer(); sb ..writeln("import 'package:flutter/material.dart';") ..writeln("import 'package:hybrid_router/hybrid_router.dart';"); /// import page collector.importClazzList.forEach((clazz) &#123; sb.writeln("import '$clazz';"); &#125;); return sb.toString(); &#125; /// 生成路由表类的 clazz 信息 String _createClazz(String className) &#123; StringBuffer sb = new StringBuffer(); /// start class sb.writeln("class \$$className &#123;"); /// generateRoute function sb ..writeln(" Map&lt;String, HybridWidgetBuilder&gt; generateRoutes()&#123;") ..writeln(" return &#123;"); collector.routeMap.forEach((key, value) &#123; String flutterPath = value.flutterPath; sb ..writeln(" '$flutterPath': (BuildContext context, Object args) &#123;") ..writeln(" $&#123;_createInstance(value)&#125;") ..writeln(" &#125;,"); &#125;); sb..writeln(" &#125;;")..writeln(" &#125;"); /// generateSpm function sb ..writeln(" Map&lt;String, String&gt; generateSpm() &#123;") ..writeln(" return &#123;"); collector.routeMap.forEach((key, value) &#123; String flutterSpm = value.flutterSpm; String flutterPath = value.flutterPath; if (flutterSpm?.isNotEmpty == true) &#123; sb.writeln(" '$flutterPath': '$flutterSpm',"); &#125; &#125;); sb..writeln(" &#125;;")..writeln(" &#125;"); /// end class sb.writeln("&#125;"); return sb.toString(); &#125; 之后1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/// 生成路由表类 _createRouteManager(element.name); String _createRouteManager(String className) &#123; return VDTemplate.routerManagerTemplate().renderString(&#123; 'classname':className, 'classes':collector.importClazzList, 'routes':collector.routeMap.values, 'createInstance':(LambdaContext ctx) &#123; CollectorItem item = ctx.lookup('.'); return _createInstance(item); &#125; &#125;); &#125; static Template routerManagerTemplate() &#123; var source = '''&#123;&#123;&gt; import&#125;&#125;&#123;&#123;&gt; clazz&#125;&#125; '''; Map&lt;String,Template&gt; map = &#123; "import":VDTemplate.importTemplate(), "clazz":VDTemplate.claszzTemplate() &#125;; return new Template(source,partialResolver: (String name) =&gt; map[name]); &#125; static Template importTemplate() &#123; var source = '''import 'package:flutter/material.dart';import 'package:hybrid_router/hybrid_router.dart';&#123;&#123;# classes &#125;&#125;import '&#123;&#123;&#123;.&#125;&#125;&#125;';&#123;&#123;/ classes &#125;&#125; '''; return new Template(source); &#125; static Template claszzTemplate() &#123; var source = '''class \$&#123;&#123;classname&#125;&#125; &#123; Map&lt;String, HybridWidgetBuilder&gt; generateRoutes()&#123; return &#123; &#123;&#123;#routes&#125;&#125; '&#123;&#123;flutterPath&#125;&#125;': (BuildContext context, Object args) &#123; &#123;&#123;createInstance&#125;&#125; &#125;, &#123;&#123;/routes&#125;&#125; &#125;; &#125; Map&lt;String, String&gt; generateSpm() &#123; return &#123; &#123;&#123;#routes&#125;&#125; &#123;&#123;# flutterSpm&#125;&#125; '&#123;&#123;flutterPath&#125;&#125;': '&#123;&#123;flutterSpm&#125;&#125;', &#123;&#123;/ flutterSpm&#125;&#125; &#123;&#123;/routes&#125;&#125; &#125;; &#125; &#125; '''; return new Template(source); &#125; 对比看下，使用mustache之后，可读性好很多，基本保持了代码结构 总结 字符串数组，可以使用{{.}} 对象数据，可以跟普通的hash一样，直接用{{对象的属性}}，mustache内部通过dart反射拿到属性值 使用partials 拆分template 增加可读性 通过lambda函数执行dart方法，也可以做到拆分的作用 lambdaContext.loopup(&quot;.&quot;) 可以获取对象实例，进而可以参数传递 mustache内部renderstring也是通过stringbuff的方式实现 参考链接mustache 1.1.1Flutter路由管理]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
        <tag>mustache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm调优]]></title>
    <url>%2F2019%2F05%2F23%2Fjvm%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[jps输出JVM中运行的进程状态信息 1234-q 不输出类名、Jar名和传入main方法的参数-m 输出传入main方法的参数-l 输出main类或Jar的全限名-v 输出传入JVM的参数 jps -mlv找到java应用的pid jstack根据java应用pid ,查看进程中线程堆栈信息 jstack pid top找出该进程内最耗费CPU的线程 top -Hp pid 转为十六进制printf &quot;%x\n&quot; 线程id 输出进程的堆栈信息，然后根据线程ID的十六进制值grepjstack pid | grep 十六进制线程id jmap查看堆内存使用状况jmap -heap pid 进程内存使用情况dump到文件中 结合MAT工具分析jmap -dump:format=b,file=dumpFileName pid jhatjhat -port 9998 /tmp/dump.datlocalhost:9998 查看内存对象情况 （不如MAT直观）]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vmstatu]]></title>
    <url>%2F2019%2F05%2F22%2Fvmstatus%2F</url>
    <content type="text"><![CDATA[vmstatuvmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。相比top，可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。 1234$vmstatprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 215220 0 771404 0 0 2 15 0 1 0 0 100 0 0 一般vmstat工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔数，单位是秒，第二个参数是采样的次数 12345$vmstat 2 2procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 202924 0 785780 0 0 2 15 0 1 0 0 100 0 0 0 0 0 203032 0 785812 0 0 0 155 748 1382 0 0 100 0 0 第二个参数如果没有，就会一直采集（ctrl+c 结束） 12345678$vmstat 2procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 194712 0 794728 0 0 2 15 0 1 0 0 100 0 0 0 0 0 194696 0 794768 0 0 0 50 782 1368 0 0 100 0 0 0 0 0 193828 0 794776 0 0 0 108 752 1156 0 0 100 0 0 0 0 0 193952 0 794804 0 0 0 4 601 997 0 0 100 0 0^C 字段procs r 等待运行的进程数 b 处在非中断睡眠状态的进程数 memory （KB） swpd 虚拟内存使用大小 注意：如果swpd的值不为0，但是SI，SO的值长期为0，这种情况不会影响系统性能。 free 空闲的内存 buff 用作缓冲的内存大小 cache 用作缓存的内存大小 注意：如果cache的值大的时候，说明cache处的文件数多，如果频繁访问到的文件都能被cache处，那么磁盘的读IO bi会非常小。 swap si 从交换区写到内存的大小 so 每秒写入交换区的内存大小 内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有些朋友看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的。 io bi 每秒读取的块数 bo 每秒写入的块数 注意：随机磁盘读写的时候，这2个值越大（如超出1024k)，能看到CPU在IO等待的值也会越大。 system in 每秒中断数，包括时钟中断。 cs 每秒上下文切换数。 注意：上面2个值越大，会看到由内核消耗的CPU时间会越大。 cpu us 用户进程执行时间(user time) 注意： us的值比较高时，说明用户进程消耗的CPU时间多，但是如果长期超50%的使用，那么我们就该考虑优化程序算法或者进行加速。 sy 系统进程执行时间(system time) 注意：sy的值高时，说明系统内核消耗的CPU资源多，这并不是良性表现，我们应该检查原因。 id 空闲时间(包括IO等待时间),中央处理器的空闲时间 。以百分比表示。 wa 等待IO时间百分比 注意：wa的值高时，说明IO等待比较严重，这可能由于磁盘大量作随机访问造成，也有可能磁盘出现瓶颈（块操作）。 1234567891011121314151617181920212223242526Procsr: The number of processes waiting for run time.b: The number of processes in uninterruptible sleep.Memoryswpd: the amount of virtual memory used.free: the amount of idle memory.buff: the amount of memory used as buffers.cache: the amount of memory used as cache.inact: the amount of inactive memory. (-a option)active: the amount of active memory. (-a option)Swapsi: Amount of memory swapped in from disk (/s).so: Amount of memory swapped to disk (/s).IObi: Blocks received from a block device (blocks/s).bo: Blocks sent to a block device (blocks/s).Systemin: The number of interrupts per second, including the clock.cs: The number of context switches per second.CPUThese are percentages of total CPU time.us: Time spent running non-kernel code. (user time, including nice time)sy: Time spent running kernel code. (system time)id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time.wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle.st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown.]]></content>
      <tags>
        <tag>java</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flutter plugin registry]]></title>
    <url>%2F2019%2F05%2F17%2Fflutter-plugin-registry%2F</url>
    <content type="text"><![CDATA[前言首先这里有三个形似得英文单词registry, registrar and registrant分别对应注册局，注册商和注册人。把它们翻译到现实的生活场景中的角色其实是一个“注册人通过注册商，更新注册信息后，注册商把信息传递给注册局进行保存”的过程。 注册人：GeneratedPluginRegistrant注册局：[(FlutterViewController*)rootViewController pluginRegistry] == flutterEngine注册商：FlutterEngineRegistrar Flutter Applicationflutter create -t plugin my_plugin xcode 打开 my_plugin/example/ios路径下的 Runner工程 AppDelegate12345678910@implementation AppDelegate- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions &#123; [GeneratedPluginRegistrant registerWithRegistry:self]; // Override point for customization after application launch. return [super application:application didFinishLaunchingWithOptions:launchOptions];&#125;@end GeneratedPluginRegistrant1234567@implementation GeneratedPluginRegistrant+ (void)registerWithRegistry:(NSObject&lt;FlutterPluginRegistry&gt;*)registry &#123; [MyPlugin registerWithRegistrar:[registry registrarForPlugin:@"MyPlugin"]];&#125;@end AppDelegate继承FlutterAppDelegate FlutterAppDelegate123456789#pragma mark - FlutterPluginRegistry methods. All delegating to the rootViewController- (NSObject&lt;FlutterPluginRegistrar&gt;*)registrarForPlugin:(NSString*)pluginKey &#123; UIViewController* rootViewController = _window.rootViewController; if ([rootViewController isKindOfClass:[FlutterViewController class]]) &#123; return [[(FlutterViewController*)rootViewController pluginRegistry] registrarForPlugin:pluginKey]; &#125; return nil;&#125; FlutterViewController123- (id&lt;FlutterPluginRegistry&gt;)pluginRegistry &#123; return _engine;&#125; FlutterEngineRegistrar1234567891011121314151617181920212223@implementation FlutterEngineRegistrar &#123; NSString* _pluginKey; FlutterEngine* _flutterEngine;&#125;- (instancetype)initWithPlugin:(NSString*)pluginKey flutterEngine:(FlutterEngine*)flutterEngine &#123; self = [super init]; NSAssert(self, @"Super init cannot be nil"); _pluginKey = [pluginKey retain]; _flutterEngine = [flutterEngine retain]; return self;&#125;- (void)addMethodCallDelegate:(NSObject&lt;FlutterPlugin&gt;*)delegate channel:(FlutterMethodChannel*)channel &#123; [channel setMethodCallHandler:^(FlutterMethodCall* call, FlutterResult result) &#123; [delegate handleMethodCall:call result:result]; &#125;];&#125;- (NSObject&lt;FlutterBinaryMessenger&gt;*)messenger &#123; return _flutterEngine;&#125; MyPlugin123456789101112131415161718@implementation MyPlugin+ (void)registerWithRegistrar:(NSObject&lt;FlutterPluginRegistrar&gt;*)registrar &#123; FlutterMethodChannel* channel = [FlutterMethodChannel methodChannelWithName:@"my_plugin" binaryMessenger:[registrar messenger]]; MyPlugin* instance = [[MyPlugin alloc] init]; [registrar addMethodCallDelegate:instance channel:channel];&#125;- (void)handleMethodCall:(FlutterMethodCall*)call result:(FlutterResult)result &#123; if ([@"getPlatformVersion" isEqualToString:call.method]) &#123; result([@"iOS " stringByAppendingString:[[UIDevice currentDevice] systemVersion]]); &#125; else &#123; result(FlutterMethodNotImplemented); &#125;&#125;@end FlutterMethodChannel1234567891011121314151617181920212223242526272829- (instancetype)initWithName:(NSString*)name binaryMessenger:(NSObject&lt;FlutterBinaryMessenger&gt;*)messenger codec:(NSObject&lt;FlutterMethodCodec&gt;*)codec &#123; self = [super init]; NSAssert(self, @"Super init cannot be nil"); _name = [name retain]; _messenger = [messenger retain]; //flutterEngine _codec = [codec retain]; return self;&#125;- (void)setMethodCallHandler:(FlutterMethodCallHandler)handler &#123; if (!handler) &#123; [_messenger setMessageHandlerOnChannel:_name binaryMessageHandler:nil]; return; &#125; FlutterBinaryMessageHandler messageHandler = ^(NSData* message, FlutterBinaryReply callback) &#123; FlutterMethodCall* call = [_codec decodeMethodCall:message]; handler(call, ^(id result) &#123; if (result == FlutterMethodNotImplemented) callback(nil); else if ([result isKindOfClass:[FlutterError class]]) callback([_codec encodeErrorEnvelope:(FlutterError*)result]); else callback([_codec encodeSuccessEnvelope:result]); &#125;); &#125;; [_messenger setMessageHandlerOnChannel:_name binaryMessageHandler:messageHandler];&#125; FlutterEngine123456789101112131415#pragma mark - FlutterPluginRegistry- (NSObject&lt;FlutterPluginRegistrar&gt;*)registrarForPlugin:(NSString*)pluginKey &#123; NSAssert(self.pluginPublications[pluginKey] == nil, @"Duplicate plugin key: %@", pluginKey); self.pluginPublications[pluginKey] = [NSNull null]; return [[FlutterEngineRegistrar alloc] initWithPlugin:pluginKey flutterEngine:self];&#125;- (void)setMessageHandlerOnChannel:(NSString*)channel binaryMessageHandler:(FlutterBinaryMessageHandler)handler &#123; NSAssert(channel, @"The channel must not be null"); FML_DCHECK(_shell &amp;&amp; _shell-&gt;IsSetup()); self.iosPlatformView-&gt;GetPlatformMessageRouter().SetMessageHandler(channel.UTF8String, handler);&#125; FlutterBinaryMessage123456789101112131415161718/** * A message reply callback. * * Used for submitting a binary reply back to a Flutter message sender. Also used * in for handling a binary message reply received from Flutter. * * @param reply The reply. */typedef void (^FlutterBinaryReply)(NSData* _Nullable reply);/** * A strategy for handling incoming binary messages from Flutter and to send * asynchronous replies back to Flutter. * * @param message The message. * @param reply A callback for submitting an asynchronous reply to the sender. */typedef void (^FlutterBinaryMessageHandler)(NSData* _Nullable message, FlutterBinaryReply reply);]]></content>
  </entry>
  <entry>
    <title><![CDATA[sublime text 3]]></title>
    <url>%2F2019%2F05%2F14%2Fsublime-text-3%2F</url>
    <content type="text"><![CDATA[sublime text3 install package controlTools -&gt; Install package control 报错信息工具栏View 点击show console 或者快捷键 ctrl+` 打开控制台看下如下报错信息 1234Visit https://packagecontrol.io/installation for manual instructionsError installing Package Control: HTTPS error encountered, falling back to HTTP - &lt;urlopen error [Errno 60] Operation timed out&gt;Error installing Package Control: HTTP error encountered, giving up - &lt;urlopen error [Errno 60] Operation timed out&gt;error: An error occurred installing Package Control 处理办法绑定域名 1250.116.34.243 sublime.wbond.net50.116.34.243 packagecontrol.io install package报错信息There are no packages available for installation 处理办法下载 channel_v3.json 文件 （google一下） 修改package control.sublime-settings 123&quot;channels&quot;: [ &quot;/path/to/channel_v3.json&quot;]]]></content>
  </entry>
  <entry>
    <title><![CDATA[vim shortcuts]]></title>
    <url>%2F2019%2F05%2F14%2Fvim-shortcuts%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flutter之Dart编译]]></title>
    <url>%2F2019%2F05%2F10%2FFlutter%E4%B9%8BDart%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[前言App中使用flutter已经有段时间了，最近遇到一个bug记录一下。更新flutter module工程pubspec plugin依赖，App工程中pod update之后，从功能表现上看依然是老代码。第一感觉是缓存导致的，xcode clean 以及删除DerivedData目录重新build依然不行，flutter module工程中执行flutter clean然后xcode build是正常的，所以应该是dart编译产物有缓存导致的。接下来看下dart编译过程。 编译12cd path/to/flutter moduleflutter build ios --debug --simulator 进入到flutter module工程目录 执行flutter build ios命令 12345Running Xcode build... ├─Assembling Flutter resources... 3.6s └─Compiling, linking and signing... 25.4sXcode build done. 43.9s 可以看到会进行xcode build，进到.ios目录通过xcode打开Runner工程 可以看到build phases中这样一段脚本，这里就是执行dart代码编译的入口。 xcode_backend.sh进入到脚本所在目录，看下build对应的方法 BuildApp 12345678910111213if [[ $# == 0 ]]; then # Backwards-compatibility: if no args are provided, build. BuildAppelse case $1 in "build") BuildApp ;; "thin") ThinAppFrameworks ;; "embed") EmbedFlutterFrameworks ;; esacfi 123456789101112131415161718192021222324252627282930BuildApp() &#123; ... StreamOutput " ├─Assembling Flutter resources..." RunCommand "$&#123;FLUTTER_ROOT&#125;/bin/flutter" --suppress-analytics \ $&#123;verbose_flag&#125; \ build bundle \ --target-platform=ios \ --target="$&#123;target_path&#125;" \ --$&#123;build_mode&#125; \ --depfile="$&#123;build_dir&#125;/snapshot_blob.bin.d" \ --asset-dir="$&#123;derived_dir&#125;/App.framework/$&#123;assets_path&#125;" \ $&#123;precompilation_flag&#125; \ $&#123;flutter_engine_flag&#125; \ $&#123;local_engine_flag&#125; \ $&#123;track_widget_creation_flag&#125; if [[ $? -ne 0 ]]; then EchoError "Failed to package $&#123;project_path&#125;." exit -1 fi StreamOutput "done" StreamOutput " └─Compiling, linking and signing..." RunCommand popd &gt; /dev/null echo "Project $&#123;project_path&#125; built and packaged successfully." return 0&#125; 可以看到 ├─Assembling Flutter resources… 在build ios 执行过程中出现过，flutter build bundle 就会开始真正的dart编译–depfile 指定参与编译的dart文件路径集合–asset-dir 指定资源产物的目录 flutter命令路径 $FLUTTER_ROOT/bin/flutter 123456789101112...FLUTTER_TOOLS_DIR="$FLUTTER_ROOT/packages/flutter_tools"SNAPSHOT_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.snapshot"STAMP_PATH="$FLUTTER_ROOT/bin/cache/flutter_tools.stamp"SCRIPT_PATH="$FLUTTER_TOOLS_DIR/bin/flutter_tools.dart"DART_SDK_PATH="$FLUTTER_ROOT/bin/cache/dart-sdk"DART="$DART_SDK_PATH/bin/dart"PUB="$DART_SDK_PATH/bin/pub""$DART" $FLUTTER_TOOL_ARGS "$SNAPSHOT_PATH" "$@" flutter_toools.snapshot实际上就是$FLUTTER_ROOT/packages/flutter_tools这个项目编译生成的snapshot文件 所以flutter build bundle 就是使用dart来执行flutter_tools项目的main方法 flutter_tools路径 $FLUTTER_ROOT/packages/flutter_tools main方法定义 $FLUTTER_ROOT/packages/flutter_tools/bin/flutter_tools.dart 123void main(List&lt;String&gt; args) &#123; executable.main(args);&#125; 再看看 lib/executable.dart ,在这里会预先创建好每一种命令对应的对象command，通过解析args参数找到对应的command。在BuildCommand类中 123456789BuildCommand(&#123;bool verboseHelp = false&#125;) &#123; addSubcommand(BuildApkCommand(verboseHelp: verboseHelp)); addSubcommand(BuildAppBundleCommand(verboseHelp: verboseHelp)); addSubcommand(BuildAotCommand()); addSubcommand(BuildIOSCommand()); addSubcommand(BuildFlxCommand()); addSubcommand(BuildBundleCommand(verboseHelp: verboseHelp)); addSubcommand(BuildWebCommand()); &#125; 看到BuildIOSCommand 以及 BuildBundleCommand的创建。BuildIOSCommand 就是前面提到的flutter build ios 会执行到的，这里我们重点看下BuildBundleCommand是如何编译dart代码的？编译后生成了哪些资源？这些资源都是些什么？ BuildBundleCommand app.dill : 这就是dart代码编译后的二级制文件 Frontend_server.d : 这里面放的是frontend_server.dart.snapshot的绝对路径，使用该snapshot来编译dart代码生成上面的app.dill snapshot_blob.bin.d : 这里面放的是所有参与编译的dart文件的绝对路径的集合，包括项目的代码和flutterSdk的代码以及pub库中的三方代码。 snapshot_blob.bin.d.fingerprint : 这里面放的是snapshot_blob.bin.d中的所有文件的绝对路径以及每个文件所对应的md5值。使用这个md5来判断该文件是否有修改。在每次编译的时候会判断，如果没有文件修改，则直接跳过编译。 编译Dart资源123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127Future&lt;void&gt; build(&#123; TargetPlatform platform, BuildMode buildMode, String mainPath = defaultMainPath, String manifestPath = defaultManifestPath, String applicationKernelFilePath, String depfilePath, String privateKeyPath = defaultPrivateKeyPath, String assetDirPath, String packagesPath, bool precompiledSnapshot = false, bool reportLicensedPackages = false, bool trackWidgetCreation = false, String compilationTraceFilePath, bool createPatch = false, String buildNumber, String baselineDir, List&lt;String&gt; extraFrontEndOptions = const &lt;String&gt;[], List&lt;String&gt; extraGenSnapshotOptions = const &lt;String&gt;[], List&lt;String&gt; fileSystemRoots, String fileSystemScheme,&#125;) async &#123; // xcode_backend.sh中通过--depfile传入进来的 // 默认是build/snapshot_blob.bin.d文件 depfilePath ??= defaultDepfilePath; // 通过--asset-dir传入 // 该目录中文件就是flutter的产物，最终合并到app.framework中的flutter_assets目录 assetDirPath ??= getAssetBuildDirectory(); packagesPath ??= fs.path.absolute(PackageMap.globalPackagesPath); // app.dill dart代码编译后的二级制文件 applicationKernelFilePath ??= getDefaultApplicationKernelPath(trackWidgetCreation: trackWidgetCreation); final FlutterProject flutterProject = await FlutterProject.current(); if (compilationTraceFilePath != null) &#123; if (buildMode != BuildMode.dynamicProfile &amp;&amp; buildMode != BuildMode.dynamicRelease) &#123; compilationTraceFilePath = null; &#125; else if (compilationTraceFilePath.isEmpty) &#123; // Disable JIT snapshotting if flag is empty. printStatus('Code snapshot will be disabled for this build.'); compilationTraceFilePath = null; &#125; else if (!fs.file(compilationTraceFilePath).existsSync()) &#123; // Be forgiving if compilation trace file is missing. printStatus('No compilation trace available. To optimize performance, consider using --train.'); final File tmp = fs.systemTempDirectory.childFile('flutterEmptyCompilationTrace.txt'); compilationTraceFilePath = (tmp..createSync(recursive: true)).path; &#125; else &#123; printStatus('Code snapshot will use compilation training file $compilationTraceFilePath.'); &#125; &#125; DevFSContent kernelContent; if (!precompiledSnapshot) &#123; if ((extraFrontEndOptions != null) &amp;&amp; extraFrontEndOptions.isNotEmpty) printTrace('Extra front-end options: $extraFrontEndOptions'); ensureDirectoryExists(applicationKernelFilePath); final KernelCompiler kernelCompiler = await kernelCompilerFactory.create(flutterProject); // 编译dart代码，生成app.dill 和 snapshot_blob.bin.d 以及 snapshot_blob.bin.d.fingerprint final CompilerOutput compilerOutput = await kernelCompiler.compile( sdkRoot: artifacts.getArtifactPath(Artifact.flutterPatchedSdkPath), incrementalCompilerByteStorePath: compilationTraceFilePath != null ? null : fs.path.absolute(getIncrementalCompilerByteStoreDirectory()), mainPath: fs.file(mainPath).absolute.path, outputFilePath: applicationKernelFilePath, depFilePath: depfilePath, trackWidgetCreation: trackWidgetCreation, extraFrontEndOptions: extraFrontEndOptions, fileSystemRoots: fileSystemRoots, fileSystemScheme: fileSystemScheme, packagesPath: packagesPath, linkPlatformKernelIn: compilationTraceFilePath != null, ); if (compilerOutput?.outputFilename == null) &#123; throwToolExit('Compiler failed on $mainPath'); &#125; kernelContent = DevFSFileContent(fs.file(compilerOutput.outputFilename)); // 生成 frontend_server.d文件，向文件中写入frontendServerSnapshotForEngineDartSdk的路径 await fs.directory(getBuildDirectory()).childFile('frontend_server.d') .writeAsString('frontend_server.d: $&#123;artifacts.getArtifactPath(Artifact.frontendServerSnapshotForEngineDartSdk)&#125;\n'); if (compilationTraceFilePath != null) &#123; final JITSnapshotter snapshotter = JITSnapshotter(); final int snapshotExitCode = await snapshotter.build( platform: platform, buildMode: buildMode, mainPath: applicationKernelFilePath, outputPath: getBuildDirectory(), packagesPath: packagesPath, compilationTraceFilePath: compilationTraceFilePath, extraGenSnapshotOptions: extraGenSnapshotOptions, createPatch: createPatch, buildNumber: buildNumber, baselineDir: baselineDir, ); if (snapshotExitCode != 0) &#123; throwToolExit('Snapshotting exited with non-zero exit code: $snapshotExitCode'); &#125; &#125; &#125; // 生成 flutter_assets final AssetBundle assets = await buildAssets( manifestPath: manifestPath, assetDirPath: assetDirPath, packagesPath: packagesPath, reportLicensedPackages: reportLicensedPackages, ); if (assets == null) throwToolExit('Error building assets', exitCode: 1); await assemble( buildMode: buildMode, assetBundle: assets, kernelContent: kernelContent, privateKeyPath: privateKeyPath, assetDirPath: assetDirPath, compilationTraceFilePath: compilationTraceFilePath, );&#125; 编译Dart代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class KernelCompiler &#123; const KernelCompiler(); Future&lt;CompilerOutput&gt; compile(&#123; String sdkRoot, String mainPath, String outputFilePath, String depFilePath, TargetModel targetModel = TargetModel.flutter, bool linkPlatformKernelIn = false, bool aot = false, @required bool trackWidgetCreation, List&lt;String&gt; extraFrontEndOptions, String incrementalCompilerByteStorePath, String packagesPath, List&lt;String&gt; fileSystemRoots, String fileSystemScheme, bool targetProductVm = false, String initializeFromDill, &#125;) async &#123; final String frontendServer = artifacts.getArtifactPath( Artifact.frontendServerSnapshotForEngineDartSdk ); FlutterProject flutterProject; if (fs.file('pubspec.yaml').existsSync()) &#123; flutterProject = await FlutterProject.current(); &#125; // TODO(cbracken): eliminate pathFilter. // Currently the compiler emits buildbot paths for the core libs in the // depfile. None of these are available on the local host. Fingerprinter fingerprinter; // 如果snapshot_blob.bin.d文件不为空，则说明有编译缓存 if (depFilePath != null) &#123; // 判断与上次编译对比，是否有文件的md5改变 fingerprinter = Fingerprinter( // snapshot_blob.bin.d.fingerprint文件 fingerprintPath: '$depFilePath.fingerprint', paths: &lt;String&gt;[mainPath], properties: &lt;String, String&gt;&#123; 'entryPoint': mainPath, 'trackWidgetCreation': trackWidgetCreation.toString(), 'linkPlatformKernelIn': linkPlatformKernelIn.toString(), 'engineHash': Cache.instance.engineRevision, 'buildersUsed': '$&#123;flutterProject != null ? flutterProject.hasBuilders : false&#125;', &#125;, depfilePaths: &lt;String&gt;[depFilePath], pathFilter: (String path) =&gt; !path.startsWith('/b/build/slave/'), ); // 判断是否有文件改动，如果没有，则直接返回。 if (await fingerprinter.doesFingerprintMatch()) &#123; printTrace('Skipping kernel compilation. Fingerprint match.'); return CompilerOutput(outputFilePath, 0, /* sources */ null); &#125; &#125; ... // 如果没有上次编译缓存，或者文件有改变，Fingerprinter不匹配，则使用dart重新编译 final List&lt;String&gt; command = &lt;String&gt;[ engineDartPath, frontendServer, '--sdk-root', sdkRoot, '--strong', '--target=$targetModel', ]; ... //参数拼接 final Process server = await processManager .start(command) .catchError((dynamic error, StackTrace stack) &#123; printError('Failed to start frontend server $error, $stack'); &#125;); final StdoutHandler _stdoutHandler = StdoutHandler(); server.stderr .transform&lt;String&gt;(utf8.decoder) .listen(printError); server.stdout .transform&lt;String&gt;(utf8.decoder) .transform&lt;String&gt;(const LineSplitter()) .listen(_stdoutHandler.handler); final int exitCode = await server.exitCode; if (exitCode == 0) &#123; if (fingerprinter != null) &#123; await fingerprinter.writeFingerprint(); &#125; return _stdoutHandler.compilerOutput.future; &#125; return null; &#125;&#125; Fingerprint对比1234567891011121314151617181920212223242526Future&lt;bool&gt; doesFingerprintMatch() async &#123; try &#123; // 获取到当前的 snapshot_blob.bin.d.fingerprint文件 final File fingerprintFile = fs.file(fingerprintPath); if (!fingerprintFile.existsSync()) return false; if (!_depfilePaths.every(fs.isFileSync)) return false; final List&lt;String&gt; paths = await _getPaths(); if (!paths.every(fs.isFileSync)) return false; // 读取缓存的的snapshot_blob.bin.d.fingerprint文件，构建一个老的Fingerprint对象 final Fingerprint oldFingerprint = Fingerprint.fromJson(await fingerprintFile.readAsString()); // 构建一个新的Fingerprint对象 final Fingerprint newFingerprint = await buildFingerprint(); // 对比两次的文件集合中的每个文件的md5是否一样 return oldFingerprint == newFingerprint; &#125; catch (e) &#123; // Log exception and continue, fingerprinting is only a performance improvement. printTrace('Fingerprint check error: $e'); &#125; return false; &#125; 重点 看看 newFingerprint 12345678910111213Future&lt;Fingerprint&gt; buildFingerprint() async &#123; final List&lt;String&gt; paths = await _getPaths(); return Fingerprint.fromBuildInputs(_properties, paths);&#125;Future&lt;List&lt;String&gt;&gt; _getPaths() async &#123; final Set&lt;String&gt; paths = _paths.toSet(); // 使用缓存的snapshot_blob.bin.d文件中的文件集合 for (String depfilePath in _depfilePaths) paths.addAll(await readDepfile(depfilePath)); final FingerprintPathFilter filter = _pathFilter ?? (String path) =&gt; true; return paths.where(filter).toList()..sort();&#125; 可以看到newFingerprint 路径依旧是使用缓存的路径，依次计算路径对应文件的md5，所以问题就在这里了 执行flutter packages upgrade更新pub依赖的时候，build目录下的缓存产物并不会有任何变动，路径依然是老的路径。有一种情况就是module工程 lib 目录下的dat文件有改动，newFingerprint就会跟old不一样，这会重新编译dart，这里又有一个问题，就是如果lib目录下是新增dart文件 则不会被编译进去。 最后综上，执行flutter clean命令，清空build目录缓存文件，build ios 就会重新编译整个dart文件，包括pub依赖中的。 参考链接Flutter深入之flutter-build-bundle命令如何编译Dart?]]></content>
      <categories>
        <category>flutter</category>
      </categories>
      <tags>
        <tag>flutter</tag>
        <tag>dart</tag>
      </tags>
  </entry>
</search>
